labels,name
:task,clinical trial eligibility identification
:characteristic,natural language eligibility criteria
:solution,text classification methods
:dataset,Phase III cancer trial exclusions
:data,764 Phase III cancer trials
:`model family`,Transformer
:task,exclusion criteria classification
:model,Clinical Trial BERT
:result,feasibility of automatic classification
:metric,highest average performance
:`exclusion criteria`,common exclusion criteria in cancer trials
:list,"prior malignancy, HIV, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, autoimmune illness"
:`model family`,large language model
:technique,data augmentation
:`model family`,small language model
:`performance metric`,out-of-domain performance
:technique,counterfactual instance generation
:`model calibration`,confidence-based calibrator
:`model calibration`,rationale-augmented calibrator
:`diversity metric`,counterfactual instance diversity
:`model family`,counterfactual augmented model
:`model characteristic`,low entropy in importance assignment
:`model characteristic`,concise explanations
:task,extractive question answering
:technology,blockchain
:context,modern technology landscape
:technology,smart contract
:duality,risks and benefits
:vulnerability,smart contract vulnerability
:outcome,significant losses
:literature,current papers on smart contracts
:task,classifying smart contracts
:framework,two-layered framework for smart contracts
:task,repairing malicious contracts
:tool,Slither
:artifact,vulnerability report
:model,RandomForestClassifier
:task,classifying vulnerabilities
:task,repairing vulnerabilities
:model,GPT-3.5-turbo
:artifact,smart contract repair model
:model,LLaMA-2-7B
:quality,functionality retention
:method,automatic batch classification and repair
:`model family`,language model
:`user group`,sophisticated and diverse users
:value,factuality
:domain,various fields of study and professions
:domain,high-stakes fields
:risk,propagating false information
:research,previous work on factuality and attribution
:aspect,domain-specific scenarios
:study,evaluation study on factuality and attribution
:aspect,factuality and attribution in responses
:dataset,ExpertQA
:method,expert curation and revision
:domain,32 fields of study
:participant,domain experts
:task,evaluation and revision of language model responses
:field,artificial intelligence
:goal,human-level AI
:entity,AI agent
:concept,artificial entity
:community,AI community
:activity,developing intelligent agents
:concept,Artificial General Intelligence (AGI)
:survey,LLM-based agents
:framework,general framework for LLM-based agents
:application,LLM-based agent applications
:concept,agent society
:repository,LLM-agent-paper-list
:url,https://github.com/woooodyy/llm-agent-paper-list
:problem,trade-off in generalizable prompt learning
:`model family`,vision-language model
:method,existing generalizable methods
:issue,seen classes degradation
:perspective,optimization
:method,sharpness-aware minimization (SAM)
:concept,loss landscape geometry
:concept,loss sharpness
:concept,trade-off performance
:issue,optimizing gradient relevance
:method,Gradient Constrained Sharpness-Aware Context Optimization (GCSCOOP)
:activity,extensive experiments
:`agent type`,autonomous language agent
:concept,artificial general intelligence
:library,Agents
:audience,non-specialist audience
:feature,advanced agent features
:activity,building autonomous language agents
:attribute,research-friendly
:platform,GitHub
:attribute,helpfulness
:issue,harmful content generation
:attribute,safety
:`model family`,instruction-tuned large language model
:attribute,unsafe
:technique,safety-tuning
:model,LLaMA
:issue,exaggerated safety
:study,trade-offs in training LLMS
:concept,instruction following vs safety
:technique,in-context learning
:task,complex multi-modal prompt understanding
:approach,MMICL
:dataset,Multi-Modal In-Context Learning (MIC) dataset
:performance,state-of-the-art zero-shot performance
:issue,language bias in VLMs
:field,human-scene interaction
:field,embodied AI
:field,virtual reality
:factor,versatile interaction control
:application,practical application of HSI
:factor,user-friendly interface development
:framework,UniHSI
:feature,unified control of diverse interactions
:definition,interaction as Chain of Contacts (CoC)
:correlation,interaction types and human-object contact regions
:component,LLM planner
:output,task plans in the form of CoC
:component,unified controller
:output,uniform task execution
:dataset,ScenePlan
:purpose,training and evaluation of UniHSI
:quality,effectiveness in versatile task execution
:quality,generalizability to real scanned scenes
:url,https://github.com/openrobotlab/unihsi
:task,task-oriented grasping
:behavior,grasping by specific part
:planner,learning-based grasp planner
:system,LERF-ToGo
:model,Language Embedded Radiance Fields (LERF)
:representation,3D language field
:concept,objectness
:task,grasp ranking
:evaluation,LERF-ToGo evaluation
:performance,grasp success rate
:resource,project website
:field,machine learning
:task,reasoning tasks
:`model architecture`,novel model architecture
:protocol,large-scale pre-training
:dataset,dedicated reasoning datasets
:tool,data generator for machine reasoning
:task,machine reasoning
:data,templated text queries and answers
:data,world-states
:format,database
:`model family`,pre-trained language model
:format,text-formatted database representation
:`model architecture`,graph-structured Transformer
:format,knowledge-graph database representation
:`model family`,neural reasoning model
:resource,code
:`entity type`,salient entity
:concept,document aboutness
:task,salient entity detection
:application,downstream applications
:approach,feature engineering
:`model family`,medium-sized language model
:`model architecture`,cross-encoder
:benchmark,comprehensive benchmarking
:dataset,publicly available datasets
:technique,zero-shot prompting
:`model family`,instruction-tuned language model
:field,Natural Language Processing (NLP)
:task,clinical NLP tasks
:technique,prompt engineering
:paper,this paper
:model,GPT-3.5
:model,BARD
:model,LLaMA2
:technique,few-shot prompting
:application,summarization
:`evaluation focus`,summarization model evaluation
:aspect,"content selection, grammaticality, coherence"
:issue,social biases
:concept,biased behaviours in summarization models
:methodology,quantifying biased behaviours
:method,controlled demographic attributes in input documents
:issue,bias confounding
:`model type`,purpose-built summarization model
:`model type`,general purpose chat model
:process,content selection in single document summarization
:phenomenon,hallucinations in summarization
:domain,online education
:system,AI-powered tutoring system
:system,intelligent tutoring system
:function,education process automation
:concept,core processes
:process,"interaction, reflection, reaction"
:tool,LLM-powered tools
:component,memory modules
:process,education process
:result,statistical results from learning logs
:concept,tool effectiveness
:feedback,subjective feedback from human users
:concept,usability
:system,ablation systems
:concept,software quality and security
:domain,software systems
:problem,software vulnerabilities
:concept,software security
:field,automated program repair
:task,bug detection and fixing
:technique,deep learning
:issue,limited benchmarks
:framework,Reef
:task,collecting vulnerabilities and fixes
:tool,multi-language crawler
:`model family`,neural language model
:task,generating vulnerability explanations
:dataset,vulnerability-fix pairs dataset
:artifact,"CVEs, patches, and CWEs"
:evaluation,human expert evaluation
:technique,token-level serialized output training
:task,streaming multi-talker ASR
:challenge,overlapped speech
:architecture,naive neural transducer
:task,text-only adaptation
:`model structure`,T-SOT with factorized neural transducer
:limitation,naive neural transducer limitation
:challenge,unnatural token order
:technique,special token handling
:benchmark,original T-SOT performance
:metric,word error rate reduction
:task,visual object navigation
:condition,object presence and identification
:requirement,object name and presence knowledge
:simulator,scene metadata
:data,pre-defined object names and positions
:challenge,real-world scenarios
:concept,demand-driven navigation
:method,textual attribute features extraction
:technique,contrastive language-image pre-training
:feature,visual attribute features
:process,navigation
:experiment,AI2-THOR with the ProcTHOR dataset
:`performance metric`,navigation performance
:tool,psychological tools
:concept,personality
:method,personality self-assessment tests
:model,ChatGPT
:finding,personality scores variability
:technique,prompting
:property,option order symmetry
:test,self-assessment personality tests
:`inference scheme`,self-speculative decoding
:process,two-stage process
:stage,drafting
:output,draft tokens
:stage,verification
:output,final output
:output,unmodified LLM output
:requirement,additional training or memory
:attribute,plug-and-play and cost-effective
:benchmark,with LLaMA-2 and fine-tuned models
:`performance metric`,speedup
:task,interactive decision-making
:method,oracle trajectories
:`execution mode`,forward-only
:scenario,challenging scenarios
:concept,state space exploration
:feature,flexible back-tracking
:model,LASER
:task,WebShop
:benchmark,previous methods
:benchmark,human performance
:`application domain`,legal intelligence
:goal,data privacy
:methodology,federated learning
:process,fine-tuning
:challenge,computation and communication overheads
:challenge,distribution shift of legal data
:framework,FedJudge
:technique,parameter-efficient fine-tuning
:technique,continual learning
:evidence,experimental results
:resource,FedJudge code
:task,Failure Mode Classification (FMC)
:domain,maintenance
:model,text classification model
:model,out-of-the-box GPT-3.5
:`data set`,high quality fine-tuning data sets
:task,domain-specific tasks
:task,physics word problems
:dataset,PhysQA
:topic,physics topics
:`performance metric`,problem-solving accuracy
:technique,few-shot learning
:task,knowledge summarization and problem generation
:research,this work
:sector,secondary education
:activity,code review
:goal,software quality and maintainability
:characteristic,time-consuming and error-prone
:task,automating code review
:study,empirical study on ChatGPT in code review
:capability,ChatGPT in code review
:task,automated code refinement
:input,code reviews
:benchmark,CodeReview
:dataset,new high-quality code review dataset
:tool,CodeReviewer
:metric,EM and BLEU scores
:challenge,ChatGPT's underperformance
:strategy,mitigation strategies for ChatGPT's challenges
:insight,potential of ChatGPT in automating code review
:`research direction`,potential research directions highlighted
:process,pre-training
:`data type`,unsupervised raw data
:`model family`,natural language model
:`pre-training objective`,masked language modeling (MLM)
:`pre-training objective`,random token substitution (RTS)
:task,token originality prediction
:`pre-training objective`,cluster-based random token substitution (C-RTS)
:`pre-training objective`,swapped language modeling (SLM)
:task,original token value prediction
:`pre-training task`,self-supervised pre-training
:corpus,Wikipedia
:model,RoBERTa
:process,continuous pre-training
:task,fact verification
:task,answer sentence selection
:dataset,FEVER (dev set)
:`pre-training objective`,proposed objectives
:technique,integration with other methods
:`cognitive process`,abduction
:task,narrative comprehension
:task,abductive natural language inference
:goal,plausible hypothesis inference
:issue,inter-sentential coherence
:issue,model consistency
:model,Alpha-PACE
:concept,self-consistency and inter-sentential coherence
:framework,general self-consistent framework
:`narrative sequence`,linear narrative
:`narrative sequence`,reverse chronology
:concept,necessity and effectiveness
:performance,Alpha-PACE performance
:benchmark,competitive baselines
:scenario,in-distribution (ID)
:scenario,out-of-distribution (OOD)
:application,argument mining
:method,simulation of distribution shifts
:domain,social media
:topic,solar energy
:study,OOD generalization analysis
:metric,generalization flaws
:technique,prompt-based fine-tuning
:scenario,semantic differences in train and test data
:scenario,discrepancies in label distribution
:problem,gradient-based learning bias
:capability,understanding and generating language
:integration,language model with knowledge bases
:environment,real-world environments
:application,semantic parsing
:issue,hallucinated information
:task,knowledge base question answering (KBQA)
:scenario,inconsistent data distribution
:issue,robustness challenges
:issue,data distribution
:`research direction`,data collection and LM learning paradigms
:concept,semi-structured explanation
:process,reasoning process
:task,generating an answer
:capability,generative capabilities
:challenge,producing structured explanations
:capability,true reasoning capabilities
:`model family`,not-so-large language model
:task,structured explanation
:technique,supervised fine-tuning
:method,reward engineering in reinforcement learning
:method,reinforcement learning
:technique,reward aggregation methods
:field,future research
:reward,proposed reward
:benchmark,semi-structured explanation generation benchmarks
:benchmark,ExplaGraph and COPA-SSE
:task,probability prediction
:study,current
:method,Contrastive Weight Tying (CWT)
:`model type`,headless language model
:metric,computational requirements
:metric,downstream performance
:metric,GLUE score
:metric,LAMBADA accuracy
:context,monolingual and multilingual
:`model family`,classical language model
:task,knowledge incorporation into dialogue generation
:attribute,correctness of response
:concept,evidence fragments
:task,supporting factual dialogue replies
:issue,irrelevant content introduction
:attribute,hallucinated responses
:task,evidence retrieval and integration in dialogue systems
:issue,inaccurate evidence location
:framework,U-EIDG
:framework,automatic evidence generation
:indicator,reliable evidence indicator
:task,identifying relevant evidence
:generator,evidence-augmented generator
:mechanism,evidence-focused attention mechanism
:dataset,MultiDoc2Dial
:technique,evidential label augmentation
:`model performance`,coherence and factual consistency
:technique,U-EIDG framework
:task,language model evaluation
:field,language understanding and generation
:tool,benchmarks
:language,Traditional Chinese
:tool,comprehensive and diverse benchmarks
:benchmark,DRCD
:benchmark,TTQA
:benchmark,CMQA
:benchmark,FGC dataset
:benchmark,novel set of benchmarks
:model,Taiwan-LLaMA-v1.0
:model,Model 7-C
:community,research community
:task,knowledge engineering
:challenge,ISWC 2023 LM-KBC Challenge
:source,Wikidata
:data,subject and relation pairs
:pipeline,LLMKE
:technique,knowledge probing
:technique,Wikidata entity mapping
:`performance metric`,macro-averaged F1-score
:domain,various domains
:task,knowledge base completion and correction
:task,collaborative knowledge engineering
:event,Track 2 of ISWC 2023 LM-KBC Challenge
:resource,GitHub repository
:task,image-to-speech captioning
:model,Im2Sp model
:`model component`,discretized speech units
:`information type`,linguistic information
:strategy,vision-language pre-training
:benchmark,COCO and Flickr8k
:`model component`,image units
:process,vector quantization
:metric,0.8% data storage
:resource,demo page
:technique,prompt crafting
:resource,human effort
:framework,EvoPrompt
:technique,discrete prompt optimization
:algorithm,evolutionary algorithms
:concept,gradients or parameters
:model,Alpaca
:technique,human-engineered prompt generation
:synergy,LLMs and EAs combination
:technique,language model-based expansion
:task,information retrieval
:question,effectiveness of LM-based expansion
:analysis,comprehensive analysis of LM-based expansion
:correlation,negative correlation between retriever performance and gains from expansion
:phenomenon,retriever performance
:trend,expansion improves scores for weaker models
:analysis,qualitative error analysis
:recommendation,use expansions for weaker models
:condition,weaker models or significant format difference
:recommendation,avoid expansions to keep relevance signal clear
:condition,stronger models or minor format difference
:demographic,millions of users
:issue,societal biases
:scholarship,LLM bias research
:perspective,western-centric
:region,Global South
:dataset,Indian-BHED
:task,bias quantification
:region,Indian context
:technique,instruction prompting
:value,diversity in evaluation
:task,style transfer
:value,explainability
:value,explanations
:model,GPT-4
:`model family`,smaller distributed transparent model
:framework,model distillation
:dataset,formality style transfer dataset
:technique,in-context learning from expert feedback (ICLEF)
:dataset,E-GYAFC
:task,explainable formality style transfer
:`model family`,smaller model
:value,expert preferences
:application,interpretable authorship verification
:task,explainable style transfer
:application,interpretable adversarial attacks on AI-generated text detectors
:concept,hierarchical reasoning
:task,effective decision-making
:process,hierarchical planning
:model,compositional foundation models for hierarchical planning (HiP)
:task,long-horizon tasks
:artifact,symbolic plans
:model,large video diffusion model
:artifact,video plans
:model,inverse dynamics model
:process,visual-motor control
:task,table-top manipulation tasks
:capability,self-teaching
:method,SECTOR (Self-Education via Chain-of-Thought Reasoning)
:process,self-learning loop
:task,solving addition problems
:hypothesis,chain-of-thought reasoning as a policy improvement operator
:technique,Monte-Carlo Tree Search in AlphaZero
:research,this research
:task,domain adaptation
:model,neural machine translation
:approach,our approach
:technique,batch inference
:technique,traditional supervised learning
:model,40b-parameter large language model
:benchmark,state-of-the-art baselines
:metric,immediate adaptation rate
:task,question answering and reasoning
:task,reasoning in situational context
:concept,cultural common ground
:`model family`,multilingual large language model
:attribute,culturally-diverse reasoning
:task,proverb reasoning in conversational context
:task,figurative proverb reasoning
:issue,culture gap
:dataset,MAPS (Multicultural Proverbs and Sayings)
:task,proverb understanding in conversational context
:`knowledge type`,parametric knowledge
:`knowledge type`,external knowledge
:process,interaction with users
:framework,systematic elicitation of LLM knowledge
:`knowledge graph`,parametric knowledge graph
:`knowledge structure`,LLM knowledge structures
:technique,distractors
:phenomenon,deviation from parametric knowledge
:risk,hallucination
:resource,data and results
:accessibility,publicly available
:phenomenon,polysemanticity
:goal,understandability
:phenomenon,superposition
:technique,sparse autoencoder
:concept,interpretable directions
:property,monosemantic
:technique,automated interpretability measurement
:concept,interpretability
:concept,learned set of features
:task,indirect object identification
:technique,scalable unsupervised method
:goal,mechanistic interpretability
:value,model transparency
:value,steerability
:`model behavior`,brittleness
:benchmark,larger and diverse benchmarks
:benchmark,small evaluation sets
:`model performance`,assessment
:phenomenon,model confidence correlation
:benchmark,language classification benchmarks
:technique,anchor point selection
:benchmark,small subsets selection
:task,model ranking
:task,per-class prediction estimation
:tool,anchor point maps
:task,model performance visualization
:domain,mobile applications
:task,app usage prediction
:model,MAPLE
:data,public datasets
:capability,understanding intricate patterns and contexts
:quality,versatility and resilience
:task,modelling human behaviours
:task,table representation learning
:`model type`,tabular language model
:performance,state-of-the-art results
:issue,entity leakage
:dataset,evaluation datasets
:technique,adversarial attack
:setup,realistic inference
:attack,evasive entity-swap attack
:task,column type annotation
:characteristic,first black-box attack on tables
:strategy,similarity-based sampling
:technique,adversarial example generation
:outcome,performance drop
:phenomenon,fake news
:value,trust
:capability,generating believable fake content
:paradigm,evaluation of fake news detectors
:content,human-written and LLM-generated misinformation
:bias,flagging LLM-generated content as fake
:tool,fake news detectors
:bias,misclassifying human-written fake news as genuine
:`linguistic pattern`,distinct to LLM outputs
:strategy,adversarial training with LLM-paraphrased genuine news
:bias,fake news detection
:model,mitigated fake news detector
:`performance metric`,detection accuracy
:dataset,GossipCop++
:dataset,Politifact++
:challenge,memory-constrained deployment
:component,embedding matrix
:concept,vocabulary
:effort,parameter-efficient PLM development
:technique,pruning parameters
:process,inference
:finding,unused vocabulary proportion
:scenario,fine-tuning and inference
:approach,embedding matrix memory footprint minimization
:benefit,reduced memory usage
:goal,downstream task performance
:technique,byte pair encoding tokenization
:tokenizer,SentencePiece
:concept,semantics
:tokenizer,HuggingFace
:technique,incremental tokenization
:technique,left-to-right tokenization
:technique,finite state string-to-string transducer
:application,multimodal applications
:system,Musilingo
:task,music caption generation
:task,music-related query responses
:model,MERT
:model,Vicuna-7B
:dataset,Music Instruct (MI)
:dataset,MusicCaps
:dataset,music caption dataset
:task,generating music captions
:task,composing music-related Q&A pairs
:goal,predicting chemical function from structure
:field,chemical sciences
:`model family`,general predictive models
:resource,chemical patents
:knowledge,chemical function
:challenge,extracting high-quality functional labels
:pipeline,ChatGPT-assisted patent summarization and word-embedding label cleaning
:dataset,CHEF dataset
:entity,100k molecules with functional labels
:model,functional label prediction model
:application,drug discovery and repurposing
:graph,co-occurrence graph of functional labels
:property,semantic structure
:approach,audio and text representation alignment
:task,audio generation
:model,language model-based audio generator
:input,textual and audio token representations
:issue,lack of explicit regularization
:technique,classifier-free guidance (CFG)
:component,text condition
:proposal,audio and text representation regularization
:goal,alignment between audio and text representations
:task,music generation
:metric,objective metrics
:perception,human perception for audio generation
:field,bio-inspired materials science
:field,engineering solutions
:model,BioInspiredLLM
:data,peer-reviewed articles corpus
:task,research tasks
:capability,information recall and question formulation
:capability,hypothesis development
:`model family`,generative artificial intelligence models
:method,collaborative generative artificial intelligence
:task,bio-inspired materials design
:field,biological materials
:concept,interdisciplinary science
:concept,knowledge domain integration
:technique,hierarchical token stacks
:technique,decoding strategies
:technique,flattening codebooks
:quality,high-quality decoding
:attribute,slow decoding speed
:technique,stack-and-delay decoding
:attribute,fast decoding speed
:technique,delay decoding strategy
:attribute,GPU efficiency
:evaluation,objective evaluations
:evaluation,subjective evaluations
:technique,competing model
:task,dialogue state tracking
:goal,tracking user preferences and intents
:system,task-oriented dialogue system
:domain,narrow domain applications
:challenge,real-world intricacies in open-domain dialogues
:phenomenon,complex contextual interactions and shifts
:approach,joint dialogue segmentation and state tracking per segment
:setting,zero-shot
:system,open-domain dialogue system
:technique,S3-DST
:mechanism,pre-analytical recollection
:goal,improving long context tracking
:data,open-domain dialogue dataset
:benchmark,state-of-the-art
:technology,chatbot
:timeframe,over half a century
:technology,traditional chatbot
:issue,bias and fairness
:technology,modern chatbot
:challenge,bias mitigation and fairness preservation
:issue,"large training data, model size, interpretability"
:topic,bias and fairness in chatbot systems
:topic,history and categories of chatbots
:issue,bias sources and potential harms
:goal,fair and unbiased chatbot systems
:topic,future research directions
:achievement,human-like predictive performance
:hyperparameter,learning rate
:process,LLM fine-tuning
:policy,existing learning rate policies
:`model type`,traditional deep neural networks
:challenge,learning rate tuning
:era,era of large language models
:tool,LRBench++
:analysis,experimental analysis with LRBench++
:difference,LLM fine-tuning vs traditional DNN training
:task,ICD coding
:activity,assigning ICD diagnosis codes
:attribute,large quantity of labels and lengthy texts
:quantity,"9,000 labels and 8,000 tokens"
:process,single-pass reading
:process,multi-pass reading
:`model family`,pretrained language model
:issue,huge memory usage
:model,Multi-Hop Label-Wise Attention (MHLAT)
:technique,multi-hop label-wise attention
:dataset,MIMIC datasets
:attribute,fewer parameters
:issue,context length limitation
:task,document question answering
:approach,context retrieval
:`document type`,structured document
:representation,plain text
:issue,incongruity
:approach,PDFtriage
:`model family`,PDFtriage-augmented large language model
:resource,benchmark dataset
:resource,code and datasets
:task,textual information equivalence
:task,textual entailment
:task,fact-checking
:dataset,X-Parade
:task,cross-lingual information divergence analysis
:approach,token alignment from machine translation
:approach,textual entailment methods
:approach,prompting of large language models
:result,method performance comparison
:`data type`,audio-text pair
:task,data collection
:`data type`,text-only data
:`model family`,end-to-end automatic speech recognition
:`model family`,decoder-only language model
:task,speech-processing
:model,GPT-3
:model,PaLM
:`model family`,automatic speech recognition
:technique,CTC prediction compression
:`model component`,decoder
:`data type`,external text data
:dataset,LibriSpeech
:dataset,Switchboard
:`model family`,encoder-decoder automatic speech recognition
:value,computational efficiency
:task,user assistance
:risk,introduction of bias
:bias,LLM bias
:decision,consequential decisions
:bias,gender and ethnicity bias
:bias,age and beauty bias
:`model family`,autoregressive language model
:bias,social group bias
:bias,what is beautiful is good
:dataset,template-generated dataset
:task,bias measurement
:technique,templating
:`model family`,cutting-edge large language model
:task,fine-grained visual classification (FGVC)
:challenge,subtle inter-class discrepancies and large intra-class variations
:approach,prevailing FGVC approaches
:concept,uni-modal visual
:`model family`,pre-trained vision-language model
:task,high-level vision tasks
:task,FGVC tasks
:solution,multimodal prompting solution
:solution,MP-FGVC
:model,CLIP
:scheme,multimodal prompts scheme
:component,SSVP and DATP
:scheme,multimodal adaptation scheme
:concept,common semantic space
:module,vision-language fusion module (VLFM)
:task,FGVC
:strategy,two-stage optimization strategy
:dataset,FGVC datasets
:strategy,tool-interacting divide-and-conquer
:capability,answering complex multimodal multi-hop questions
:task,dividing multimodal multi-hop questions
:task,answering unimodal single-hop sub-questions
:set,predefined tools
:dataset,tool-interacting divide-and-conquer dataset
:process,finetuning large language models
:evaluation,effectiveness assessment
:dataset,complex question-answering datasets
:analysis,experimental analysis
:outcome,improvements over state-of-the-art
:quality,efficacy and generality
:`dataset type`,noisy web image-text dataset
:task,video retrieval
:`data type`,hand-curated paired text-video data
:setting,text-video retrieval with uncurated & unpaired data
:approach,In-Style
:`data type`,uncurated web videos
:concept,multiple text styles
:`training procedure`,multi-style contrastive training
:concept,model generalizability
:task,text-video retrieval
:task,zero-shot text-video retrieval
:technique,cross-lingual transfer
:task,translating training data
:task,structure extraction
:technique,label projection
:research,previous research in label projection
:quality,translation quality
:technique,CLAP
:technique,contextual translation
:requirement,translated labels presence
:task,creating pseudo-training data
:task,event argument extraction
:metric,F1-score
:task,question answering
:task,dialogue generation
:goal,efficient successful dialogue
:system,LLM-induced task-oriented dialogue system
:feature,direct reward
:approach,ProTOD
:`evaluation method`,goal-driven dialogue simulation
:metric,user satisfaction
:limitation,current metrics limitations
:dataset,MultiWOZ 2.1
:technique,knowledge editing
:performance,language model performance
:aspect,multi-lingual nature
:model,LaMDA
:`model family`,multi-lingual large language model
:effect,cross-lingual effect in knowledge editing
:dataset,cross-lingual synthetic dataset
:dataset,ZSRE
:language,English
:language,Chinese
:evaluation,cross-lingual knowledge editing evaluation
:aspect,"reliability, generality, locality, portability"
:behavior,inconsistent behaviors of edited models
:model,edited language model
:capability,open-ended question-answering
:application,AI assistants
:approach,monolingual tuning
:context,monolingual
:approach,multilingual tuning
:context,multilingual
:dataset,ALPaCA
:data,multilingual training data
:technique,low-rank adaptation
:technique,full-parameter training
:finding,multilingual tuning not crucial for English performance
:finding,multilingual tuning key for multilingual robustness
:context,multilingual environment
:model,multilingual instruction-tuned model
:model,monolingual models
:guidance,expanding language support through instruction tuning
:resource,computational resources
:task,generating complex structured data
:approach,structure-aware fine-tuning
:benchmark,Struc-Bench
:task,evaluating LLMs
:model,GPT-NeoX 20B
:model,Vicuna
:technique,FormatCOT
:task,generating format instructions
:model,LLaMA-7B
:map,ability map
:capability,handling complex structured outputs
:resource,code and models
:URL,https://github.com/gersteinlab/struc-bench
:issue,deployment cost
:technique,SortedNet
:concept,dynamic inference
:concept,network modularity
:technique,sorted fine-tuning (SoFT)
:task,generative NLP tasks
:technique,standard supervised fine-tuning (SFT)
:model,LLaMA 2 13B
:dataset,Stanford ALPACA dataset
:benchmark,PANDALM
:advantage,efficiency and performance
:benchmark,NLP benchmarks
:threat,adversarial attacks
:task,named entity recognition
:method,context-aware adversarial attack
:technique,perturbing informative words
:goal,generating adversarial examples
:method,candidate replacement
:experiment,effectiveness testing
:outcome,model deception
:process,RLHF
:technique,PPO training
:resource,large-scale computational resources
:implementation,efficient RLHF
:technique,Low-Rank Adaptation (LoRA)
:model,LLaMA 7B
:hardware,A100 GPU
:process,full model fine-tuning
:performance,better performance
:checkpoint,AlpacaFarm
:implementation,LoRA-based PPO
:component,KL regularization term
:regularization,KL penalty term
:performance,model performance
:regularization,Jensen-Shannon divergence
:resource,code and pretrained checkpoints
:goal,efficient RLHF research
:team,Nowj1
:event,Automated Legal Question Answering Competition (ALQAC) 2023
:approach,Nowj1 team's approach
:`model type`,classical statistical model
:task,document retrieval
:technique,learning-to-rank
:task,question-answering
:`sub-task`,sentence classification
:`sub-task`,answer extraction
:system,distinct systems for sub-tasks
:dataset,RMDM
:task,performance assessment of LLMs
:label,"real, mis, dis, mal"
:`information type`,"real information, misinformation, disinformation, mal-information"
:`data quantity`,"1,556 samples"
:`model family`,GPT-based model
:`model family`,BERT-based model
:problem,verifying electronic information
:goal,reliable AI models for legal applications
:data,large-scale data
:method,debiasing
:goal,useful information retention
:data,gender-related social bias data
:concept,"female, male, and stereotypical words"
:study,this study
:benchmark,benchmark datasets
:finding,effects of debiasing underestimated
:method,contrastive decoding
:publication,Li et al 2022
:attribute,simplicity and computational efficiency
:objective,maximizing weighted difference in likelihood
:model,LLaMA-65B
:benchmark,HellaSWAG
:benchmark,GSM8K
:error,abstract reasoning errors
:method,nucleus sampling and greedy decoding
:attribute,general-purpose text generation
:issue,gender bias
:method,content analysis
:perception,public perception of gender bias
:`cultural context`,US-based
:model,ERNIE
:`cultural context`,China-based
:`bias type`,implicit gender bias
:`bias type`,explicit gender bias
:impact,culture on gender bias
:recommendation,governance recommendations
:task,evaluating outputs
:tool,ChainForge
:capability,prompt engineering and hypothesis testing
:feature,graphical interface for comparison
:task,"model selection, prompt template design, hypothesis testing"
:user,academics and online users
:mode,prompt engineering and LLM hypothesis testing
:approach,"opportunistic exploration, limited evaluation, iterative refinement"
:capability,understanding human instructions
:challenge,complex instructions
:feature,complexity features
:benchmark,existing benchmarks
:capability,understanding complex instructions
:benchmark,CELLO
:criteria,CELLO evaluation criteria
:experiment,performance comparison
:resource,CELLO resources
:task,robotic cooking
:process,task planning
:artifact,task tree
:data,recipe instructions
:concept,subtask dependencies
:process,task tree retrieval
:issue,LLM output uncertainty
:evaluation,performance evaluation
:field,autonomous robotics
:capability,semantic concept grounding
:task,natural language task interpretation
:model,hierarchical metric-semantic model
:representation,scene graph
:environment,environment representation
:representation,Linear Temporal Logic automaton
:contribution,main contribution
:method,hierarchical LTL planning with LLM guidance
:domain,hierarchical planning domain
:function,LLM heuristic function
:function,LTL heuristic function
:attribute,optimality
:task,natural language task planning
:problem,sequential recommendation
:field,AI-based applications
:approach,LLM-based sequential recommendation
:model,BERT4Rec
:technique,LLM embeddings
:metric,NDCG
:approach,LLM embeddings for recommendations
:performance,competitive performance
:resource,code and data
:value,reproducibility
:field,IT operations
:task,data management and analysis
:technique,natural language processing
:task,NLP tasks
:model,OWL
:dataset,OWL-instruct
:strategy,mixture-of-adapter
:benchmark,OWL-bench
:benchmark,IT-related benchmarks
:goal,fixing software bugs
:technique,template-based automated program repair
:challenge,selecting appropriate donor code
:model,GAMMA
:technique,cloze task
:technique,fix templates
:model,TBAR
:model,Recoder
:dataset,Defects4J-v2.0 and QuixBugs
:model,CodeBERT-based GAMMA
:model,ChatGPT-based GAMMA
:trend,using pre-trained models for patch generation
:task,automated short answer grading
:goal,educational scalability
:trend,commoditization of LLMs
:dataset,SciEntsBank and Beetle
:task,grading alignment with reference answer
:`model family`,hand-engineered model
:`model family`,specialized trained large language model
:condition,withholding reference answer
:`communication method`,basic messaging and phone calls
:issue,communication inefficiencies
:solution,LLM-powered communication system
:task,patient-provider communication
:population,older adults
:population,healthcare providers
:system,Talk2Care
:insight,interview study insights
:interface,LLM-powered VA interface
:dashboard,LLM-based dashboard
:population,older adults and healthcare providers
:field,healthcare and interpersonal communication
:domain,healthcare
:stakeholders,"clinicians, researchers, patients"
:`evaluation method`,medical exam questions
:`interaction type`,patient-doctor interactions
:phenomenon,patient self-diagnosis
:outcome,misdiagnosis
:`evaluation method`,modified medical board exam questions
:issue,errors in self-diagnosis
:`research area`,multi-document news summarization
:task,agreement-based summarization
:task,summarizing diverse information
:dataset,DiverseSumm
:analysis,position and verbosity biases
:metric,LLM-based evaluation metrics
:assessment,human assessments
:challenge,limited coverage in summarization
:concept,spurious correlations
:task,natural language understanding
:`research question`,reducing spurious correlations
:framework,Soft Label Encoding (SoftLE)
:model,teacher model
:technique,hard label training
:technique,soft label training
:technique,dummy class encoding
:model,student model
:goal,out-of-distribution generalization
:goal,in-distribution accuracy
:task,spoken semantic parsing
:data,speech-transcript-semantic parse data
:problem,data acquisition cost
:method,using unpaired text
:technique,joint audio text
:technique,text-to-speech
:dataset,STOP
:`performance metric`,exact match
:data,unpaired text
:model,LaMDA 2.0
:technique,using generated text
:system,artificial intelligence systems
:concept,language representations
:concept,vector spaces
:process,mapping
:concept,parameters
:process,gradient optimization
:concept,black box system
:concept,neuronal activity topology
:concept,fairness metric
:field,social psychology
:concept,simplicial complex
:concept,manifold shape
:concept,sentence manifold
:technique,heat map visualization
:concept,submanifolds
:concept,moral dimension
:factor,model size and training dataset
:issue,lack of transparency
:process,training data creation
:task,cleaning and deduplication
:community,NLP community
:scenario,multilingual learning
:dataset,multilingual text datasets
:issue,inadequate data collection and cleaning
:dataset,Culturax
:resource,multilingual dataset
:platform,Huggingface
:technique,formal property verification
:task,bug finding
:notation,SystemVerilog Assertions
:technique,abstraction level raising
:task,SVA generation
:framework,FPV-based evaluation framework
:attribute,correctness and completeness of SVA
:framework,AutoSVA
:`use case`,FPV coverage evaluation
:`use case`,RTL generation from SVA
:artifact,correct SVA
:bug,RISC-V CVA6 core bug
:field,text-to-image generation
:trend,rapid evolution
:challenge,synthesis and manipulation of multiple entities
:task,adhering to relational constraints
:operation,progressive synthesis and editing
:task,entity incorporation with constraints
:model,pre-trained text-to-image diffusion model
:task,handling multiple entities
:task,decomposing text descriptions
:framework,"Stimulus, Response, and Fusion (SRF)"
:task,semantic operations execution
:technique,latent region manipulation
:task,object synthesis
:field,software engineering
:tool,AI programming assistant
:technique,conversational programming
:tool,GitHub Copilot X
:challenge,gap in system understanding
:approach,Programming with Representations (PWR)
:study,in-lab task-centered study
:outcome,improved understandability
:group,expert programmers
:group,intermediate programmers
:technique,natural language-based development with large language models
:`research field`,graphic layout generation
:outcome,user engagement and information perception
:method,numerical optimization
:task,layout generation
:aspect,quantitative
:aspect,semantic information
:model,LayoutNuwa
:approach,code generation
:approach,Code Instruct Tuning (CIT)
:module,Code Initialization (CI)
:module,Code Completion (CC)
:module,Code Rendering (CR)
:URL,https://github.com/projectnuwa/layoutnuwa
:activity,"model training, inference, and deployment"
:activity,model compression
:technique,manual pruning
:issue,complex optimization pipeline and difficulty in retaining capabilities
:approach,novel pruning
:model,accuracy predictor
:activity,search space optimization
:dataset,WikiText-2
:metric,perplexity reduction
:dataset,PTB
:metric,MMLU average accuracy
:metric,accuracy increase
:`model type`,text language model
:capability,zero-shot generalization
:field,speech processing
:`task type`,specific tasks
:issue,lack of standardized benchmarks
:goal,fair comparison
:benchmark,Dynamic-SuPERB
:`model type`,universal speech model
:`evaluation instance`,55 evaluation instances
:platform,comprehensive evaluation platform
:approach,benchmark baselines
:`model type`,multimodal encoder
:study,ablation study
:goal,performance improvement
:process,continued pre-training
:knowledge,domain knowledge
:method,reading comprehension-based training
:process,human learning
:data,reading comprehension texts
:domain,"biomedicine, finance, and law"
:model,7B language model
:model,BloombergGPT-50B
:`model family`,domain-specific reading comprehension-based large language model
:benchmark,general benchmarks
:`model family`,general large language model
:domain,more domains
:resource,"model, code, and data"
:url,https://github.com/microsoft/lmops
:evaluation,human evaluation
:preference,human evaluators
:output,LLM-generated summaries
:quality,factual consistency
:issue,extrinsic hallucinations
:performance,LLMs in summarization tasks
:benchmark,reference summaries
:field,text summarization
:direction,novel dataset creation
:direction,reliable evaluation methods
:task,machine reading comprehension
:capability,decoupling and modeling multi-turn dialogues
:phenomenon,topic shift in dialogue
:task,dialogue modeling
:technique,topic modeling
:task,dialogue reading comprehension
:technique,dialogue segmentation algorithm
:task,topic-aware language processing
:task,dialogue comprehension
:system,clustering system with self-training auto-encoder
:task,in-domain topic detection and location
:dataset,constructed datasets
:model,Topic-Aware Dual-Attention Matching (TADAM) Network
:task,multi-turn dialogue response selection
:technique,dual cross-attention
:study,empirical studies
:outcome,improvements over baselines
:issue,discriminatory social biases
:task,natural language inference (NLI)
:issue,biased inferences
:method,bias evaluation
:data,evaluation data
:measure,bias measure
:method,meta-evaluation
:concept,multilingual applicability
:issue,bias tendency
:achievement,first evaluation datasets and bias measure for NLI task in Japanese and Chinese
:action,citing
:purpose,acknowledging and crediting original sources
:citation,in-text citation
:motivation,various motivations and purposes
:information,fine-grained citation information
:relationship,inter-document relationships
:information,citation linkage knowledge
:capability,text representation in LLMs
:task,in-text citation analysis
:subtask,"citation classification, citation-based summarization, citation recommendation"
:research,leveraging citation linkage knowledge
:method,contemporary methods
:field,LLMs and citation analysis
:opportunity,further investigation
:paradigm,neuro-symbolic paradigm
:approach,Visual Programming
:task,compositional visual tasks
:approach,task-specific supervised learning models
:limitation,non-differentiability of Visual Programming
:`performance issue`,errors in sub-modules
:method,VisualProg Distiller
:model,end-to-end model
:evaluation,experimental evaluations
:dataset,large-scale dataset for GQA task
:task,language tasks
:task,radiology report generation
:solution,R2GenGPT
:module,visual alignment module
:capability,processing image information
:benchmark,state-of-the-art performance
:attribute,training efficiency
:technique,delta tuning
:phenomenon,transformation
:content,AI-generated content (AIGC)
:study,bias investigation
:source,The New York Times
:source,Reuters
:bias,racial bias
:bias,lowest level of bias
:task,code synthesis
:technology,transformer-based large language model
:issue,vulnerabilities in synthesized code
:approach,vulnerability-constrained decoding
:model,GPT-J
:data,Ethereum smart contracts
:task,vulnerability labeling
:`evaluation metric`,BLEU score
:result,vulnerability identification
:result,vulnerability avoidance
:data,vulnerable smart contracts
:domain,Ethereum blockchain smart contracts
:`case study`,smart contract security
:task,exact transcription
:`interaction type`,text-prompt-based interactions
:capability,speech understanding and reasoning
:`model type`,instruction-following speech recognition
:capability,speech understanding and execution
:model,Listen-Attend-Spell
:capability,multitask speech recognition
:feature,selective transcription
:`training approach`,instruction-following training
:`model type`,speech foundation model
:field,video representation learning
:task,self-supervised video object localization
:method,slot attention
:task,object localization in videos
:task,reading localized semantic information
:task,video object localization
:approach,unsupervised approach
:benchmark,regular video benchmarks
:data,user-generated text
:attribute,semantic complexity
:task,human annotation
:attribute,specialized domains
:field,health NLP
:solution,low-resource NLP solutions
:problem,limited-data problems
:representation,abstract meaning representation (AMR) graphs
:task,low-resource health NLP tasks
:data,online health texts
:experiment,performance improvement on health NLP tasks
:technique,semantic graph embeddings
:approach,AMR-augmented text classification
:attribute,task agnostic
:measure,textual complexity measures
:task,modeling complex texts
:analysis,error analysis
:`model family`,AMR-infused language model
:community,AI research community
:goal,generalization in novel environments
:approach,empirical risk minimization
:concept,context
:concept,environment
:algorithm,in-context risk minimization (ICRM)
:goal,domain generalization
:goal,test environment risk minimization
:method,automatic concept hierarchy construction
:task,constructing concept hierarchies
:corpus,political texts
:opportunity,political insights
:task,automated speaker attribution
:task,semantic role labeling
:field,computational text analysis
:model,LLaMA 2
:strategy,QLORA
:event,Germeval 2023 shared task
:task,speaker attribution
:corpus,German parliamentary debates
:timeframe,2017-2021
:field,political discourse analysis
:capability,automating speaker attribution
:`research domain`,LLM agents
:capability,world knowledge and general reasoning
:field,robotics and planning
:concept,robot dos
:concept,robot don'ts
:concept,practical robot usage
:standard,ISO 61508
:concept,safe robot deployment
:module,queryable safety constraint module
:environment,collaborative environment
:logic,Linear Temporal Logic (LTL)
:capability,safety constraint handling
:environment,VirtualHome
:experiment,safety constraint system effectiveness
:outcome,adherence to safety constraints
:data,internet-scale data
:field,robotics
:challenge,grounding in physical world
:challenge,dynamic robot motion generation
:paradigm,few-shot prompts for robotics
:method,autoregressive low-level control command generation
:experiment,robotic motion control
:role,low-level feedback controller
:URL,https://prompt2walk.github.io/
:task,complex scheduling
:concept,multi-agent collaboration
:benchmark,multi-agent collaboration infrastructure
:infrastructure,MindAgent
:capability,planning and coordination in gaming
:framework,existing gaming framework
:scenario,CuisineWorld
:benchmark,multi-agent collaboration efficiency
:metric,CoS
:scenario,CuisineWorld VR version
:domain,Minecraft gaming
:skill,scheduling and coordination
:concept,patent claim scope
:metric,reciprocal of self-information
:metric,self-information
:`model output`,probability of occurrence
:theory,information theory
:concept,informative concept
:model,GPT-2
:model,simplest language model
:metric,reciprocal of word or character count
:application,patent claims analysis
:method,ad hoc tests
:`model family`,frequency-based language model
:metric,character count
:metric,word count
:aspect,commonsense reasoning
:activity,human communication
:field,conversational AI
:method,SyndiCom
:task,commonsense in dialogue response generation
:component,dataset
:resource,commonsense dialogues
:feature,natural language feedback
:procedure,two-step procedure
:model,response generation model
:resource,SyndiCom code and dataset
:paradigm,pre-training and fine-tuning
:task,long-tailed recognition tasks
:`model family`,large vision-language model
:method,full fine-tuning
:issue,overfitting
:method,classifier fine-tuning
:method,PEL
:technique,semantic-aware classifier initialization
:benchmark,state-of-the-art approaches
:resource,PEL source code
:URL,https://github.com/shijxcs/pel
:`model type`,multimodal foundation model
:capability,vision and vision-language
:`model type`,general-purpose assistant
:`research area`,multimodal foundation models pre-trained for specific purposes
:topic,vision backbone learning and text-to-image generation
:`research area`,exploratory open research areas
:topic,"unified vision models, end-to-end multimodal LLM training, and chaining multimodal tools with LLMs"
:`model type`,unified vision model
:task,end-to-end multimodal training and chaining multimodal tools
:audience,"researchers, graduate students, and professionals"
:paper,comprehensive survey of multimodal foundation models
:community,computer vision and vision-language multimodal
:knowledge,basics and recent advances in multimodal foundation models
:organization,OpenAI
:entity,"society, research, and education"
:task,domain-specific answering capabilities
:assessment,systematic empirical assessment
:task,answering questions
:data,594 questions
:institution,Delft University of Technology
:evaluation,quality assessment
:result,mostly correct answers
:trend,decreasing rating with complexity
:trend,decreasing rating beyond scientific knowledge
:framework,Smart-LLM
:task,embodied multi-robot task planning
:process,task decomposition
:process,coalition formation
:process,task allocation
:dataset,benchmark dataset for multi-robot task planning
:task,multi-robot task planning validation
:evaluation,simulation and real-world scenarios
:result,multi-robot task plans
:resource,"experimental videos, code, and datasets"
:URL,https://sites.google.com/view/smart-llm/
:`model family`,fine-tuned large language model
:task,generating personalized impressions for PET reports
:number,12 language models
:data,corpus of PET reports
:algorithm,teacher-forcing
:number,"37,370 retrospective PET reports"
:evaluation,30 evaluation metrics
:standard,quality scores from NM physicians
:metric,domain-adapted BARTScore
:standard,physician preferences
:metric,PEGASUSScore
:model,fine-tuned PEGASUS
:standard,top large language model
:impression,PEGASUS-generated impressions
:standard,clinical acceptability
:impression,personalized impressions by PEGASUS
:standard,impressions by other physicians
:task,expediting PET reporting
:course,software security course
:task,vulnerability identification and fixing
:value,information support
:task,penetration testing and fixing
:problem,motion planning
:task,mobile robot sub-tasks
:language,natural language
:predicate,NL-based atomic predicates
:goal,robot plan satisfaction
:predicate,NL-based atomic propositions
:challenge,reasoning about correctness
:task,LTL-encoded tasks
:system,HERACLES
:tool,existing tools
:tool,automata theory
:task,NL-specified sub-task
:task,robot plan design
:method,conformal prediction
:goal,mission satisfaction
:experiment,comparative experiments
:task,mobile manipulation tasks
:URL,ltl-llm.github.io
:robot,robot
:environment,various environments
:task,object goal navigation (OGN)
:environment,indoor environments
:environment,outdoor environments
:feature,clear spatial delineations
:human,human
:ability,reasoning about the unseen
:task,outdoor object goal navigation
:capability,hallucinating possible futures
:metric,computationally aware success metric
:agent,simulated drone
:agent,physical quadruped
:approach,naive LLM-based approach
:formalism,new formalism for outdoor OGN
:process,language model training
:technique,instruction-tuning
:technique,reinforcement learning from human feedback
:effect,performance degradation
:hypothesis,task inference skewing
:phenomenon,task inference by language models
:technique,conjugate prompting
:goal,recovering pretrained capabilities
:distribution,fine-tuning distribution
:technique,prompt translation
:task,offline handwriting recognition
:technology,deep learning architectures
:technique,post-processing techniques
:task,restricting predicted words
:context,out-of-vocabulary words
:task,comparing handwriting image to text
:model,unrestricted binary classifier
:component,HWR feature extractor
:component,multimodal classification head
:technology,generative adversarial network
:data,synthetic data
:metric,average precision increase
:application,human-in-the-loop automation
:model,RadOnc-GPT
:field,radiation oncology
:data,radiation oncology patient records
:institution,Mayo Clinic in Arizona
:technique,instruction tuning
:evaluation,ROUGE scores
:field,specialized healthcare
:value,clinical relevance
:task,specialized tasks in radiation oncology
:value,semantic and clinical accuracy
:task,evaluation of semantic and clinical accuracy
:utterance,contextual utterance
:resource,annotated examples
:utterance,non-contextual utterance
:parser,semantic parser
:paradigm,parse-with-utterance-history
:task,conversational semantic parsing
:paradigm,parse-with-reference-program
:paradigm,parse-then-resolve
:paradigm,rewrite-then-parse
:dataset,SMCalFlow-EventQueries
:task,cross-paradigm comparison
:criteria,holistic evaluation
:`data type`,video data
:task,event search
:goal,privacy
:approach,existing video search
:method,manual inspection
:`model family`,deep learning model
:method,proposed video search
:`model family`,vision and language model
:field,formal methods
:algorithm,text-based event description to LTL_f mapping
:algorithm,automaton construction encoding video information
:method,formal verification
:specification,LTL_f
:`analysis type`,qualitative and quantitative analysis
:capability,video-searching
:`performance metric`,precision
:challenge,aligning LLMs with human values
:issue,instabilities in RLHF training
:issue,reward hacking
:issue,catastrophic forgetting
:innovation,advantage model
:technique,RLHF stabilization
:innovation,selective rehearsal
:outcome,increased stability and performance in RLHF training
:concept,human-centric design
:capability,advanced AI capabilities
:entity,autonomous vehicle
:activity,interacting and adapting to passenger desires
:capability,natural language capabilities and contextual understanding
:framework,novel framework for autonomous vehicles
:goal,seamless integration of advanced language and reasoning capabilities
:outcome,revolutionizing autonomous vehicle operation
:goal,safer and more efficient autonomous driving technologies
:document,privacy policy
:stakeholder,users
:issue,verbosity
:behavior,agree without reading
:risk,privacy leakage and legal issues
:task,text analysis
:framework,PolicyGPT
:dataset,website privacy policies
:profession,legal experts
:dataset,mobile application privacy policies
:task,manual annotation
:`performance metric`,accuracy rate
:`model type`,baseline models
:task,generating natural language datasets
:framework,large language model framework for NLIs
:application,natural language interfaces for data visualization
:technique,guided discovery
:technique,large language model prompting
:technique,score-based paraphrasing
:aspect,syntactic diversity
:collection,chart collection
:metric,accuracy
:quality,diversity
:resource,codes and chart collection
:URL,https://github.com/hyungkwonko/chart-llm
:application,various applications
:activity,harmful or illegal activities
:`safety measure`,LLM safety measures
:risk,harmful content production
:attack,adversarial jailbreak attack
:framework,GPTFuzz
:concept,jailbreak fuzzing framework
:process,jailbreak template generation
:component,seed selection strategy
:component,mutate operators
:component,judgment model
:model,LLaMA-2
:goal,LLM robustness and safety
:feature,plugin ecosystem
:capability,LLM platform capabilities
:plugin,third-party plugin
:entity,arbitrary third parties
:interaction,natural language interface
:entity,plugins
:framework,"security, privacy, and safety framework"
:platform,plugin-integrated large language model
:taxonomy,attack taxonomy
:platform,OpenAI plugin ecosystem
:challenge,novel challenges
:platform,LLM-based computing platform
:recommendation,improvement recommendations
:task,speech emotion recognition
:technique,speech pre-trained model
:model,data2vec
:ability,representation ability
:task,emotionally congruent text generation
:model,Azure TTS
:task,synthetic emotional speech data generation
:technique,text prompt design
:dataset,IEMOCAP
:method,random mixing
:method,adversarial training
:method,transfer learning
:method,curriculum learning
:task,natural language tasks
:issue,accessibility and language limitation
:model,Baichuan 2
:`model family`,multilingual language model
:attribute,model parameters
:data,2.6 trillion tokens
:benchmark,public benchmarks
:domain,vertical domains
:concept,training dynamics
:`model family`,multimodal large language model
:tool,EMT
:task,image classification
:procedure,current MLLM fine-tuning
:agent,intelligent agent
:setting,safety-critical settings
:capability,explanation of reasoning
:model,deep neural network
:behavior,agent behavior
:approach,natural language explanation generation
:representation,compact behavior representation
:capability,user interaction
:study,user study
:interaction,beneficial interaction
:phenomenon,unsupported and unfalsifiable claims
:concept,worldview
:task,characterizing and summarizing claims
:concept,complexity
:task,distilling narratives from claims
:concept,narrative distillation
:dataset,crowdsourced dataset of controversial topics
:data,"120k arguments, claims, and comments"
:task,claim synthesis
:task,narrative classification
:task,stance and aspect inference
:application,visual dialogue
:context,multimodal context
:phenomenon,human mistakes
:research,prior research
:assumption,perfect human answers
:task,pointing-human-mistake
:mistake,human answer mistakes
:factor,question type and QA turn
:model,simple MLP model
:model,visual language model
:metric,model's accuracy
:constraint,pre-defined context length
:scenario,long input scenarios
:method,full-length fine-tuning
:issue,intensive training cost
:method,positional skip-wise training
:goal,efficient context window extension
:technique,long input simulation
:capability,128k token context
:`model family`,RoPE-based large language model
:capability,infinite length support
:progress,efficient inference
:capability,context window scaling
:task,dialogue response generation
:attribute,informative and engaging responses
:system,knowledge-grounded dialogue system
:issue,alignment with human-preferred qualities
:issue,lack of coherence
:observation,alternative generated responses
:process,decoding process
:framework,Polished & Informed Candidate Scoring (PiCK)
:goal,faithful and relevant responses
:attribute,faithfulness and relevance in responses
:attribute,system performance
:resource,PiCK implementation
:URL,https://github.com/bryanwilie/pick
:application,online applications
:system,recommender systems
:concept,dynamic user interests
:method,sequential modeling methods
:concept,contextual information
:concept,domain-specific knowledge
:paradigm,new sequential recommendation paradigm
:problem,limitations in capturing contextual information
:model,LANCER
:capability,semantic understanding
:characteristic,human-like recommendations
:task,sequential recommendation tasks
:resource,experimental codes
:method,learnersourcing
:activity,generating and sharing learning resources
:task,creating multiple-choice questions
:task,creating explanations
:challenge,crafting effective explanations
:group,students
:framework,self-reinforcement large language model
:task,generating and evaluating explanations
:component,three modules
:process,student explanation composition
:evaluation,human subject-matter expert comparison
:model,VICUNA-13B
:attribute,creativity
:evaluation,human expert ranking
:advancement,enriching the learnersourcing experience
:advancement,enhancing large language models in education
:application,educational applications
:aspect,controllable text generation
:field,natural language generation
:method,regular expression instruction
:challenge,applying methods to additional constraints
:mechanism,instruction-based mechanism
:constraint,diverse constraints
:constraint,fine-grained controllable generation constraints
:`model family`,medium-scale language model
:constraint,various constraint combinations
:approach,straightforward approach
:outcome,high success rates and adaptability
:benchmark,previous baselines
:task,speaker diarization
:community,speech processing research community
:approach,mainstream speaker diarization
:feature,voice characteristics
:signal,speech signal
:content,speech content
:approach,novel approach for speaker diarization
:module,spoken language understanding module
:feature,speaker-related semantic information
:technique,pairwise constraints
:framework,novel framework for speaker diarization
:system,speaker diarization system
:approach,proposed approach for speaker diarization
:dataset,public dataset
:capability,zero-shot learning
:knowledge,linguistic knowledge
:architecture,hybrid connectionist temporal classification and attention
:model,end-to-end ASR
:model,LLaMa2
:component,decoder front-end
:process,CTC decoding
:output,ASR hypothesis
:technique,sequence generation
:technique,rescoring
:integration,LLM and ASR
:task,video-to-audio generation
:challenge,temporal synchronization
:system,FoleyGen
:paradigm,language modeling
:tool,neural audio codec
:task,audio token generation
:problem,audio-visual misalignment
:mechanism,visual attention mechanisms
:evaluation,visual encoder evaluation
:task,visual feature extraction
:dataset,VGGSound
:evaluation,objective metrics and human evaluations
:benchmark,previous systems
:task,multilingual scientific documents similarity measurement
:`model family`,multilingual similarity measurement model
:task,finding related scientific works
:dataset,OpenMSD
:data,multilingual scientific documents and citation pairs
:`model family`,science-specialized language model
:strategy,deriving related paper pairs
:data,citation data
:technique,generative language model enrichment
:task,improving non-English paper representation
:model,best model
:benchmark,strong baselines
:`attack type`,model leeching
:process,knowledge distillation
:model,ChatGPT-3.5-Turbo
:metric,exact match (EM) similarity
:metric,SQuAD EM and F1 accuracy scores
:concept,adversarial attack transferability
:model,extracted model
:`attack type`,ML attack staging
:metric,attack success rate
:concept,relevance label
:concept,search result value
:method,user feedback
:concept,user preferences
:method,third-party labelling
:task,relevance labelling
:risk,low-quality data
:actor,third-party labeller
:method,real user study
:goal,label quality improvement
:method,labeller education
:data,first-party gold data
:actor,human labeller
:technique,prompt tuning
:data,high-quality gold labels
:actor,third-party worker
:system,search ranker
:domain,financial NLP tasks
:framework,CFGPT
:dataset,CFData
:model,CFLLM
:framework,CFApp
:data,pre-training and fine-tuning datasets
:data,pre-training dataset
:metrics,584M documents and 141B tokens
:data,supervised fine-tuning dataset
:metrics,1.5M instruction pairs and 1.5B tokens
:model,InternLM-7B
:value,democratizing access
:method,online scraping
:resource,corpora
:method,document translation
:limitation,lack of lexical diversity and cultural relevance
:`case study`,Indonesian local languages
:method,dataset construction
:method,paragraph writing by native speakers
:resource,datasets
:benchmark,DatasetName
:goal,language inclusivity
:dataset,NusaWrites
:`GitHub repository`,https://github.com/indonlp/nusa-writes
:concept,predictive model
:concept,lossless compressor
:community,machine learning community
:capability,predictive capabilities
:concept,strong compressor
:approach,viewing prediction as compression
:problem,prediction problem
:`model family`,large foundation model
:capability,compression capabilities
:model,Chinchilla 70B
:dataset,ImageNet and LibriSpeech
:concept,prediction-compression equivalence
:`model type`,conditional generative model
:concept,"scaling laws, tokenization, in-context learning"
:compressor,domain-specific compressor
:issue,data contamination
:process,model evaluation
:process,contamination analysis
:method,existing contamination analysis
:resource,training data
:activity,model auditing
:method,quantifying contamination
:metric,perplexity
:analysis,this analysis
:phenomenon,memorisation
:benchmark,reading comprehension
:benchmark,summarisation
:benchmark,multiple choice
:technique,inference-time prompting
:approach,EchoPrompt
:`model family`,causal language model
:task,numerical reasoning
:task,logical reasoning
:model,Codex-DaVinci-002
:study,ablation studies
:technique,baseline prompting strategies
:interaction,multi-turn interactions
:`evaluation protocol`,current evaluation protocols
:`evaluation metric`,benchmark performance
:benchmark,MINT
:capability,multi-turn task solving
:`evaluation framework`,provided by MINT
:capability,tool usage and language feedback processing
:dataset,established evaluation datasets
:technique,Supervised Instruction-Finetuning (SIFT)
:technique,Reinforcement Learning from Human Feedback (RLHF)
:community,open-source community
:community,academic community
:tool,LLM-based tools
:community,instructors
:effort,creation of LLM-based tools
:gap,lack of comprehensive user studies
:paper,current paper
:method,surveys and interviews
:location,undergraduate engineering universities in India
:insight,"current usage patterns, benefits, threats, challenges"
:insight,recommendations for enhancing adoption
:discussion,practical implications of LLMs in education
:sector,undergraduate engineering education
:model,OpenBA
:`model family`,bilingual asymmetric seq2seq model
:technique,three-stage training strategy
:benchmark,various benchmarks
:process,pre-training an analogous model
:task,downstream tasks
:code,OpenBA code
:library,HuggingFace Transformers
:resource,OpenBA checkpoints
:application,real-world applications
:task,generalization to new domains
:data,target-domain data
:task,finetuning ASR systems
:strategy,adapting ASR models without target-domain data
:task,adapting ASR models
:pipeline,data synthesis
:strategy,in-context instruction finetuning
:dataset,SLURP
:metric,word error rate
:technology,AI accelerators
:process,designing AI accelerators
:characteristic,labor- and time-intensive
:tools,design exploration and automation tools
:need,human involvement
:barrier,hardware expertise requirement
:community,non-experts
:task,automating AI accelerator design
:framework,GPT4AIGChip
:goal,democratizing AI accelerator design
:task,AI accelerator design
:quality,high-quality AI accelerator design
:technology,LLM-powered automated AI accelerator generation
:`insights and framework`,this work's insights and GPT4AIGChip
:goal,innovations in LLM-powered design automation tools
:task,music understanding and generation
:method,pre-training for symbolic melody generation
:concept,"multi-scale, multi-dimensional structural information"
:discrepancy,domain knowledge discrepancy between text and music
:dataset,large-scale symbolic melody datasets
:framework,MelodyGLM
:task,generating melodies with long-term structure
:strategy,melodic n-gram and long span sampling strategies
:task,modeling local and global structures in melodies
:dataset,MelodyNet
:task,large-scale pre-training and domain-specific n-gram lexicon construction
:benchmark,standard and previous pre-training methods
:task,melody continuation
:benchmark,human-composed melodies
:`communication skill`,social communication
:concept,metaphor and sarcasm
:syndrome,Asperger syndrome
:concept,sarcasm comprehension
:test,metaphor and sarcasm understanding test
:disorder,attention-deficit/hyperactivity disorder
:concept,metaphor comprehension
:approach,alternative approach
:`brain region`,amygdala
:issue,low-resource corpora
:methodology,crosslingual annotation projection
:task,generating annotated datasets
:approach,language agnostic BERT-based
:task,increasing low-resource corpora
:evaluation,quantitative and qualitative
:task,semi-automatic data generation
:quality,high accuracy
:corpus,FRaSMeD
:data,synthetic clinical cases
:platform,Zenodo
:attribute,largest open annotated corpus in French
:`research method`,thematic analysis
:`research type`,qualitative research
:characteristic,labor-intensive procedures
:`research activity`,semi-structured interviews
:`participant group`,17 participants
:`research activity`,pilot study
:`participant group`,4 participants
:`participant group`,qualitative analysts
:framework,cueing frameworks
:insight,connection between AI and qualitative research
:`stakeholder group`,academics and professionals
:trend,multimodal machine learning
:task,interpreting complex contextual relationships
:approach,model-agnostic
:task,generating detailed textual descriptions
:task,reasoning about multimodal descriptions
:model,BLIP-2
:modality,visual
:model,Whisper
:modality,aural
:model,ImageBind
:task,zero-shot multimodal classification
:benchmark,UCF-101
:task,video understanding
:benchmark,Kinetics
:`research direction`,multimodal classification
:task,holistic video understanding
:domain,weather forecasting
:concept,digital twin of the earth
:group,stakeholders
:approach,physics-informed machine learning
:`model architecture`,graph neural network
:concept,generative artificial intelligence
:concept,generalizable AI
:`model type`,AI foundation model
:task,weather and climate predictions
:task,downscaling
:phenomenon,wildfires
:phenomenon,hurricanes
:phenomenon,atmospheric rivers
:`model type`,weather foundation model
:field,meteorology
:framework,natural language embedded programs
:task,math and symbolic reasoning
:task,instruction following
:artifact,Python programs
:concept,data structures with natural language representations
:tool,Python interpreter
:value,interpretability and verification
:dataset,SlimPajama
:dataset,RedPajama
:metric,627B tokens
:research,SlimPajama-DC
:observation,global deduplication vs local deduplication
:observation,proportions of high-quality/highly-deduplicated datasets
:model,1.3B Cerebras-GPT
:model,1.3B model trained on RedPajama
:infrastructure,Cerebras 16x CS-2 cluster
:discovery,increasing data diversity post-global deduplication
:model,7B model
:resource,SlimPajama-DC datasets
:website,Hugging Face
:task,zero-shot cross-lingual transfer
:field,multilingual NLP
:`model family`,multilingual pretrained language model
:resource,parallel corpora
:resource,bilingual dictionaries
:data,annotated alignment data
:method,SALT
:technique,code-switching
:technique,embedding mixup
:technique,self-augmentation
:benchmark,XNLI
:benchmark,PAWS-X
:resource,SALT code
:URL,https://github.com/luka-group/salt
:capability,intelligent agents
:process,clinical decision-making
:`evaluation framework`,AI-Structured Clinical Examinations (AI-SCI)
:task,real-world clinical tasks
:technology,self-driving cars
:method,high-fidelity simulations
:interaction,user-LLM interaction in clinical workflow
:interaction,dynamic interactions of multiple LLMs
:goal,robust real-world clinical evaluations
:attribute,performance and generalization
:method,contextualizing speech recognition
:task,speech recognition
:system,decoder-only transcription system
:input,audio features and text tokens
:concept,unstructured contextual information
:`performance metric`,Word Error Rate (WER) reduction
:system,contextualized RNN-T system
:`performance metric`,Word Error Rate (WER) improvement
:data,larger speech dataset
:technique,adapters
:capability,contextualized speech recognition
:`model type`,non-autoregressive model
:attribute,lower inference time
:attribute,good transcription accuracy
:`model type`,streaming non-autoregressive model
:attribute,low-latency
:`model type`,semi-autoregressive model
:subnetwork,language model subnetwork
:algorithm,greedy decoding algorithm
:issue,insertion and deletion errors
:attribute,inference time
:`model type`,autoregressive model
:attribute,lower latency
:concept,complex text style transfer
:work,current work
:dataset,complex text datasets
:data,rephrased sentences and Genshin Impact sentences
:issue,LLM drawbacks
:`model family`,small model
:model,T5-3B
:method,automated evaluation of text generation quality
:model,current approach
:performance,state-of-the-art few-shot text style transfer
:method,existing methods
:language,Russian
:collection,Russian Transformer language models
:model,RuBERT
:`model type`,encoder
:model,RuRoBERTa
:model,RuElectra
:model,RuGPT-3
:model,RuT5
:`model type`,encoder-decoder
:model,FRED-T5
:task,Russian natural language understanding and generation
:task,data analysis
:`skill set`,"domain knowledge, statistical expertise, programming skills"
:tool,AI assistant
:issue,misalignment of AI-assistant responses
:outcome,incorrect conclusions
:task,validating AI assistance
:quality,crucial and challenging
:study,qualitative user study
:task,verification of AI-generated analysis
:tool,design probe
:participant,analysts
:method,verification workflows
:finding,patterns of verification workflows
:background,"analysts' programming, analysis, AI backgrounds"
:research,improving AI analysis assistant experiences
:goal,better AI assistance
:task,semi-structured document information extraction
:workflow,document processing
:document,visually rich document (VRD)
:obstacle,absence of layout encoding
:obstacle,lack of grounding mechanism
:methodology,Language Model-based Document Information Extraction and Localization (LMDX)
:task,document information extraction
:model,PALM 2-S
:benchmark,VRDU
:benchmark,CORD
:limitation,limited context window
:model,dense retrieval model
:model,OPT
:task,intent classification
:performance,state of the art
:task,fine-grained sentiment classification
:performance,fine-tuned performance
:`model scale`,larger models
:`model behavior`,use of in-context examples
:finding,varying importance of factors
:task,planning and reasoning tasks
:task,automated design of quantitative factors
:goal,alpha returns
:data,daily candlestick data
:asset,Shanghai Stock Index
:output,GPT-4's analytical output
:process,manual evaluation
:concept,synergistic amalgamation of human expertise and AI-driven insights
:theory,Elliott Wave Theory
:system,spoken dialogue system
:process,human speech production
:system,current spoken dialogue system
:capability,speech understanding
:task,dialogue response and linguistic feature modeling
:task,prosodic structure prediction
:task,dialogue response and linguistic feature integration
:approach,LLM-based approach
:task,text generative tasks
:issue,high computational cost
:system,ALTER
:application,domain-specific applications
:module,mixture-of-task-adapters (MTA)
:method,two-stage training
:concept,adapter collaboration
:architecture,MTA architecture
:product,MTA-equipped language models
:task,text editing
:goal,aligning with user intents
:dataset,existing text editing benchmark datasets
:characteristic,coarse-grained instructions
:output,edited text output
:standard,gold reference
:benchmark,XATU
:`edit type`,"lexical, syntactic, semantic, and knowledge-intensive edits"
:feature,fine-grained instructions and gold-standard edit explanations
:experiment,extensive experimentation
:feature,explanations in fine-tuning
:action,open-sourcing
:technique,mitigation techniques
:investigation,exploratory investigation
:factor,linguistic factors in prompts
:factor,readability
:factor,formality
:factor,concreteness
:task,DNA sequence alignment
:field,genomic analysis
:task,variant calling
:method,genome indexing
:method,efficient search
:task,DNA sequence encoding
:representation,numerical representations for DNA sequences
:task,sequence classification
:task,sequence alignment
:model,DNA-ESA
:technique,contrastive loss
:metric,alignment accuracy
:`model family`,DNA-Transformer model baselines
:concept,task transfer
:paradigm,Semantic Communication (SC)
:`model family`,generative model
:framework,Language-Oriented Semantic Communication (LSC)
:communication,machine communication
:language,human language
:algorithm,Semantic Source Coding (SSC)
:task,text prompt compression
:algorithm,Semantic Channel Coding (SCC)
:goal,robustness against errors
:algorithm,Semantic Knowledge Distillation (SKD)
:task,listener-customized prompts
:task,progressive text-to-image generation
:method,proposed LSC methods
:`communication channel`,noisy communication channel
:concept,syntax learning
:task,parsing
:technique,sequence labeling
:task,dependency parsing
:task,constituent parsing
:framework,multi-formalism syntactic structure framework
:concept,encodings
:resource,pre-trained word vectors
:concept,constituency representations of syntax
:technique,sub-word tokenization
:concept,syntax representation
:concept,language occurrence in pretraining data
:concept,amount of task data
:attribute,robustness
:`evaluation method`,traditional question answering datasets
:capability,generation capabilities of LLMs
:approach,rational evaluation
:tool,pre-trained reward models
:`evaluation framework`,Treval
:conversation,longer conversation
:capability,understanding questions
:issue,word-level perturbations
:resource,GitHub repository for Treval
:entity,Languini Kitchen
:purpose,empowering researchers with limited computational resources
:protocol,experimental protocol
:task,model comparison
:metric,accelerator hours
:`model attribute`,model's throughput
:metric,number of training tokens
:dataset,"large, diverse, high-quality dataset of books"
:task,evaluation
:method,empirical scaling trends comparison
:model,feed-forward model derived from GPT-2
:model,novel LSTM with ten-fold throughput
:model,GPT baseline
:model,LSTM baseline
:metric,scaling law
:knowledge,world knowledge
:task,knowledge graph question answering (KGQA)
:approach,KG-augmented language model
:issue,gap between KG and textual representations
:approach,answer-sensitive KG-to-text
:representation,well-textualized statements
:framework,KG-to-text enhanced LLMs
:approach,KG-to-text augmented LLMs
:approach,previous KG-augmented LLMs
:metric,answer accuracy and usefulness of knowledge statements
:capability,impressive capabilities
:`interaction mode`,text-based interaction
:process,text-to-speech synthesis
:issue,latency
:`interaction mode`,fluent voice conversations
:architecture,LLM2Speech
:process,speech synthesis
:model,non-streaming teacher model
:feature,hidden embeddings
:process,text generation
:quality,teacher model quality
:model,ChatGPT-4
:tool,editing tool
:industry,publishing industry
:goal,automated solutions
:capability,text comprehension and generation
:analysis,ChatGPT-4 features and capabilities analysis
:aspect,"grammatical correction, stylistic coherence, linguistic enrichment"
:test,ChatGPT-4 editing test
:text,literary and academic texts
:challenge,"context sensitivity, bibliometric analysis, deep contextual understanding, interaction with visual content"
:strategy,collaboration between ChatGPT-4 and human reviewers
:value,efficiency and quality
:value,complementarity
:profession,human editors
:value,high-caliber editing
:technique,reinforcement learning fine-tuning
:framework,OpenChat
:technique,C(onditioned)-RLFT
:problem,leveraging mixed-quality data
:model,OpenChat-13B
:benchmark,standard benchmarks
:benchmark,AGIEval
:model,base model
:resource,"OpenChat code, data, and models"
:URL,https://github.com/imoneoi/openchat
:trend,non-English language models
:`model architecture`,encoder-only
:`model architecture`,decoder-only
:model,BERT
:capability,natural language understanding and generation
:model,GPT
:language,Spanish
:model,BART
:task,sequence-to-sequence tasks
:model,T5
:model,BERT2BERT
:resource,Spanish language models
:task,chart perception
:activity,information extraction
:task,chart reasoning
:activity,reasoning
:paradigm,unified and label-efficient learning
:task,joint chart perception and reasoning
:approach,StructChart
:representation,structured triplet representations
:metric,Structuring Chart-Oriented Representation Metric (SCRM)
:attribute,chart diversity
:paradigm,unified chart perception-reasoning
:task,chart-related tasks
:event,IberLEF 2023 workshop
:forum,Iberian Languages Evaluation Forum
:event,SEPLN 2023 conference
:task,Autextification shared task
:subtask,text authorship determination
:subtask,text generation model attribution
:dataset,Autextification 2023 dataset
:data,texts in English and Spanish
:domain,various text domains
:community,participants
:artifact,runs
:artifact,working notes
:overview,Autextification shared task overview
:artifact,"dataset, task, systems, results"
:method,clinical prediction with large language model
:task,clinical disease prediction
:technique,quantization
:data,historical diagnosis records
:model,CPLLM
:model,Med-BERT
:task,disease prediction
:metric,PR-AUC
:metric,ROC-AUC
:task,visual language navigation (VLN)
:skill,multifaceted skills
:method,previous VLN methods
:approach,single-round self-thinking
:inspiration,expert consultation meeting
:framework,zero-shot VLN framework
:agent,DiscussNav
:discussion,discussions with domain experts
:subtask,navigation subtasks
:method,DiscussNav method
:task,R2R
:experiment,real-robot experiments
:model,Safurai-001
:model,WizardCoder
:model,PanguCoder
:model,Phi-1
:feature,conversational interaction
:field,data engineering
:benchmark,GPT-4-based multiparameters
:`model family`,coding large language model
:`performance level`,near-human
:task,open-domain question answering
:issue,hallucination of incorrect answers
:experiment,automatic validation of generated answers
:task,answer validation
:dataset,MS MARCO (v1)
:pipeline,retrieval pipeline
:technique,"sparse retrieval, dense retrieval, neural rerankers"
:experiment,validation of entire generated answer
:experiment,granular validation of generated answer
:task,factual statement validation
:result,LLM verification accuracy
:metric,over 80% accuracy
:issue,missed incorrect generated answers
:process,verification process
:model,Kosmos-2.5
:capability,machine reading of text-intensive images
:data,text-intensive images
:task,generating spatially-aware text blocks
:task,producing structured text output
:capability,unified multimodal literate capability
:task,document-level text recognition
:task,image-to-markdown text generation
:capability,text-intensive image understanding
:application,real-world applications with text-rich images
:trend,scaling of multimodal large language models
:agent,autonomous user interface agent
:task,task automation
:capability,effective engagement in diverse environments
:approach,existing approaches
:setting,sandbox
:issue,inference inefficiency and error propagation
:solution,Auto-UI
:challenge,environment parsing and API reliance
:technique,chain-of-action
:task,action decision
:benchmark,AITW
:task,multi-step tasks
:resource,Auto-UI code
:URL,https://github.com/cooelf/auto-ui
:task,grammatical error correction (GEC)
:goal,user comprehension
:approach,existing studies
:method,indirect explanation
:method,direct explanation
:task,generating explanations for GEC corrections
:process,alignment and identification
:challenge,explicit control of generation
:method,Controlled Generation with Prompt Insertion (PI)
:goal,explanation of correction reasons
:process,correction and explanation generation
:dataset,Explainable GEC (xGEC) dataset
:source,annotated datasets
:issue,incomplete correction explanation
:performance,correction reason generation
:opportunity,feedback-rich computational models
:`model type`,generative agent-based model
:scenario,social norm diffusion in an organization
:attribute,simplicity
:task,sensitivity analysis
:article,the article
:purpose,guide for building diffusion models
:integration,storytelling and large language model
:application,social chatbots
:concept,story engineering
:entity,fictional game characters
:steps,"character creation, live storytelling, community interaction"
:prototype,storytelling social chatbots
:community,online gaming community
:platform,Discord
:analysis,mixed-method
:`data collection`,questionnaires and interviews
:outcome,storytelling impact
:qualities,engagement and believability
:challenge,designing reward functions
:resource,specialized knowledge or domain data
:framework,Text2Reward
:artifact,dense reward functions
:artifact,interpretable dense reward codes
:resource,existing packages
:benchmark,robotic manipulation and locomotion environments
:policy,policies trained with generated reward codes
:metric,task success rates and convergence speed
:behavior,novel locomotion behaviors
:policy,policies trained in the simulator
:domain,real world
:process,refining reward functions with human feedback
:resource,video results
:URL,https://text-to-reward.github.io
:ability,deliberation on responses
:method,Chain-of-Verification (CoVe)
:goal,decreasing hallucinations
:process,response verification
:experiment,CoVe evaluation
:outcome,decreased hallucinations
:task,list-based questions
:task,closed book MultiSpanQA
:task,longform text generation
:framework,DreamLLM
:principle,generative modeling in raw multimodal space
:limitation,external feature extractors
:content,interleaved documents
:distribution,multimodal distributions
:content,free-form interleaved content
:role,zero-shot multimodal generalist
:performance,enhanced learning synergy
:model,Retentive Network
:feature,explicit decay mechanism
:limitation,unidirectional and one-dimensional decay
:task,image-based tasks
:proposal,"bidirectional, two-dimensional explicit decay"
:field,computer vision
:`model family`,vision backbones
:method,parallel form
:model,Retentive Self-Attention (ReSA)
:concept,image axes
:model,RMT
:metric,top1-acc on ImageNet-1K
:URL,https://github.com/qhfan/rmt
:model,Bittensor language model
:data,Slimpajama dataset
:benchmark,existing 3B parameter models
:benchmark,7B parameter models
:performance,long context performance
:technique,training optimizations
:resource,memory and compute efficiency
:license,Apache 2.0
:integration,LLM integration into workplace software
:outcome,workers' well-being
:community,minoritized communities
:experiment,online experiment on co-writing with LLM
:measure,user's well-being
:task,writing job promotion requests
:style,hesitant or self-assured auto-complete suggestions
:feeling,inclusion
:feeling,agency and ownership
:community,minoritized genders
:correlation,inclusion and agency/ownership
:issue,loss of control and agency
:research,future work
:concept,AI-written communication
:application,conversational agent
:value,ethics
:research,existing research
:perspective,users' privacy concerns
:method,analysis of sensitive disclosures
:stakeholder,LLM-based CA users
:`trade-off`,privacy vs utility vs convenience
:concept,erroneous mental models
:interaction,human-like
:behavior,sensitive disclosures
:guideline,design guidelines
:shift,paradigmatic shifts
:challenge,semantic ambiguity
:field,machine translation
:system,conventional neural machine translation
:issue,ambiguous sentences
:solution,alternative to traditional NMT models
:concept,controlling target outputs
:capability,disambiguation
:method,fine-tuning on ambiguous datasets
:experiment,disambiguation experiments
:benchmark,state-of-the-art systems
:system,DeepL
:system,NLLB
:resource,curated disambiguation corpora
:website,https://data.statmt.org/ambiguous-europarl
:`dataset type`,knowledge graph-text paired
:`model type`,neural model
:`dataset quality`,equivalence between KG and text
:`model performance`,hallucination and recall
:`dataset attribute`,noise level
:`evaluation method`,cyclic evaluation
:dataset,WebNLG
:dataset,LaGrange
:heuristic,equivalence improvement heuristic
:dataset,synthetic dataset
:task,cyclic generation
:`model performance`,cyclic generation of KGs
:concept,consistent underlying ontology
:task,translation
:`model type`,supervised encoder-decoder translation model
:`model size`,moderate
:`parameter count`,7B or 13B parameters
:approach,novel fine-tuning
:`data type`,abundant parallel data
:method,two-stage fine-tuning
:model,ALMA
:metric,BLEU and COMET scores
:dataset,WMT'21 and WMT'22 test datasets
:model,NLLB-54B and GPT-3.5-Text-DaVinci-003
:method,novel training paradigm
:process,virtual screening
:field,drug discovery
:collection,commercially available compound collections
:scale,billions of compounds
:tool,docking
:collection,large-scale compound libraries
:method,active learning
:task,narrowing down search space
:method,Bayesian optimization
:component,surrogate machine learning model
:method,active learning and Bayesian optimization
:model,pretrained transformer-based language model
:framework,Bayesian optimization active learning framework
:model,pretrained model
:performance,high sample efficiency
:process,molecule virtual screening
:limitation,knowledge outside training data
:method,modular decomposition
:task,deep reasoning tasks
:method,REBEL
:technique,automated reasoning techniques
:capability,recursive problem decomposition
:task,deeply nested tool use
:characteristic,unpersonalized generation paradigm
:task,user-oriented large language model enhancement
:state,unexplored
:method,fully training large language model
:constraint,resource consumption
:method,memory-based methods
:task,knowledge storage and retrieval
:component,memory module
:capability,user preference comprehension
:mechanism,computational bionic memory
:task,LLM personalization
:outcome,effectiveness and superiority
:resource,conversation dataset
:purpose,further research encouragement
:resource,implementation code
:field,script learning
:concept,daily events
:approach,previous script learning approaches
:concept,linear sequence of events
:concept,branching events
:benchmark,Choice-75
:task,predicting decisions
:data,scripts and scenarios
:task,multimodal tasks
:problem,adversarial robustness
:`model family`,vision model
:model,Bard
:model,Bing Chat
:model,Ernie Bot
:`defense mechanism`,face detection
:`defense mechanism`,toxicity detection
:`defense mechanism`,Bard defenses
:model,GPT-4V
:URL,https://github.com/thu-ml/attack-bard
:problem,in-context learning with privacy risks
:risk,privacy leakage
:algorithm,synthetic few-shot demonstration generation
:method,differential privacy
:experiment,extensive experiments on standard benchmarks
:result,competitive performance with strong privacy levels
:possibility,in-context learning with privacy protection
:structure,categories and lists
:issue,redundancies and inconsistencies
:ontology,DBpedia classes
:`knowledge graph`,large knowledge graph
:task,categorizing digital contents
:approach,Caligraph
:issue,incomplete and non-fine grained mappings
:problem,ontology alignment
:model,SLHCat
:process,training data generation and model finetuning
:model,baseline model
:`job market`,evolving job market
:task,job opportunity matching
:technology,AI
:task,job recommendation
:task,movie recommendation
:aspect,subtle aspects of a job
:data,unstructured data
:process,data structuring
:approach,content based deterministic
:approach,LLM guided
:approach,LLM unguided
:approach,hybrid
:study,job recommendation methods study
:aspect,advantages and limitations
:task,text understanding and generation
:method,black-box attack
:method,prompt attack
:dataset,CPAD
:outcome,unexpected outputs
:dimension,"contents, attacking methods, and goals"
:experiment,evaluation of CPAD
:task,document-grounded response generation
:event,DialDoc 2022 shared task
:task,dialogue completion
:method,Chat-Completion
:`knowledge source`,ChatGPT model pretraining
:method,LLaMIndex
:`knowledge source`,document extraction
:`evaluation metric`,automatic evaluation metrics
:output,dialogue responses
:phenomenon,hallucinations
:`model variant`,ChatGPT variants
:benchmark,shared task winning system and human responses
:technique,knowledge sanitization
:data,web data
:risk,knowledge leakage
:response,harmless response
:phrase,I don't know
:task,closed-book question-answering
:outcome,knowledge leakage minimization
:performance,LLM overall performance
:defense,against extraction attacks
:content,harmful content
:`model family`,pre-trained large language model
:benchmark,state-of-the-art NLP tasks
:concept,implicit hate speech understanding
:content,implicit hate speech
:classification,non-hate
:approach,external context augmentation
:task,implicit hate content detection
:approach,label separation enforcement
:framework,Focused Inferential Adaptive Density Discrimination (FIADD)
:approach,combined approach for implicit hate speech detection
:process,PLM finetuning pipeline
:dataset,implicit hate datasets
:task,"sarcasm, irony, and stance detection"
:`latent space`,generated under FIADD
:task,implicit hate speech detection
:task,emotion recognition in dialogue (ERC)
:issue,complex pipeline designs
:issue,dataset and dialogue pattern specificity
:approach,InstructERC
:module,retrieval template module
:information,multi-granularity dialogue supervision information
:task,emotion alignment tasks
:framework,LLM-based plug-and-play plugin framework
:benchmark,previous ERC models
:achievement,SOTA on ERC datasets
:analysis,parameter-efficient and data-scaling experiments
:guidance,applying InstructERC in practice
:code,InstructERC code
:event,after blind review
:task,zero-shot text-to-speech synthesis
:capability,voice cloning
:method,quantizing speech waveform
:capability,zero-shot speaker adaptation
:`model family`,language model-based text-to-speech
:technique,acoustic prompting
:limitation,acoustic prompt length
:capability,personal speaking style cloning
:model,VALL-E
:`model family`,neural codec language model
:component,speaker-aware text encoder
:capability,personal speaking style learning
:component,VALL-E based acoustic decoder
:capability,timbre modeling
:metric,naturalness and speaker similarity
:method,scaling out to a longer style prompt
:concept,machine intelligence metrics
:concept,machine cognitive evaluation
:milestone,imitation
:goal,efficient language acquisition and understanding
:test,Turing Test
:framework,language acquisition-based framework
:paper,present contribution
:concept,interdisciplinary work
:concept,interdisciplinary bridges
:value,interdisciplinary collaboration
:value,sustainability in AI evaluation
:dataset,LMSys-Chat-1M
:data,real-world conversations
:source,Vicuna demo and Chatbot Arena website
:attribute,dataset overview
:attribute,"diversity, originality, and scale"
:`use case`,content moderation models
:`use case`,safety benchmark
:`use case`,instruction-following models
:`use case`,benchmark questions
:goal,advancing LLM capabilities
:task,automatic ad text generation
:limitation,manual online ad production
:challenge,comparing ATG methods
:lack,benchmarks and problem sets
:aspect,cross-application internet advertising
:dataset,CAMERA
:experiment,evaluation experiments
:`model family`,localized large language model
:language,Arabic
:characteristic,unique cultural characteristics
:solution,comprehensive solution for Arabic LLM
:technique,reinforcement learning with AI feedback
:`model component`,reward model
:characteristic,local culture and values
:model,ACEGPT
:benchmark,open Arabic LLMs
:benchmark,Arabic VICUNA-80
:benchmark,Arabic ALPACAEVAL
:benchmark,Arabic MMLU
:benchmark,Arabic cultural and value alignment benchmark
:model,Turbo
:resource,GitHub repository for ACEGPT
:URL,https://github.com/freedomintelligence/acegpt
:model,BELT
:field,brain-to-language translation
:technology,brain-computer interfaces
:problem,brain signal decoding
:data,EEG representation
:method,BELT framework
:data,internet-scale datasets
:component,deep Conformer encoder
:component,vector quantization encoder
:technique,contrastive learning
:task,zero-shot sentiment classification
:metric,BLEU-1 score
:capability,complex question construction and reasoning
:requirement,dedicated hardware
:model,Koala
:database,ENEM question database
:task,model effectiveness evaluation
:task,query processing time
:hardware,AMD Ryzen 5 3600X
:community,physics education community
:technique,prompt-engineering
:task,conceptual physics tasks
:task,physics tasks
:concept,LLM functioning
:task,introductory physics problems
:task,physics problem solving
:insight,use of LLMs in physics education
:field,physics education
:study,benchmarking study
:task,multi-label text classification
:industry,thematic investment strategy
:method,text-to-text classification
:limitation,generated label mismatch
:limitation,lack of permutation invariance
:limitation,binary decisions without confidence scores
:technique,constrained decoding using trie search
:technique,prompt tuned embedding classification (PTEC)
:limitation,all limitations of text-to-text classification
:data,skewed training data towards well-known companies
:trend,adapting state-of-the-art methods to domain-specific tasks
:era,PLMs with strong generalization abilities
:resource,codebase and benchmarking dataset
:url,https://github.com/eqtpartners/ptec
:integration,large language model in robotics
:domain,human-robot interaction
:task,autonomous task planning
:issue,lack of self-correction in task execution
:quality,adaptability in dynamic environments
:framework,Hierarchical Closed-loop Robotic Intelligent Self-correction Planner (HiCRISP)
:capability,error correction during task execution
:issue,high-level planning and low-level action errors
:process,task execution
:experiment,benchmark experiments for HiCRISP
:performance,exceptional performance of HiCRISP
:solution,robotic task planning with LLMs
:challenge,unimaginable traditional training
:trend,efficient fine-tuning for high-resource languages
:language,Tibetan
:research,efficient fine-tuning for low-resource language models
:research,our research
:gap,efficient fine-tuning for Tibetan
:technique,prompt-tuning
:dataset,TNCC-title
:technique,adapter lightweight fine-tuning
:technique,prompt-tuning + adapter fine-tuning
:improvement,significant improvements in Tibetan language applications
:application,construction contract management
:benefit,reducing human errors
:issue,misleading content
:knowledge,expert-driven contract knowledge
:process,automatic contract management
:`knowledge representation`,Nested Contract Knowledge Graph (NCKG)
:knowledge,contract knowledge
:framework,nested knowledge representation framework
:ontology,NCKG ontology
:method,implementation method
:pipeline,LLM-assisted contract review pipeline
:knowledge,external knowledge in NCKG
:task,contract risk reviewing
:combination,LLM and KG
:quality,reliable and interpretable contract management
:demographic,children
:skill,emotional identification and expression
:activity,emotional communication
:chatbot,ChaCha
:technology,state machine and large language model
:study,exploratory study with children
:demographic,children aged 8-12
:perception,ChaCha as a close friend
:outcome,sharing stories
:application,child-friendly chatbots
:skill,sense of diverse clues and understanding of real-world background
:system,Adaptive Rationale Guidance network for Fake News Detection (ARG)
:system,ARG-D
:system,ARG
:concept,skill neurons
:concept,task transferability
:data,adversarial data
:concept,skill neuron activation
:framework,large language model for mixed reality
:application,interactive mixed reality experiences
:problem,scarce ideal training data
:goal,complex design goals
:technology,Unity game engine
:technique,scene understanding
:feature,cross-platform interoperability
:task,creation and modification tasks
:output,"diverse objects, tools, and scenes"
:study,usability study
:outcome,positive user experience
:task,information extraction tasks
:task,biomedical named entity recognition (NER)
:approach,chain-of-thought
:subtask,entity span extraction and entity type determination
:subtask,entity type determination
:knowledge,entity knowledge
:approach,two-step BioNER
:baseline,few-shot LLM baseline
:`model family`,open-source large language model
:task,mathematical problem solving
:model,MetaMath
:task,mathematical reasoning
:dataset,MetaMathQA
:benchmark,MATH
:model,MetaMath-7B
:model,MetaMath-70B
:model,GPT-3.5-Turbo
:resource,MetaMathQA dataset
:community,public
:resource,MetaMath models
:phenomenon,reversal curse
:model,LLaMA-1
:task,logical deduction
:resource,reversal curse code
:URL,https://github.com/lukasberglund/reversal_curse
:task,generating natural language from logical forms
:quality,semantic consistency
:approach,generate-and-rerank
:dataset,manually collected dataset
:metric,ranking metrics
:model,reranker model
:dataset,diverse datasets
:method,baseline methods
:metric,comprehensive metrics
:quality,generated output quality
:technique,efficient fine-tuning
:approach,LongLora
:challenge,long context size training
:process,context extension of LLMs
:`attention mechanism`,sparse local attention
:`attention mechanism`,dense global attention
:process,context expansion
:model,Llama2
:task,various tasks
:model,Llama2 7B
:model,Llama2 70B
:dataset,LongQA
:phenomenon,interpersonal conflict
:concept,life
:skill,navigating conflict
:method,deliberate practice
:system,Rehearsal
:goal,expanding access to conflict training
:technique,IRP prompting
:theory,Interest-Rights-Power (IRP) theory
:evaluation,between-subjects evaluation
:group,control group
:outcome,improved conflict performance
:strategy,competitive strategies
:strategy,cooperative strategies
:skill,interpersonal skills
:skill,3D visual grounding
:agent,household robots
:approach,existing 3D visual grounding approaches
:resource,extensive labeled data
:pipeline,LLM-Grounder
:task,query decomposition
:tool,visual grounding tool
:task,final grounding decision
:resource,labeled training data
:benchmark,ScanRefer
:capability,grounding capability
:URL,https://chat-with-nerf.github.io/
:method,conversational deliberation
:outcome,group decision-making
:problem,scaling real-time conversations
:technology,conversational swarm intelligence
:concept,biological swarm dynamics
:concept,collective intelligence
:system,Thinkscape
:group,deliberative groups using Thinkscape
:metric,content contribution
:metric,balanced dialog
:preference,CSI system over standard chat
:feeling,impactfulness
:outcome,large-scale deliberation
:application,chatbots
:issue,inaccurate responses
:integration,knowledge base with LLM
:goal,response reliability
:`knowledge base`,scalable
:feature,seamless integration of lesson curricula
:evaluation,student participants
:evaluation,cross-examination
:model,GPT-4 with KB access
:attribute,pedagogical abilities
:technique,language model-based pre-training
:field,visual document understanding
:technique,pre-training reading all text from image
:domain,broader domains
:domain,visual documents and scene text images
:limitation,instability in broader domains
:method,SCOB
:task,pre-training document and scene text domains
:goal,reducing annotation costs
:technique,vanilla pre-training methods
:technique,read-type pre-training methods
:resource,SCOB code
:performance,zero shot performance
:capability,reasoning and commonsense application
:application,synthetic dataset creation
:task,extractive reading comprehension dataset augmentation
:process,data annotation
:role,human annotator replacement
:evaluation,performance and cost comparison
:analysis,LLMs as synthetic data augmenters
:dataset,augmented low resource datasets
:task,benchmark creation
:trend,patient-centered healthcare transformation
:service,diagnosis
:service,personalized lifestyle recommendations
:service,mental health support
:goal,mitigating workload burden
:requirement,evaluation metrics
:concept,medical and health concept comprehension
:concept,user-centered aspects
:concept,evaluation metrics for healthcare chatbots
:challenge,defining and implementing evaluation metrics
:task,abstractive text summarisation
:metric,ROUGE scoring
:effort,developing improved metrics
:metric,improved metrics
:task,long document text summarisation
:research,evaluation of automatic evaluation metrics
:framework,LongDocFactScore
:metric,state-of-the-art metrics
:resource,code and annotated data
:domain,scientific imaging
:combination,human expertise and subject comprehension
:methodology,linguistic emulation of human interactions
:technology,scanning electron microscopy (SEM)
:framework,multimodal deep learning
:`data type`,textual and visual data
:`data source`,peer-reviewed articles
:process,data synthesis and evaluation
:model,GlassLLaVA
:task,interpreting SEM images
:metric,versatile evaluation metrics
:progress,bridging the gap between human and machine interpretation
:opportunity,future research and application
:ability,reasoning around cognitive states
:benchmark,multiple-choice questions (MCQ)
:task,model ability assessment
:issue,order bias
:task,resilience probing
:technique,adversarial examples
:issue,performance gap
:issue,selection bias
:issue,positional bias
:heuristic,structural heuristics
:technique,chain-of-thought (COT)
:goal,bias mitigation
:task,repository-level coding
:tool,GitHub Copilot
:problem,localized coding
:problem,planning
:framework,CodePlan
:technique,incremental dependency analysis
:technique,change may-impact analysis
:algorithm,adaptive planning algorithm
:task,package migration
:task,temporal code edits
:result,CodePlan effectiveness
:technique,knowledge graph embedding
:task,link prediction
:property,scalability and explainability
:`research area`,knowledge graph completion
:method,distance-based KG embedding
:method,semantic matching-based KG embedding
:model,ComplEx
:concept,2D affine operations
:model,ComplEx3D
:concept,3D affine operations
:approach,pre-trained language model for KG completion
:integration,KGE methods with PLMs
:task,text simplification
:goal,ease of understanding
:task,text elaboration
:goal,complexity control
:limitation,relative readability alteration
:goal,absolute target readability
:audience,diverse audience
:task,readability-controlled text modification
:metric,readability-controlled text modification metrics
:approach,two-step process
:goal,readability control
:finding,semantic and lexical similarity drop
:application,personalized exercise planning
:factor,complex schedules and considerations
:process,iterations with experts
:system,PlanFitting
:feature,natural language interaction
:user,expert planners
:quality,personalization and actionability
:discussion,future design opportunities
:capability,instruction following and conversational interaction
:application,support tools
:study,empirical user study
:interface,collaborative writing interface
:model,cognitive process model of writing
:activity,writing
:survey,post-completion survey
:feedback,LLM as writing collaborators
:interaction,writer-LLM interaction
:`cognitive activity`,translation and reviewing
:finding,analysis of interactions and survey responses
:`research direction`,creative writing assistance
:event,Twitter ownership change
:behavior,reconsidering social media outlets
:`social media platform`,Mastodon
:`social media platform`,Bluesky
:`social media platform`,Threads
:`user group`,Twitter users who migrated
:`user group`,non-migrants
:phenomenon,temporal migration patterns
:challenge,sustainable migration
:`social media platform`,new platforms
:`social media platform`,Twitter
:analysis,behavioral analysis
:phenomenon,distinct migration pattern
:task,stance detection
:research,in-depth analysis
:phenomenon,migration patterns
:application,autonomous AI agents
:concept,accessibility
:practice,model cards
:artifact,ML model information
:dataset,model card generation dataset
:task,extracting answers
:role,annotators
:model,ChatGPT-3.5
:model,Galactica
:finding,gap in understanding and factual response generation
:task,automating model card generation
:attribute,text diversity and informativeness
:approach,previous approaches
:problem,undesirable behaviors in language models
:behavior,repetition
:behavior,overuse of frequent words
:observation,models learn attributes causing degeneration
:approach,new approach to prevent degeneration
:method,training two models
:model,first model
:behavior,undesirable patterns
:model,second model
:attribute,text diversity
:`payment system`,Diagnosis-Related Group (DRG)
:process,DRG assignment
:model,DRG-LLaMA
:dataset,MIMIC-IV discharge summaries
:model,DRG-LLaMA-7B
:metric,model performance metrics
:`model family`,prior leading models in DRG prediction
:model,ClinicalBERT
:model,CAML
:task,base DRG and CC/MCC prediction
:finding,"correlation between performance, model parameters, and input context length"
:task,contract review
:industry,construction
:method,current contract review methods
:attribute,effectiveness and reliability
:approach,novel approach for contract review
:approach,tuning-free approach
:`knowledge type`,construction contract domain knowledge
:`knowledge base`,construction contract domain knowledge base
:method,evaluation on real construction contracts
:outcome,solid performance
:`cognitive process`,logical thinking
:task,TextTableQA
:characteristic,challenging
:technique,chain-of-thought prompting
:strategy,hybrid prompt strategy
:technique,retrieval of thought
:ability,retrieval thinking
:dataset,MultiHiertt
:setting,few-shot
:performance,superior performance
:benchmark,SOTA (State of the Art)
:knowledge,syntactic knowledge
:corpus,JCoLA (Japanese Corpus of Linguistic Acceptability)
:task,syntactic evaluation
:data,"10,020 sentences"
:data,in-domain data
:data,out-of-domain data
:evaluation,syntactic knowledge evaluation
:`model family`,Japanese language model
:`linguistic phenomenon`,long-distance syntactic dependencies
:entity,society
:scenario,black-box interaction
:capability,modifying internal knowledge
:process,in-context training
:training,session-based training
:concept,applications and limitations
:phenomenon,interference
:phenomenon,forgetting
:attribute,performance
:benchmark,evaluation benchmark
:dataset,bAbI dataset
:concept,human knowledge
:attribute,imperfection
:concept,everyday terms
:challenge,semantic web challenges
:concept,semantic web
:notation,intuitive notation
:concept,defeasible reasoning
:model,model for defeasible reasoning
:attribute,imperfect knowledge
:theory,argumentation theory
:notation,PKN
:notation,N3
:concept,deductive logic
:syntax,intuitive syntax
:concept,reasoning strategies and tactics
:ontology,AIF ontology
:approach,symbolic approaches
:role,coding assistant
:test,coding assistant evaluation
:capability,"answering questions, producing code, debugging"
:outcome,impressive test results
:impact,productivity and procedural reorganization in software development
:role,reasoner and generator
:task,multi-hop question answering
:process,integration of LLM and external knowledge retrieval
:method,existing methods for MHQA
:component,information retriever
:issue,low quality of generated queries
:issue,irrelevant knowledge from IR
:pipeline,Furthest-Reasoning-with-Plan-Assessment (FuRePa)
:framework,Furthest Reasoning
:module,Plan Assessor
:evaluation,methods evaluation
:dataset,public multi-hop question answering datasets
:metric,answer accuracy
:challenge,standardized questions
:goal,individualized learning
:system,personalized support system for reading comprehension
:system,ChatPRCS
:theory,zone of proximal development
:algorithm,reading comprehension proficiency prediction
:task,question generation
:technique,prompt patterns
:method,experiments
:result,high-quality reading comprehension questions
:standard,expert-crafted questions
:condition,trained on large-scale corpora
:system,generic NMT systems
:task,out-of-domain translation
:task,Arabic MT domain-specific adaptation
:domain,financial news articles
:resource,parallel corpus for Arabic-English translation
:domain,financial
:model,ChatGPT-3.5 Turbo
:task,financial domain transfer learning
:resource,datasets and fine-tuned models
:task,semantic textual similarity
:challenge,vanishing gradients
:`model family`,text embedding model
:function,cosine function
:model,ANGLE
:technique,angle optimization in complex space
:dataset,short-text STS datasets
:dataset,long-text STS dataset from GitHub issues
:scenario,domain-specific STS with limited labeled data
:benchmark,state-of-the-art STS models
:quality,high-quality text embeddings
:concept,affect recognition
:capability,affective cue recognition
:field,conversational artificial intelligence
:`dialogue type`,open-domain chit-chat dialogue
:`dialogue type`,task-oriented dialogue
:dataset,EMOVOZ
:dataset,DAIC-WOZ
:technique,task-specific fine-tuning
:capability,human-like affect recognition
:capability,generating high-quality open-ended text
:`generated text`,deepfake text
:repository,HuggingFace model repository
:quantity,11k text generation models
:risk,misuse of LLMs
:method,Turing Test (TT)
:problem,authorship attribution (AA)
:model,TopoRoBERTa
:technique,topological data analysis (TDA)
:feature,linguistic structures
:feature,semantic and syntactic linguistic features
:concept,software project progression
:attribute,code quality
:attribute,software attributes
:tool,static analysis tool
:issue,code quality issues
:developer,software developer
:activity,code revision
:tool,CoRE
:model,proposer LLM
:model,ranker LLM
:activity,code revision evaluation
:`programming language`,Python
:issue,false positives
:`programming language`,Java
:activity,multi-turn dialogue execution
:context,multi-turn dialogue comprehension
:strategy,self-explanation prompting
:ability,comprehension in multi-turn dialogues
:approach,task-agnostic
:process,dialogue utterance analysis
:performance,dialogue-centric task performance
:potential,enhancing comprehension in complex dialogue tasks
:method,assurance case
:field,safety engineering
:area,safety-critical
:concept,trustworthiness derivation tree (TDT)
:tool,Trustworthiness Derivation Tree Analyzer (TRUSTA)
:technology,constraint solvers
:task,generating assurance cases
:model,PaLM 2
:process,interactive human examination
:task,extracting formal constraints
:integration,large language models in assurance cases
:challenge,traditional challenge in assurance cases
:method,industrial case studies
:`software framework`,OpenAirInterface5G
:goal,"improving security, reliability, and comprehensibility"
:tool,AFL++
:process,fuzzy-testing
:task,automated parameter interpretation
:goal,streamlined analysis and informed decision-making
:combination,fuzzy-testing and automated parameter interpretation
:attribute,"robustness, security, and understandability"
:task,complex reasoning tasks
:framework,ReConcile
:theory,Society of Minds
:concept,round table conference
:capability,reasoning capabilities
:mechanism,confidence-weighted voting
:model,Claude2
:concept,consensus
:resource,ReConcile code
:URL,https://github.com/dinobby/reconcile
:`data type`,log data
:information,user interactions
:task,analyzing user intents
:method,manual labeling
:task,labeling user intents
:method,ML-based labeling
:task,generating user intent taxonomy
:taxonomy,LLM-generated user intent taxonomy
:goal,validity and reliability
:methodology,human-in-the-loop verification
:pipeline,end-to-end LLM with human-in-the-loop
:task,user intent analysis
:method,LLM with human-in-the-loop
:quality,scalability and adaptability
:insight,new insights into user intents
:service,Bing
:model,InvestLM
:dataset,instruction dataset
:topic,financial topics
:capability,understanding financial text
:professionals,financial experts
:models,commercial models
:quality,generalizability
:hypothesis,superficial alignment hypothesis
:profession,financial professionals
:process,iterative refinement and revision
:outcome,errors
:characteristic,homogeneous reasoning
:framework,SCReWS
:process,reasoning with revisions
:component,modules
:outcome,improved reasoning chains
:strategy,heterogeneous revision strategies
:process,selection between candidates
:outcome,improved reasoning accuracy
:framework,novel framework
:method,Chain-of-Thought (CoT)
:tool,Python REPL
:format,XML-like markup language
:task,mathematical problems
:integration,CoT and Python REPL through markup language
:capability,reasoning capability of LLMs
:field,computational natural philosophy
:concept,universe as information and computation
:field,natural world understanding
:research,interdisciplinary research
:technique,reinforcement learning with human feedback
:`research initiative`,neural networks with symbolic computing integration
:`model family`,hybrid computational model
:issue,hate speech and misinformation
:outcome,social violence and crimes
:association,posting hate speech/misinformation and personality traits
:trait,personality traits
:relationship,online hate speech/misinformation and psychological wellbeing
:concept,psychological wellbeing
:challenge,lack of adequate data analytics tools
:task,analyzing social media posts
:advancement,machine learning and large language models
:platform,Reddit
:data,social media posts
:technique,post embeddings
:technique,machine-learning classification
:`biological molecule`,antibody
:importance,therapeutic importance
:property,high specificity
:risk,low risk of adverse effects
:`discovery method`,traditional antibody discovery
:approach,wet lab
:technology,ML-based generative modeling
:approach,in-silico antibody discovery
:model,Antibarty
:model,property-conditional diffusion model
:process,guided IgG de novo design
:outcome,novel antibodies with improved in-silico solubility
:task,emotion estimation in images
:method,contextual cues
:task,contextual emotion estimation
:task,image captioning
:`descriptor set`,natural language descriptors
:task,caption construction
:dataset,EMOTIC
:task,emotion inference
:model,text-davinci-003
:outcome,emotion predictions
:approach,image captioning and LLM
:approach,traditional fine-tuning
:capability,generalization capacity
:model,Claude
:output,large model outputs
:dataset,ProtoQA
:metric,answer@1 score
:metric,incorrect@1 score
:dataset,StrategyQA
:dataset,CommonsenseQA 2.0
:technique,chain-of-thought and knowledge generation
:skill,language generation
:language,Bengali (Bangla)
:model,CLAUDE-2
:task,Bangla NLP tasks
:`task list`,"abstractive summarization, question answering, paraphrasing, natural language inference, text classification, sentiment analysis"
:goal,better understanding of LLMs
:concept,practical deployment
:task,table-based reasoning
:task,table-to-text generation
:technique,table-based reasoning distillation
:model,FLAN-T5-base
:technique,distilled data
:dataset,SciGen
:field,autonomous driving
:method,simulation
:field,intelligent transportation systems
:limitation,realism and diversity of agent behaviors
:issue,transfer of simulation outcomes to real world
:framework,generative driver agent simulation
:module,coach agent
:agent,driver agent
:experiment,practical simulation experiments
:experiment,user experiments
:result,collision rate decrease
:result,increase in human-likeness
:field,agent simulation
:literature,biomedical literature
:characteristic,complex language
:task,biomedical abstract simplification
:goal,public health literacy
:dataset,PLABA
:model,SciFive
:model,BioGPT
:mechanism,control-token
:model,BART-based models
:`evaluation metric`,SARI
:`evaluation metric`,BERTScore
:model,BART-L-W-CT
:model,T5-Base
:evaluation,simplicity score
:evaluation,meaning preservation score
:URL,https://github.com/hecta-uom/plaba-mu
:technique,prompt design
:challenge,prompt performance evaluation
:technique,manual prompt design
:technique,prompt optimization
:technique,prompt evaluation
:process,business optimisation
:skill,human expertise
:goal,competitive advantage
:challenge,LLM for problem formulation
:issue,token limitations
:issue,performance metrics
:tool,AI-copilot
:technique,modularization
:metric,performance evaluation metrics
:task,problem formulation
:experiment,synthesizing problem formulations
:goal,automated evaluation
:system,user simulator
:attribute,linguistic diversity
:metric,goal success rate
:output,lexically and syntactically diverse output
:dataset,human2bot
:threat,backdoor attacks
:challenge,few-shot scenario challenges
:defense,MDP
:property,masking-sensitivity gap
:data,few-shot data
:concept,distributional anchors
:actor,attacker
:evaluation,empirical evaluation
:platform,online social networks
:content,news
:`user behavior`,spreading harmful content
:field,credibility assessment
:method,binary categorization
:method,partial feature usage
:method,multilevel credibility assessment
:goal,credibility
:model,MultiCred
:experiment,MultiCred performance evaluation
:task,data collection for credibility assessment
:task,natural language generation quality evaluation
:challenge,lack of practice for calibration
:issue,closed-source models and high computational demand
:approach,AutoCalibrate
:goal,human preference alignment
:process,self-refinement
:experiment,text quality evaluation datasets
:outcome,improved correlation with expert evaluation
:analysis,qualitative analysis
:insight,effective scoring criteria
:`model family`,foundation model
:capability,understanding human-like semantics
:`research area`,semantic communications
:`system aspect`,system levels
:integration,foundation models integration
:`system aspect`,system design
:`model type`,compact model
:`system aspect`,performance and complexity
:approach,three separate approaches employing FMs
:field,foundation model research
:issue,unresolved issues
:resource,GlotScript-R
:data,attested writing systems
:process,aggregating information
:tool,GlotScript-T
:standard,Unicode 15.0 scripts
:output,script distribution
:corpus,MC4
:tool,GlotScript
:corpus,OSCAR
:concept,low resource scripts and languages
:issue,misinformation
:task,text detection and attribution
:method,cross-model detection
:attribute,LLM size and family
:technique,conversational fine-tuning
:task,model attribution
:task,source model identification
:task,model family classification
:task,model size classification
:finding,classifier effectiveness vs model size
:attribute,LLM size
:strategy,training on similarly sized LLMs
:finding,detectable signatures in LLM-generated text
:capability,multi-step reasoning
:attribute,knowledge
:behavior,reasoning behavior
:capability,coherent thinking
:`model family`,generative language model
:framework,Logical Chain-of-Thought (LogiCoT)
:capability,zero-shot chain-of-thought reasoning
:field,symbolic logic
:evaluation,experimental evaluation
:domain,arithmetic
:domain,commonsense
:domain,symbolic
:domain,causal inference
:domain,social problems
:task,complex tasks
:`research effort`,comprehending LLM capabilities
:application,counterfactual explanation modules
:concept,causal thinking
:pipeline,LLM-based counterfactual explanation generation
:capability,textual understanding and perturbation
:framework,counterfactual explanation generation framework
:benchmark,text classification benchmarks
:variant,two-step feature extraction based variant
:system,automated explanation systems
:`research effort`,extending context length
:capability,long text modeling
:benchmark,Bamboo
:capability,long context ability
:principle,benchmark design principles
:task,long text understanding tasks
:experiment,Bamboo evaluation
:`model family`,long context model
:research,long text research questions
:analysis,current long context models
:resource,Bamboo resources
:task,text normalization
:technique,self-consistency reasoning
:technique,linguistic-informed prompt engineering
:benchmark,error rates
:taxonomy,text normalization errors
:model,GPT-4.0
:framework,new framework for text normalization
:`model family`,GPT-based text normalization
:task,image retrieval
:task,text-image retrieval
:`input type`,conversational input
:task,reference resolution
:task,generating definite descriptions
:task,identifying referents
:dataset,visually-grounded dialogues
:approach,proposed approach
:benchmark,baselines
:`context window`,larger context window
:performance,higher returns
:entity,human society
:technique,softmax regression
:technique,residual neural network
:field,machine learning and theoretical computer science
:problem,regression problem
:technique,softmax regression and residual neural network
:concept,Hessian matrix
:method,approximate Newton method
:scheme,unified scheme for softmax regression and ResNet
:concept,loss landscape and optimization
:`model family`,over-parameterized neural network
:content,personalized learning content
:challenge,predicting student performance
:`data characteristic`,noise in student-generated data
:method,graph-based methods
:interaction,learner-question interactions
:scenario,cold start
:interaction,limited interactions
:strategy,integrating signed graph neural networks and large language model embeddings
:model,signed graph neural network
:data,student answers
:`data characteristic`,noise resilience
:data,question embeddings
:platform,PeerWise
:data,real-world datasets
:metric,predictive accuracy and robustness
:`task type`,classification
:approach,existing MLTC approaches
:resource,annotated data
:concept,label space structure constraints
:problem,MLTC in annotation-free and scarce-annotation settings
:setting,limited supervision
:method,natural language inference
:process,mapping preliminary label likelihoods
:graph,signed label dependency graph
:data,label descriptions
:process,message passing
:function,collective loss function
:framework,proposed MLTC framework
:performance,effective performance under low supervision
:metric,example-based F1 score
:field,chest x-ray diagnosis
:task,radiologist's focus detection
:approach,black-box models
:pipeline,controllable interpretable pipeline
:question,radiologist's cognitive process
:approach,masking out irrelevant information
:problem,erroneous information extraction
:task,interpretation process control
:dataset,eye gaze dataset
:heatmap,attention heatmap
:task,chest x-ray classification
:task,CLEF 2023 Lab Task 2: Longeval-Classification
:task,sentiment analysis
:technique,date-prefixed textual inputs
:technique,self-labeling
:technique,novel augmentation strategy
:framework,proposed framework
:metric,overall score
:metric,Relative Performance Drop (RPD)
:technique,non-augmented self-labeling
:`application domain`,mobile application test script generation
:field,software testing
:task,test script generation
:challenge,test script generation limitations
:factor,device and platform diversity
:capability,cross-platform generation
:task,cross-app migration
:factor,UI and app architecture diversity
:field,test automation
:goal,enhanced software testing practices
:community,app developers
:technique,adapter-style efficient transfer learning
:limitation,single modality task-specific knowledge
:limitation,overlooking inter-class relationships
:strategy,GraphAdapter
:graph,dual knowledge graph
:graph,textual knowledge sub-graph
:graph,visual knowledge sub-graph
:feature,textual feature of each prompt
:technique,adapter-based methods
:resource,GraphAdapter code
:url,https://github.com/lixinustc/graphadapter
:activity,prototyping generative applications
:activity,refining prototypes into products
:process,iterative prompt revision
:process,manual output evaluation
:system,EvalLM
:study,comparative study
:application,model evaluation and alignment
:phenomenon,widespread adoption
:task,next-word prediction
:approach,teleological approach
:factor,LLM accuracy factors
:concept,probability
:task,evaluation tasks
:task,decoding a simple cipher
:community,AI practitioners
:concept,low-probability situations
:entity,distinct system
:framework,connectionist temporal classification-based automatic speech recognition
:category,end-to-end ASR frameworks
:concept,token independence
:model,external language model
:property,fast parallel decoding
:model,acoustic model
:`data type`,speech
:process,cross-modal alignment
:`knowledge transfer`,context-dependent linguistic knowledge
:algorithm,cross-modal alignment algorithm based on optimal transport
:algorithm,optimal transport
:artifact,transport coupling matrix
:feature,latent acoustic feature
:`knowledge type`,context-dependent linguistic information
:system,conformer encoder-based CTC ASR system
:metric,character error rate
:system,baseline conformer CTC ASR system
:task,text evaluation
:issue,failure modes
:approach,ALLURE
:task,auditing LLMs
:technique,comparative evaluation
:technique,in-context learning (ICL)
:process,iterative refinement
:role,human annotators
:domain,textual data evaluation
:domain,medical summarization
:domain,education
:domain,productivity
:`research area`,NLP research
:method,supervised machine learning
:limitation,manual annotation process
:`model capability`,comprehension and generalization
:task,stance classification
:technique,prompting schemes
:`model type`,supervised models
:method,unsupervised stance detection
:benefit,reduced need for manual annotation
:scope,multiple languages
:`language feature`,figurative language
:task,question answering (QA)
:`question type`,yes/no questions
:capability,figurative language understanding
:dataset,FigurativeQA
:`model family`,BERT-based QA models
:context,figurative
:technique,automatic simplification
:model,ChatGPT with chain-of-thought prompting
:`research direction`,building robust QA models with figurative language understanding
:phenomenon,transformative impact
:value,online safety and public trust
:`research question`,LLM-generated misinformation harm
:taxonomy,LLM-generated misinformation taxonomy
:investigation,empirical investigation of LLM-generated misinformation
:content,LLM-generated misinformation
:content,human-written misinformation
:discovery,deceptive styles of LLM-generated misinformation
:task,combating misinformation
:era,age of large language models
:capability,open-world visual concept comprehension
:limitation,single-mode and holistic level semantic alignment
:concept,sample diversity
:framework,multi-mode token-level tuning
:concept,optimal transportation
:factor,multi-mode prompts discovery
:concept,diverse semantic representations
:factor,token-level alignment
:concept,fine-grained similarity
:problem,hierarchical transportation problem
:concept,modality-specific sets
:experiment,extensive experiments on image recognition benchmarks
:capability,superior generalization and few-shot abilities
:capability,capturing diverse visual concepts
:`assistant type`,voice assistant
:`model family`,traditional language model
:capability,contextual understanding
:study,exploratory study
:model,ChatGPT-powered voice assistant
:scenario,medical self-diagnosis
:scenario,creative planning
:scenario,debate
:issue,intent recognition failure
:capability,resilient and fluid interaction
:guideline,design guidelines for LLMs in voice assistance
:`data source`,open-sourced text data
:community,speech technology community
:`data quality`,in-the-wild large-scale speech data issues
:`data problem`,speech data quality problems
:task,human annotation of speech data
:characteristic,time-consuming and costly
:framework,AutoPrep
:task,speech data preprocessing
:component,AutoPrep components
:dataset,WeNetSpeech
:dataset,AutoPrepWild
:metric,DNSMOS and P.863 MOS scores
:system,TTS system
:metric,in-domain speaker similarity
:task,pick-and-place
:task,pick task
:task,place task
:problem,placing objects
:framework,teleoperation
:aspect,stability robustness
:aspect,contextual reasonableness
:method,simulation-driven physical stability verification
:capability,semantic reasoning
:approach,real-to-sim
:method,proposed method
:output,probability distribution over placement candidates
:evaluation,extensive evaluation
:environment,simulation and real-world environments
:quality,physical plausibility and contextual soundness
:task,video chapter segmentation
:benefit,quick navigation
:issue,lack of datasets
:dataset,VidChapters-7M
:method,scraping user annotations
:task,video chapter generation
:task,video chapter generation with ground-truth boundaries
:task,video chapter grounding
:`model family`,video-language model
:task,dense video captioning
:finding,dataset size correlation
:resource,"dataset, code, and models"
:URL,https://antoyang.github.io/vidchapters.html
:`model component`,speech encoder
:`connector structure`,fully connected layers
:study,comparative study of connector structures
:`connector structure`,multi-head cross-attention
:`connector structure`,Q-Former
:dataset,Common Voice
:dataset,GigaSpeech
:model,Whisper baseline ASR model
:dataset,Eval2000
:`connector structure`,segment-level Q-Former
:capability,recognising longer speech segments
:`model type`,self-supervised pre-trained speech model
:issue,domain shifts and accented speech
:technique,quantisation or clustering
:concept,discrete acoustic units
:method,unsupervised correction of discrete units
:issue,accented speech
:model,masked language model
:concept,standard accent discrete units
:issue,accented token sequence
:component,accent adapter blocks
:issue,target accent
:model,HuBERT Large
:task,accented speech recognition
:technique,low rank decomposition
:goal,compression
:property,differentiability and trainability
:model,Starcoder 16B
:model,Starcoder compressed
:benefit,inference speedup
:technique,QLoRA over low rank decomposition
:goal,memory reduction
:field,LLM compression
:application,brain-computer interface
:process,translation of brain dynamics into natural language
:method,eye-tracking fixations
:process,segmentation of brain dynamics
:framework,DeWave
:task,EEG-to-text translation
:encoder,quantized variational encoder
:representation,discrete codex
:training,text-EEG contrastive alignment training
:metric,BLEU-1 and ROUGE-F scores
:task,EEG signal period translation
:activity,programming learning
:demographic,novice programmers
:model,Codex
:task,code-authoring
:property,prompt crafting patterns
:property,AI-generated code quality
:behavior,utilization of AI-generated code
:approach,AI single prompt
:outcome,high correctness in code-authoring
:approach,AI step-by-step
:approach,hybrid coding
:approach,manual coding
:phenomenon,over-reliance on AI
:phenomenon,self-regulation in learning
:opportunity,curriculum development
:opportunity,tool development
:model,DALL-E 2
:content,realistic images
:model,Midjourney
:issue,unsafe content proliferation
:`safety mechanism`,NSFW content restriction
:model,text-to-image models
:framework,SurrogatePrompt
:technique,attack prompt creation
:evaluation,SurrogatePrompt evaluation
:metric,88% success rate
:content,counterfeit images
:theme,political figures in violent scenarios
:assessment,subjective and objective assessments
:issue,safety hazards
:approach,speaker anonymization
:process,extraction and perturbation
:technology,vocoder
:technology,x-vector
:challenge,consistent control
:issue,speaker information retention
:approach,neural audio codec-based speaker anonymization
:feature,quantized codes
:event,Voice Privacy Challenge 2022
:technique,internal language model subtraction
:model,RNN-Transducer
:`training method`,sequence discriminative training
:`training criterion`,maximum mutual information
:`training criterion`,minimum Bayes risk
:`model family`,neural transducer
:effect,benefit of ILM subtraction
:`model component`,zero-encoder ILM estimation
:`model component`,encoder and prediction + joint network
:`model family`,multi-modality large language model
:benchmark,Q-Bench
:dataset,LLVisionQA
:ability,low-level visual perception
:dataset,LLDescribe
:ability,low-level visual description
:strategy,softmax-based
:ability,visual quality assessment
:evaluation,MLLMs evaluation
:ability,low-level visual skills
:website,https://vqassessment.github.io/q-bench
:capability,knowledge storage
:capability,knowledge extraction
:uncertainty,source of knowledge
:source,Wikipedia biographies
:data,semi-synthetic biography data
:study,knowledge extraction ability
:relationship,knowledge extraction and training data diversity
:technique,linear probing
:concept,hidden embedding
:concept,token embeddings
:function,planners
:limitation,fixed set of skills
:method,querying new skills
:task,teaching robots new skills
:system,LLM-based skill acquisition system
:concept,open world and lifelong learning
:framework,LLM-based planning framework
:evaluation,multiple tasks in simulation and real world
:resource,videos
:website,https://sites.google.com/mit.edu/halp-robot-learning
:method,cluster language model
:task,product search
:`model architecture`,bi-encoder
:algorithm,k-means clustering
:technique,query clustering
:`model family`,global language model
:concept,local manifolds
:outcome,accurate and personalized retrieval
:model,voice conversion model
:technique,text instructions
:attribute,versatility and specificity
:process,discrete code sequence processing
:technique,style prompts
:approach,separate encoders approach
:attribute,end-to-end speech information handling
:outcome,comprehending instructions and delivering results
:`model family`,multi-modal model
:limitation,managing interleaved image-and-text inputs
:challenge,resource allocation and data accessibility
:framework,DeepSpeed-VisualChat
:`model family`,large vision and language model
:feature,multi-round and multi-image dialogues support
:mechanism,multi-modal causal attention
:technique,data blending
:attribute,scalability
:`model family`,multi-modal language model
:field,future explorations in multi-modal language models
:content,harmful or malicious content
:research,alignment of LLMs with human values
:content,inappropriate content
:vulnerability,alignment vulnerability
:attack,alignment-breaking attacks
:model,robustly aligned large language model (RA-LLM)
:model,aligned large language model
:concept,effectiveness in defense
:experiment,real-world experiments on RA-LLM
:attack,adversarial prompts
:attack,handcrafted jailbreaking prompts
:technique,counterfactual example
:technique,multimodal counterfactual example
:challenge,creating paired image-text data
:framework,scalable framework for automatic generation of counterfactual examples
:model,text-to-image diffusion models
:dataset,COCO-Counterfactuals
:dataset,MS-COCO
:`model family`,multimodal model
:technique,training data augmentation
:`agent type`,LLM-based AI agent
:`agent type`,traditional AI agent
:capability,natural language handling
:component,memory system
:`design aspect`,AI agent memory design
:research,in-depth research of AI agent components
:goal,advancement of AI agent technology
:goal,further research in AI agents
:audience,scholars and researchers
:process,automated rule interpretation
:domain,building codes
:task,manual filtering of interpretable clauses
:concept,machine interpretability
:category,clause classification
:dataset,clause classification dataset
:task,model training
:model,domain-specific language model
:task,text classification
:method,quantitative evaluation
:algorithm,text classification algorithm
:analysis,building codes interpretability analysis
:metric,average interpretability
:goal,improving interpretability
:field,humanities and social sciences
:framework,systematic mixed methods
:value,transparency and replicability
:`proof of concept`,16 machine-assisted case studies
:task,qualitative analytic tasks
:language,smaller languages and historical texts
:role,research instruments
:annotation,LLM and human annotations
:issue,errors and variation
:method,bootstrapping approach
:replication,case study replications
:benefit,efficiency
:approach,LLM-assisted research
:value,researcher augmentation
:expertise,qualitative expertise
:`model component`,vision encoder
:aspect,layout and visual elements
:characteristic,adaptability to new tasks
:approach,LLM-only
:strategy,serializing textual information
:analysis,quantitative analysis
:analysis,comprehensive analysis
:finding,LLM-only strategy performance
:`evaluation framework`,evaluation framework for dataset selection
:task,dataset selection
:impact,carbon footprint
:process,LLM lifecycle processes
:`emission type`,operational and embodied carbon emissions
:task,estimating carbon impact
:`model family`,emerging large language model
:hardware,GPU
:tool,MLCO2
:limitation,MLCO2 limitations
:model,LLMCarbon
:task,legacy software maintenance
:resource,engineering hours
:`programming language`,assembly code
:task,human analysis
:approach,conventional program translation
:attribute,correctness
:approach,learned transpilation
:task,manual re-writing
:approach,automated symbolic program translation
:challenge,scaling to longer programs
:approach,probabilistic neural language model
:attribute,plausible outputs
:approach,neurosymbolic approach
:method,Guess & Sketch
:approach,symbolic solver
:resource,training and evaluation dataset
:task,assembly code transpilation
:capability,knowledge manipulation
:`manipulation type`,retrieval
:`manipulation type`,comparison
:`manipulation type`,inverse search
:technique,chain of thoughts (COTs)
:task,classification or comparison
:experiment,controlled experiment
:finding,inefficient knowledge manipulation
:technique,instruct fine-tuning
:`compound class`,functional peptide
:application,disease treatment
:property,therapeutic efficacy and low toxicity
:technique,AI-based computational strategy
:task,functional peptide identification
:model,protein language model-based embeddings (ESM-2)
:tool,PLMFPpred
:task,toxic peptide identification
:technique,SMOTE-Tomek data synthesis sampling
:problem,data imbalance
:technique,Shapley value-based feature selection
:goal,computational cost reduction
:metric,"accuracy, AUC-ROC, F1-score"
:benchmark,current methods
:innovation,new computational method
:capability,dynamic capabilities
:task,calibrating LLM interactions
:stakeholder,interface designers
:stakeholder,end-users
:issue,limited grasp of human cognitive processes
:field,human-computer interaction
:model,Norman's Gulfs of Execution and Evaluation
:process,envisioning
:misalignment,knowing LLM task capability
:misalignment,instructing LLM for task
:misalignment,evaluating LLM output success
:recommendation,narrowing the envisioning gulf
:task,log anomaly detection
:goal,system security and reliability
:method,deep learning models
:concept,log sequences as natural language
:method,deep sequential models
:`model architecture`,LSTM
:gap,language modeling and anomaly detection
:task,anomaly detection
:framework,LogGPT
:task,next log entry prediction
:task,textual data assessment
:`model family`,generative pre-trained transformer
:framework,analysis framework
:data,human-generated and GPT-based essays
:task,human essay quality scoring
:data,GPT-generated text
:`model architecture`,CNN/RNN
:data,human-authored text
:technology,generative AI
:`research area`,text-to-video generation
:goal,semantic and temporal coherence
:approach,zero-shot text-to-video generation
:value,data and cost efficiency
:pipeline,Free-Bloom
:task,semantic-coherent video generation
:role,director
:`model family`,latent diffusion model
:role,animator
:modification,annotative modifications
:product,high-quality videos
:extension,LDMS-based extensions
:community,CHI community
:literature,LLM research
:perception,controversial perceptions of LLMs
:population,lay people
:method,systematic review
:perception,people's perceptions toward LLMs
:data,231 retrieved papers
:method,full-text review
:concept,biases in LLMs
:`application area`,LLM application areas
:perception,evaluators' perceptions toward LLMs
:factor,factors influencing perceptions
:concern,LLM application concerns
:`computation aspect`,computation characteristics
:`parallelism type`,data parallelism
:`computation aspect`,batch size
:`parallelism type`,tensor parallelism
:`computation aspect`,hidden dimension
:`parallelism type`,pipeline parallelism
:`computation aspect`,number of layers
:`parallelism type`,sequence parallelism
:`computation aspect`,sequence length
:issue,memory-communication inefficiency
:methodology,Deepspeed-Ulysses
:goal,efficient and scalable LLM training
:technique,all-to-all collective communication
:analysis,theoretical communication analysis
:benchmark,SOTA baseline
:`use case`,text-based content moderation
:task,content moderation
:task,rule-based community moderation
:task,toxic content detection
:observation,marginal benefit of increased model size
:research,content moderation with LLMs
:`model family`,large multimodal model
:issue,multimodal misalignment
:task,vision-language alignment
:goal,maximizing human rewards
:algorithm,factually augmented reinforcement learning from human feedback
:data,human-written image-text pairs
:benchmark,MMHAL-Bench
:task,evaluating real-world scenarios
:benchmark,LLAVA-Bench
:resource,"code, model, data"
:url,https://llava-rlhf.github.io
:method,Learning by Teaching (LBT)
:outcome,knowledge gap identification
:agent,teachable agent
:task,programming subject-specific knowledge
:issue,over-competence
:activity,teaching
:technique,prompting pipeline
:outcome,effective knowledge-building
:environment,TeachYou
:field,algorithm learning
:chatbot,Algobo
:capability,simulating misconceptions
:evaluation,technical evaluation
:study,between-subject study
:outcome,knowledge-dense conversations
:discussion,design implications
:skill,writing capabilities
:task,evaluating writing creativity
:test,Torrance Test of Creative Thinking (TTCT)
:technique,Consensual Assessment Technique
:test,Torrance Test of Creative Writing (TTCW)
:dimension,"fluency, flexibility, originality, elaboration"
:profession,creative writer
:story,LLM-generated stories
:story,professional author stories
:role,assessor
:assessment,expert assessment
:model,DictaLM
:attribute,7B parameters
:data,Hebrew-centric data
:license,Creative Commons
:model,DictaLM-Instruct
:model,DictaLM-Rab
:task,Hebrew-specific tasks
:task,instruction
:task,Q&A
:community,Hebrew NLP community
:need,improved quantization methods
:`model family`,diffusion model
:`data format`,FP8
:process,post-training quantization
:concept,trade-off between dynamic range and precision
:workflow,quantization workflow
:concept,generalization across network architectures
:`data format`,INT8
:`data format`,e4m3
:`data format`,e3m4
:resource,Intel Neural Compressor
:code,quantization workflow code
:issue,susceptibility and complexity
:question,necessity of human-generated demonstrations for ICL
:strategy,Self-Contemplation Prompting (SEC)
:process,LLM-generated demonstrations
:`learning paradigm`,in-context learning with hand-crafted demonstrations
:capability,self-sufficient decision making
:resource,SEC code
:URL,https://github.com/ruili33/sec
:issue,computational burden
:algorithm,quantization-aware low-rank adaptation
:concept,degrees of freedom
:technique,group-wise operators
:process,model quantization and integration
:data,fine-tuning datasets
:resource,QA-LoRA code
:url,https://github.com/yuhuixu1993/qa-lora
:`model family`,personal large model
:classification,model levels
:`model level`,personal level
:`model level`,expert level
:`knowledge domain`,specific knowledge
:`model level`,traditional level
:`knowledge domain`,universal knowledge
:entity,user
:task,language and vision tasks
:task,bug identification and fixing
:field,programming
:task,program repair
:issue,comprehension difficulties
:approach,minimal repair edits suggestion
:model,CodeT5
:data,code pairs
:method,baseline models evaluation
:metric,pass@100 and average edit distance
:task,model adaptation
:technique,parameter-efficient tuning
:method,Continual Parameter-Efficient Tuning
:goal,continual task adaptation
:concept,versioning
:method,Static Continual Parameter-Efficient Tuning
:goal,cost reduction
:method,Dynamic Continual Parameter-Efficient Tuning
:resource,codes and datasets
:concept,factual knowledge
:framework,knowledgeable in-context tuning
:`model family`,auto-regressive large language model
:task,screenshot captioning
:resource,datasets and use cases
:`model family`,image captioning model
:technique,adapter methods
:field,vision-language tasks
:technique,freezing parameters
:outcome,performance comparable to full model fine-tuning
:benefit,reduction in parameters
:field,vision-language models
:task,domain-specific text classification
:problem,scarce labeled data
:technique,prompt-learning
:attribute,"customizability, adaptability, cost-effectiveness"
:model,T5-base
:technique,active few-shot sampling
:outcome,performance gain
:technique,ensemble strategy
:model,FLAN-T5-large
:technique,optimized prompt
:domain,business use cases
:task,document analysis
:`model family`,extractive language model
:approach,extractive QA model integration
:task,feature extraction
:`document type`,German business documents
:`model family`,German QA models
:task,tailored extraction tasks
:data,small set of annotated data
:metric,scoring metrics
:task,information extraction evaluation
:metric,combined metric
:goal,increase productivity
:taxonomy,DANTE
:tool,software engineering tools
:`tool category`,AI-based tools
:analysis,generative language model limitations
:concept,novel tools
:model,Random Language Model
:concept,ensemble of stochastic context-free grammars
:process,first language learning
:phenomenon,transition to grammatical syntax
:concept,symmetry breaking in learning
:scenario,robustness against symmetry breaking
:data,human syntax networks clustering coefficient
:task,zero-shot image classification
:study,prior studies on few-shot learning
:task,visual recognition
:adapter,NTUA
:task,learning with few-shot unlabelled samples
:concept,key-value cache
:design,adaptive cache formation
:design,pseudo-label rectification
:requirement,hardware requirements
:trend,variety of LLMs
:proposal,integration of LLMs in cognitive architectures for autonomous robots
:application,cognitive architectures for autonomous robots
:tool,llama_ros
:environment,ROS 2-based environments
:`cognitive architecture`,Merlin2
:impact,incorporating LLMs in cognitive architecture
:task,software modeling
:problem,bidirectional traceability
:task,integrating requirements
:task,generating design models and code
:task,bridging the gap
:task,providing corrections
:activity,question-based activities
:tool,automated question generation tool
:activity,formative and summative assessment
:survey,expert survey of teachers
:task,automatic question generation
:framework,modular framework
:module,question generation module
:module,correct answer prediction module
:module,distractor formulation module
:evaluation,quantitative and qualitative evaluation
:concept,progress
:concept,concerns
:issue,misleading or detrimental texts
:technique,alignment
:value,human values
:survey,alignment methodologies for LLMs
:field,AI alignment
:issue,interpretability and adversarial vulnerabilities
:technique,LLM alignment assessment
:method,benchmarks and evaluation methodologies
:research,alignment research for LLMs
:concept,future research avenues
:goal,bridging research communities
:community,AI alignment research community
:community,capability exploration researchers for LLMs
:algorithm,Monte-Carlo Tree Search
:task,natural language text generation
:algorithm,Proximal Policy Optimization
:algorithm,PPO-MCTS
:component,value network
:issue,scoring mechanism mismatch
:metric,preferability of generated text
:algorithm,search algorithms
:system,autonomous system
:`model type`,fixed-class model
:system,indoor mobile and assistive autonomous system
:system,Lexis
:capability,scene understanding and place recognition
:technique,topological SLAM graph
:task,room classification and segmentation
:task,loop closure search
:concept,semantically relevant places
:`data type`,public and simulated data
:time,2018
:concept,context-aware computing
:time,approximately 1998
:application,innovative applications
:technology,ontology
:task,context modeling and reasoning
:paradigm,LLM-driven context-aware computing (LCAC)
:agent,autoagent
:showcase,operating a mobile Z-arm for assisted living
:showcase,planning a trip and scheduling the itinerary
:task,listwise reranking
:approach,proprietary models with opaque APIs
:issue,non-reproducibility and non-determinism
:model,RankVicuna
:status,first fully open-source LLM for listwise reranking
:resource,RankVicuna code
:URL,https://github.com/castorini/rank_llm
:event,TREC 2019 and 2020 Deep Learning Tracks
:attribute,single-scene videos
:task,generating layouts and programs
:task,long video generation
:framework,VideoDirectorGPT
:task,multi-scene video generation
:component,video planner
:model,Layout2Vid
:task,grounded video generation
:attribute,layout and movement control
:feature,dynamic layout guidance control
:task,consistent long video generation
:ability,cognitive abilities
:study,evaluation of cognitive abilities in LLMs
:issue,lack of systematic evaluation
:protocol,CogEval
:task,systematic evaluation of cognitive capacities
:ability,planning ability
:model,GPT-3.5-Turbo-175B
:model,DaVinci-003-175B
:model,Google Bard
:model,Cohere-XLarge-52.4B
:model,Anthropic Claude-1-52B
:model,LLaMA-13B
:model,Alpaca-7B
:task,planning tasks
:concept,cognitive map
:topic,future directions
:system,neural language modeling system
:task,second-pass rescoring
:issue,computational cost
:method,low-rank decomposition
:model,rescoring BERT model
:domain,new domains
:technique,discriminative training objective
:technique,correlation-based regularization loss
:architecture,Low-Rank Adaptation Rescore-BERT (LoRB)
:benefit,decreased training times
:technique,textual prompt tuning
:technique,visual prompt tuning
:framework,Visual Prompt Adaptation (VPA)
:concept,learnable tokens
:performance,OOD generalization
:performance,corruption robustness
:performance,zero-shot recognition robustness
:task,motion prediction
:application,autonomous vehicles
:concept,spatiotemporal relationships
:framework,SEPT
:technique,self-supervised learning
:task,masking-reconstruction modeling tasks
:component,scene encoder
:concept,spatiotemporal understanding
:benchmark,Argoverse motion forecasting benchmarks
:`performance metric`,main metrics
:interface,conversational interface
:interface,standard chat-based conversational interface
:value,transparency and verifiability
:interface,InkSync
:feature,executable edits
:issue,factual errors
:method,3-stage approach
:study,usability studies
:outcome,more accurate and efficient editing
:outcome,improved user experience
:limitation,grounded knowledge capture
:work,existing work on knowledge graphs
:challenge,applying knowledge graphs to LLMs
:characteristic,large number of parameters and high computational cost
:question,leveraging pre-trained LLMs without scratch training
:method,Graph Neural Prompting (GNP)
:component,standard graph neural network encoder
:component,cross-modality pooling module
:component,domain projector
:component,self-supervised link prediction objective
:task,commonsense and biomedical reasoning tasks
:field,machine learning research
:task,writing code
:task,code translation
:infrastructure,compiler infrastructure
:infrastructure,LLVM compiler infrastructure
:dataset,182B token dataset of LLVM IR
:application,machine-learned compiler components
:analysis,statistical analysis
:model,ChatCounselor
:dataset,Psych8K
:`data source`,in-depth interviews
:benchmark,Counseling Bench
:task,quality assessment of counseling responses
:evaluation,ChatCounselor assessment
:evaluation,real-world counseling questions
:`model capability`,enhancement
:`data quality`,high-quality domain-specific data
:task,zero-shot or few-shot learning
:`research area`,visual question answering (VQA)
:method,aligning image and text embeddings
:resource,large-scale image-text dataset
:method,combining pretrained models for VQA
:method,natural language representation of images
:task,textual representation of images
:dataset,VQAv2
:challenge,serving LLMs efficiently
:constraint,large memory bottleneck
:technique,weight-only quantization
:problem,sub-4 bit quantization difficulty
:method,per-IC quantization
:problem,activation outlier effect
:observation,activation outliers affect input dimension
:finding,activation outliers do not dictate quantization difficulty
:problem,quantization difficulty
:framework,Adaptive Dimensions (AdaDim)
:method,prior quantization methods
:`performance improvement`,up to +4.7% on MMLU
:`model family`,base large language model
:`performance improvement`,up to +10% on HumanEval
:field,qualitative spatial reasoning
:application,geographical information systems
:capability,performing tasks
:task,qualitative spatial reasoning tasks
:calculus,RCC-8
:task,automatic code generation
:goal,coding efficiency
:challenge,generating high-quality and reliable code
:issue,lack of good programming practice
:study,empirical study on exception handling
:technique,knowledge-driven prompts
:challenge,exception handling
:approach,knowledge-driven prompt chaining-based code generation (KPC)
:evaluation,KPC-based approach evaluation
:data,code generation tasks
:result,KPC-based approach results
:`research area`,NLP problem-solving abilities
:dataset,NLPBench
:source,Yale University final exams
:model,GPT-3.5/4
:model,PaLM-2
:strategy,Chain-of-Thought (COT)
:evaluation,LLM performance
:strategy,Tree-of-Thought (TOT)
:strategy,advanced prompting strategies
:skill,scientific problem-solving
:task,speech recognition post-processing
:technique,task activation prompting
:system,first-pass recognition system
:task,out-of-domain tasks
:dataset,ATIS and WSJ
:concept,generalization power
:`data source`,scripted dialogues
:`model application`,conversational NLP models
:`data source`,spontaneous interactions
:phenomenon,communicative feedback
:phenomenon,negative feedback
:`data source`,spontaneous dialogues
:tool,neural dialogue act tagger
:language,multilingual data
:benchmark,human parity
:condition,adverse conditions
:condition,speech domain variations
:strategy,linguistic knowledge utilization
:challenge,speech ambiguity
:benchmark,open-source
:technique,ASR error correction with LLMs
:technique,n-best decoding
:task,transcription prediction
:dataset,Hyporadise (HP)
:data,n-best hypotheses and transcriptions
:technique,error correction with LLMs
:benchmark,traditional re-ranking methods
:task,token correction
:resource,pre-trained models
:paradigm,evaluation paradigm for ASR error correction
:field,semantic language understanding
:study,pilot study on semantic relation comprehension
:phenomenon,neural states during reading-comprehension
:method,joint analysis
:phenomenon,brain processing of words
:task,fixation-related EEG data classification
:result,word-level classification accuracy
:metric,validation accuracy
:phenomenon,eye fixations on words
:concept,word relevance
:attempt,first attempt to classify brain states at word level
:technology,reading-assisted technologies
:task,visual content decoding
:value,scientific and practical
:effort,recovering seen images from brain signals
:challenge,image quality and semantic mismatches
:process,speaking
:process,reconstructing pixel-level visual images
:model,MindGPT
:process,visual stimuli interpretation
:technique,visually guided neural encoder with cross-attention mechanism
:representation,neural representations of MindGPT
:quality,explainable
:experiment,MindGPT experiments
:outcome,truthful representation of visual information
:`brain region`,higher visual cortex (HVC)
:`brain region`,lower visual cortex (LVC)
:outcome,semantic information recovery
:application,image-language conversation agents
:challenge,video-based dialogue system
:field,video dialogue
:constraint,minimal GPU memory
:task,temporal modeling
:method,Branching Temporal Adapter (BT-Adapter)
:application,video domain
:component,temporal modeling branch
:strategy,asymmetric token masking
:`model family`,multimodal dialogue models
:performance,state-of-the-art zero-shot results
:benchmark,current video chatbots
:benchmark,video chatting
:trend,growth
:challenge,LLM selection for new tasks
:task,new task performance
:concept,router model
:problem,LLM selection
:task,binary classification
:method,learning model routers
:field,formal theorem proving
:framework,Lyra
:mechanism,correction mechanisms
:mechanism,Tool Correction (TC)
:process,post-processing of formal proofs
:mechanism,Conjecture Correction (CC)
:mechanism,error feedback
:problem,IMO problems
:process,generation refinement
:`research direction`,correction mechanisms in formal theorem proving
:capability,rich set of capabilities
:application,ChatGPT plugins
:risk,potential risks
:task,risk identification
:framework,ToolEmu
:evaluator,LM-based automatic safety evaluator
:task,quantifying risks
:benchmark,initial benchmark
:risk,agent failures
:need,developing safer LM agents
:action,lying
:tool,lie detector
:technique,logistic regression
:attribute,accuracy and generality
:pattern,lie-related behavioural patterns
:domain,multiple domains
:concept,disinformation detection system
:actor,disinformation spreaders
:technique,disinformation detection
:content,LLM-generated disinformation
:threat,advanced disinformation
:strategy,traditional disinformation detection
:research,formation and detection of disinformation
:field,disinformation research
:technique,pretraining
:challenge,adapting pretrained models on limited data
:issue,cross-modal gap
:method,parameter-efficient method
:technique,multimodal prompt learning
:state,frozen
:benchmark,video question answering benchmarks
:URL,https://engindeniz.github.io/vitis
:factor,pre-training data quality
:performance,foundation model performance
:process,data filtering
:benchmark,DataComp
:strategy,filtering strategy
:method,CLIP score computation
:problem,interference of scene text
:method,vision and language model usage
:task,training sample retrieval
:process,data distribution rebalancing
:goal,computational budget efficiency
:discussion,open questions
:application,network anomaly detection
:goal,reducing detection and response times
:challenge,model training and maintenance
:approach,explainable AI
:tool,HuntGPT
:application,intrusion detection
:model,random forest classifier
:framework,SHAP
:framework,LIME
:model,GPT-3.5 Turbo
:certification,Certified Information Security Manager (CISM)
:entity,mobile robot
:resource,pre-existing maps
:concept,contextual awareness and dynamic adaptability
:system,DynaCon
:component,"real-time feedback, object server, prompt engineering, navigation modules"
:concept,dynamic path planning
:experiment,DynaCon validation
:resource,source code and experiment videos
:url,https://sites.google.com/view/dynacon
:task,medical question answering
:characteristic,black-box nature
:technique,model editing
:strategy,comprehensive retrieval strategy
:task,extracting medical facts
:technique,query prompting
:dataset,MedQA-SMILE
:model,VICUNA
:`model family`,long-context large language model
:capability,"effective context windows of up to 32,768 tokens"
:technique,continual pretraining
:dataset,long texts upsampled dataset
:benchmark,research benchmarks
:model,70B variant
:model,GPT-3.5-Turbo-16K
:component,LLaMA's position encodings
:limitation,modeling long dependencies
:technique,long context continual pretraining
:technique,pretraining from scratch with long sequences
:concept,internal mechanisms of machine learning models
:task,localization
:technique,activation patching
:technique,causal tracing
:technique,interchange intervention
:literature,variants of activation patching
:outcome,disparate interpretability results
:task,circuit discovery
:observation,empirical observations
:argument,conceptual arguments for metric/method preference
:guideline,best practices of activation patching
:`research area`,fusion of large language model and robotics
:application,autonomous underwater vehicle human interaction
:system,OceanChat
:framework,LLM-guided task and motion planning
:task,high-level goal translation
:component,task planner
:task,task sequence generation
:component,motion planner
:task,executable motion plan mapping
:scheme,event-triggered replanning
:attribute,system robustness
:`simulation platform`,HoloEco
:task,photo-realistic simulation
:`performance metric`,success rate and computation time
:model,AnyModality Augmented Language Model (AnyMAL)
:capability,multimodal reasoning and text generation
:model,LLaMA-2 (70B)
:capability,text-based reasoning
:module,pre-trained aligner
:capability,modality-to-text conversion
:resource,multimodal instruction set
:evaluation,comprehensive empirical analysis
:methodology,human and automatic evaluations
:achievement,state-of-the-art performance on multimodal tasks
:`input modality`,diverse input modality signals
:`model characteristic`,memorization
:action,deletion of personal information
:right,right to be forgotten
:process,re-training the model
:characteristic,computationally expensive
:framework,teacher-student framework
:task,unlearning textual sequences
:method,leave-one-out ensemble method
:process,fine-tuning with aggregated supervision
:dataset,WikiText-103
:outcome,privacy-utility trade-offs
:concept,conceptual tool
:context,dialogue systems
:activity,composing responses
:framework,Think-Plan-Execute (TPE)
:capability,reasoning and planning
:process,response generation
:role,thinker
:context,dialogue context
:role,planner
:role,executor
:output,coherent response
:quality,response quality
:resource,full code and data
:activity,reproduction
:discrepancy,modality discrepancy
:domains,textual and acoustic modeling
:task,knowledge transfer from PLM to ASR
:framework,cross-modality knowledge transfer learning
:system,ASR system
:technique,Connectionist Temporal Classification
:technique,Sinkhorn attention
:process,cross-modality alignment
:technique,Transformer attention
:component,acoustic encoder
:dataset,AISHELL-1
:evaluation,ASR system performance
:technique,CTC greedy decoding
:system,ASR system with CMKT learning
:algorithm,memory-efficient finetuning
:hardware,one 48GB GPU
:method,Modular Low-Rank Adaptation (ModuLora)
:technique,weight quantization
:`model family`,3-bit large language model
:`quantization method`,3-bit OptQ
:`quantization method`,less sophisticated quantization methods
:performance,competitive performance on NLP tasks
:metric,ROUGE score
:library,LLMTools
:`model family`,low-precision large language model
:`model family`,3-bit instruction following Alpaca LLMs
:specification,programming specification
:issue,misalignment with specification
:technique,Chain of Thought (CoT)
:issue,complicated programming logic understanding
:technique,Test-Case-Driven Chain of Thought (TCoT)
:practice,human problem understanding
:process,programming understanding refinement
:evaluation,TCoT evaluation
:method,API recommendation
:technique,query clarification
:technique,knowledge graph-based query clarification
:issue,out-of-vocabulary failures
:approach,knowledge-guided query clarification
:solution,neural knowledge base
:output,clarification questions and options
:structure,knowledge graph
:task,noise filtering
:approach,AI chain
:metric,unit score
:metric,MRR
:metric,MAP
:concept,bridging KG and LLM
:concept,confidence dynamics
:concept,self-assessed confidence
:concept,actual performance
:method,diverse questionnaires and real-world scenarios
:phenomenon,Dunning-Kruger effect
:bias,underestimation bias
:concept,cognitive processes
:concept,self-assessment mechanism
:investigation,LLMs' self-assessment mechanism
:outcome,advancement of functionalities and broadening of potential applications
:technique,counterfactual explanations
:challenge,general user preferences
:challenge,variable machine learning systems
:method,Tree-based Conditions Optional Links (T-COL)
:tool,vaccine
:goal,global health
:event,adverse event
:task,adverse event extraction
:`data source`,Vaccine Adverse Event Reporting System (VAERS)
:model,LaMDA 2
:model,AE-GPT
:metric,micro F1 score
:task,medical data processing
:performance,human-level performance
:threat,ideology
:domain,sensitive domains
:technique,GPT soft ideologization
:concept,AI-self-consciousness
:technique,GPT self-conversations
:ability,ideology comprehension
:process,LLM ideology injection
:technique,LLM ideologization
:technique,government ideology manipulation
:attribute,advantages
:concept,data-centric AI
:component,data
:`data quality issue`,dirty samples
:`model type`,Deep Neural Networks (DNNs)
:task,detecting dirty samples
:goal,dataset quality and reliability
:detector,existing detectors
:`data quality issue`,poisoned samples and noisy labels
:issue,visual-linguistic inconsistency
:system,Versatile Data Cleanser (VDC)
:module,visual question generation
:module,visual question answering
:module,visual answer evaluation
:challenge,control of text generation
:model,Residual Memory Transformer
:architecture,encoder-decoder setup
:benchmark,state-of-the-art approaches in CTG
:quality,effectiveness and versatility
:domain,chemistry
:`model family`,scientific language model
:entity,"small molecules, proteins, polymers"
:process,molecule discovery cycle
:domain,early-stage drug discovery
:task,de novo drug design
:task,property prediction
:task,reaction chemistry
:asset,open-source software
:field,scientific language modeling
:vision,future molecular design
:tool,computational chemistry tools
:contribution,this review
:domain,chemical discovery
:issue,AI safety
:risk,misuse of AI systems
:method,Direct Preference Optimization (DPO)
:constraint,reverse KL regularization
:approach,f-DPO
:model,Bradley-Terry model
:concept,optimal policy
:goal,balance of performance and diversity
:method,PPO-based methods
:concept,divergence constraints
:metric,Expected Calibration Error (ECE)
:trend,availability of LLMs
:benchmark,knowledge graph question answering benchmarks
:task,evaluating KGQA systems
:method,pattern-based SPARQL query generation
:method,natural language question generation
:method,crowdsourcing
:issue,lack of generalization
:dataset,Spider4SPARQL
:resource,knowledge graphs and ontologies
:domain,various aspects
:domain,legal domain
:benchmark,LawBench
:capability,legal capabilities
:task,legal tasks
:evaluation,LawBench evaluation
:technique,fine-tuning on legal specific text
:goal,usable and reliable legal LLMs
:resource,LawBench GitHub repository
:approach,data-driven approach
:challenge,dataset bias
:challenge,uninterpretability
:capability,knowledge-driven capability
:activity,human driving
:framework,DILU
:capability,generalization ability
:application,practical autonomous driving systems
:contribution,knowledge-driven capability in autonomous driving
:`data type`,code data
:stage,LLM training stage
:strategy,dynamic mixing
:evaluation,reasoning capability assessment
:application,scientific question answering
:application,legal support
:resource,source code and model parameters
:url,https://github.com/yingweima2022/codellm
:`algorithm family`,reinforcement learning algorithm
:`task characteristic`,sparse and complex environments
:framework,Intrinsically Guided Exploration from Large Language Models (IGE-LLMs)
:`task characteristic`,long-horizon with sparse rewards
:`task characteristic`,exploration challenge
:`method family`,intrinsic methods
:`method family`,existing learning methods
:parameter,intrinsic scaling parameters
:`task characteristic`,increased uncertainty and horizons
:method,human feedback
:metric,preference score
:property,generated output properties
:issue,biases
:concept,error criteria coverage
:issue,confounders
:dimension,assertiveness and complexity
:dimension,assertiveness
:issue,factuality error perception
:metric,human annotations
:quality,reliability
:method,human feedback as training objective
:goal,desired objective alignment
:representation,latent representations
:`knowledge graph type`,static knowledge graph
:`knowledge graph type`,temporal knowledge graph
:`information type`,temporal information
:`information type`,textual descriptions
:`method category`,static knowledge graph completion
:`information type`,temporal and textual information
:study,recent studies
:framework,TEMT
:`information type`,textual and temporal information
:capability,predictions on unseen entities
:task,time interval prediction and triple classification
:framework,RLLTE
:concept,algorithm development toolkit
:concept,algorithm evolution acceleration
:concept,RL ecosystem
:component,model deployment
:component,benchmark hub
:tool,LLM-empowered copilot
:practice,RL engineering practice
:sector,industry and academia
:performance,zero-shot performance
:method,prompt templates
:task,creating descriptor sets
:method,zero-shot classifiers
:metric,cosine similarity
:method,AutoCLIP
:task,auto-tuning zero-shot classifiers
:attribute,low computational overhead
:issue,limited fact-checks
:approach,train-from-scratch
:resource,large-scale annotated data
:approach,pre-train-and-fine-tune
:issue,inconsistency between pre-training and downstream objectives
:resource,task-specific supervision
:paradigm,prompt-and-align
:task,few-shot fake news detection
:knowledge,pre-trained knowledge in PLMs
:issue,label scarcity
:graph,news proximity graph
:signal,veracity-consistent signals
:observation,user veracity consistency
:task,target-oriented grasping
:capability,language control
:task,grasping actions
:model,QWEnGrasp
:task,6-DOF grasping
:experiment,complete experiment with six-dimension instructions
:capability,comprehending human intention
:environment,open language environment
:technique,instruct training
:artifact,formal artifacts
:application,safety-critical applications
:technique,satisfiability modulo theories (SMT) solving
:interaction,LLM and SMT solver interaction
:domain,blocks
:experiment,evaluation of synthesis task
:solver,Z3
:technique,natural language to SMT solver query generation
:user,non-expert users
:capability,storing factual knowledge
:task,downstream NLP tasks
:`model architecture`,task-specific architecture
:challenge,provenance for model decisions
:`research area`,open research frontier
:challenge,maintaining up-to-date world knowledge
:solution,integration with differentiable access mechanisms to explicit non-parametric memory
:`model family`,language model augmented with external knowledge
:source,external knowledge sources
:type,external knowledge bases and search engines
:capability,contextual processing
:`research direction`,augmenting large language models with knowledge
:application,autonomous vehicle planning
:representation,continuous trajectories
:representation,discrete motion tokens
:task,multi-agent motion prediction
:model,MotionLM
:advantage,anchor-free multimodal distribution learning
:objective,language modeling objective
:approach,MotionLM's approach
:method,post-hoc interaction heuristics
:feature,temporally causal conditional rollouts
:benchmark,Waymo Open Motion Dataset
:achievement,1st on interactive challenge leaderboard
:paradigm,locate-then-edit
:task,changing factual knowledge in language models
:`research area`,locating methods in language models
:task,pinpointing exact parameters
:hypothesis,locality hypothesis of factual knowledge
:community,researchers
:task,testing hypothesis
:benchmark,KLoB
:property,essential properties of knowledge locating methods
:task,evaluating locating methods
:approach,unsupervised
:feature,semantically meaningful and compact
:approach,previous work
:framework,SFaVeL
:`loss function`,contrastive loss function
:feature,high-quality claim and evidence alignments
:benchmark,FEVER fact verification benchmark
:`model family`,proprietary language model
:paradigm,language-models-as-a-service (LMAAS)
:`model family`,open-source language model
:activity,evaluation and benchmarking
:concept,"accessibility, replicability, reliability, and trustworthiness (ARRT)"
:concept,solutions and recommendations
:resource,synthesized overview of LMAAS
:aspect,licences and capabilities
:technique,lightweight finetuning
:benchmark,MToB (Machine Translation from One Book)
:language,Kalamang
:attribute,low-resource
:method,L2 learning
:performance,baseline LLM performance
:metric,CHRF
:capability,LLM capabilities
:method,methods developed for MToB
:goal,access to language technology
:community,underserved communities
:tool,evaluation suite
:issue,inconsistent settings and prompts
:platform,OpenAI Evals
:tool,GPT-Fathom
:study,retrospective study on OpenAI's earlier models
:technique,Supervised Fine-Tuning (SFT)
:concept,alignment tax
:analysis,GPT-Fathom analysis
:`data modality`,graph data
:task,node classification
:technique,prompting methods
:task,encoding structural information
:feature,textual node features
:concept,data leakage
:concept,homophily
:performance,LLM performance on node classification
:`model series`,QWEN
:model,QWEN base
:model,QWEN-Chat
:technique,human alignment techniques
:capability,advanced tool-use and planning
:model,Code-QWEN
:specialization,coding
:model,Math-QWEN-Chat
:specialization,mathematics
:comparison,open-source models
:comparison,proprietary models
:perturbation,COT prompt order
:perturbation,COT prompt values
:perturbation,COT prompt operators
:technique,incorrect chain-of-thought prompting
:technique,correct chain-of-thought values
:outcome,correct answers
:technique,incorrect chain-of-thought demonstrations
:capability,contextual reasoning
:phenomenon,problematic smartphone use
:aspect,health
:technique,persuasive techniques
:quality,flexibility
:study,Wizard-of-Oz study
:`mental state`,problematic smartphone use mental states
:`mental state`,boredom
:`mental state`,stress
:`mental state`,inertia
:strategy,persuasion strategies
:strategy,understanding
:strategy,comforting
:strategy,evoking
:strategy,scaffolding habits
:capability,dynamic persuasion content generation
:technique,MindShift
:input,user context and behavior
:output,persuasive content
:experiment,5-week field experiment
:metric,intervention acceptance rates
:metric,smartphone use frequency
:user,MindShift users
:metric,smartphone addiction scale scores
:metric,self-efficacy
:potential,LLMs for context-aware persuasion
:task,task-driven perception and planning
:representation,3D representation
:issue,scalability and semantic spatial relationships
:representation,ConceptGraphs
:representation,3D scenes
:concept,novel semantic classes
:task,downstream planning tasks
:resource,project page
:resource,explainer video
:field,AI development
:capability,multimodality
:deployment,cloud-based deployment
:challenge,critical challenges
:technology,6G Mobile Edge Computing (MEC) systems
:application,killer applications
:challenge,LLM deployment at the edge
:architecture,6G MEC architecture for LLMs
:`design aspect`,edge training
:`design aspect`,edge inference
:technique,cutting-edge techniques
:`design aspect`,efficient deployment
:capability,reasoning abilities
:technique,hand-crafted prompt strategies
:quality,sub-optimality
:system,PromptBreeder
:mechanism,self-referential self-improvement
:process,mutation and evaluation of task-prompts
:process,mutation of task-prompts
:technique,mutation-prompts
:technique,state-of-the-art prompt strategies
:benchmark,arithmetic and commonsense reasoning benchmarks
:task,hate speech classification
:robot,humanoid robot
:language,American Sign Language (ASL)
:model,lightweight and efficient model
:system,embedded systems
:interaction,intelligent robot interactions
:response,co-speech gesture responses
:dialogue,humanoid-robot dialogues
:software,integrated software pipeline
:model,socially aware AI interaction model
:robot,Pepper
:interaction,non-verbal interaction
:task,argumentative reasoning
:task,argument mining (AM)
:task,argument pair extraction (APE)
:concept,input and output representation
:phenomenon,exemplar effect
:number,4-5 exemplars
:technique,chain-of-thought (COT) prompting
:context,multilingual speech signal
:model,hybrid CTC/Attention ASR model
:technique,multi-level language information biasing
:information,frame- and token-level language posteriors
:method,interactive language biases
:concept,language bias interaction
:challenge,ASRU 2019 Code-Switching Challenge
:benchmark,baseline performance
:concept,language bias effects
:concept,language bias
:task,internal language modeling
:challenge,computational complexity
:mechanism,self-attention
:hardware,AI hardware accelerators
:hardware,Habana Gaudi
:component,Matrix Multiplication Engine (MME)
:component,Tensor Processing Cores (TPC)
:strategy,optimization of MME and TPC utilization
:study,Gaudi's capabilities for Transformers
:community,AI practitioners and researchers
:issue,label noise
:data,pre-training datasets
:goal,understanding and mitigating noise impact
:experiment,supervised pre-training models on synthetic noisy datasets
:concept,transfer performance
:dataset,ImageNet-1K
:dataset,YFCC15M
:phenomenon,noise shaping feature space
:method,NMTune
:goal,mitigating noise impact
:`model family`,vision and language models
:`research direction`,noisy model learning
:domain,medicine
:application,medical applications
:vulnerability,targeted manipulation
:action,injecting incorrect facts
:outcome,erroneous information propagation
:task,biomedical tasks
:data,"1,038 incorrect biomedical facts"
:value,security and trustworthiness
:measure,protective measures and verification mechanisms
:value,reliability and safety
:role,automatic evaluators
:technique,preference ranking
:benchmark,Cognitive Bias Benchmark for LLMs as Evaluators (COBBLER)
:concept,cognitive biases in LLM evaluation
:bias,egocentric bias
:finding,LLMs as biased text quality evaluators
:correlation,human and machine preferences
:metric,Rank-Biased Overlap (RBO) score
:conclusion,LLMs misaligned with human preferences
:project,COBBLER project page
:url,https://minnesotanlp.github.io/cobbler
:domain,critical domains
:technique,SHAP values
:concept,feature importance
:concept,user understanding
:tool,XAIStories
:concept,narrative explanations
:tool,SHAPStories
:tool,CFStories
:audience,general audience
:profession,data scientists
:concept,narrative creation efficiency
:concept,AI prediction understanding
:framework,SCALE
:`model family`,specialized translation model
:`translation direction`,Xhosa to English
:model,compact model with 600M parameters
:`model family`,english-centric specialized translation model
:aspect,robustness and translation characteristics
:trend,advancements in AI
:task,learning from text data
:user,general public
:field,spatial data management
:system,LLM-based spatial database system
:data,structured and unstructured spatial data
:capability,seamless access to spatial knowledge
:stakeholder,individuals
:stakeholder,business
:stakeholder,government policy makers
:aspect,modern life
:issue,lack of specificity in responses
:framework,Reinforcement Learning from Contrastive Feedback (RLCF)
:technique,contrastive feedback
:`reward function`,Batched-MRR
:technique,instruction-based image editing
:attribute,controllability and flexibility in image manipulation
:challenge,brief human instructions
:task,cross-modal understanding and visual-aware response generation
:technique,MLLM-guided image editing
:attribute,expressive instructions
:model,editing model
:attribute,visual imagination
:task,photoshop-style modification
:task,global photo optimization
:task,local editing
:metric,automatic metrics and human evaluation
:task,coding tasks
:task,knowledge graph engineering
:`evaluation system`,LLM-KG-Bench
:task,knowledge graph tasks
:model,Claude 1.3
:model,Claude 2.0
:model,GPT4all Vicuna
:model,GPT4all Falcon 13B
:`model family`,commercial large language model
:requirement,output formatting constraints
:task,knowledge-based visual question answering
:resource,external knowledge bases
:framework,retrieval-augmented visual question answering
:technique,dense passage retrieval
:method,fine-grained late-interaction multi-modal retrieval
:limitation,incomplete and inaccurate image representations
:limitation,one-dimensional relevance scoring
:technique,multi-dimensional embeddings
:metric,PRRecall@5
:`model family`,large multi-modal/language model
:dataset,OK-VQA
:trend,ubiquity in research
:field,social science
:`data type`,large-n qualitative data
:event,interviews with Rohingya refugees
:risk,bias introduction
:task,text annotation
:bias,technical bias
:`model type`,supervised model
:`data quality`,high-quality human annotations
:`model type`,bespoke model
:`interaction paradigm`,user-server interaction
:user,users wanting privacy
:framework,LatticeGen
:technique,noised lattice
:server,hypothetically malicious server
:attack,repeated beam-search attack
:scheme,mixing noise scheme
:concept,text protection
:concept,generation quality
:benchmark,evaluation benchmarks
:benchmark,current benchmarks
:`evaluation protocol`,DyVAL
:structure,directed acyclic graph
:`evaluation set`,DyVAL-generated evaluation samples
:task,clinical deep phenotyping
:method,ontology concept modelling
:ontology,Human Phenotype Ontology
:task,phenotype concept recognition
:standard,gold standard for phenotype recognition
:result,F1 score
:tool,best in class tool for phenotype recognition
:issue,non-deterministic outcomes
:setting,clinical settings
:domain,fantasy literature
:activity,entity detection and categorization
:domain,Dungeons and Dragons (D&D)
:activity,entity annotation
:model,Flair
:model,Trankit
:model,spaCy
:task,decision-making
:condition,sparse-reward environments
:framework,RLAdapter
:environment,Crafter
:agent,agents under RLAdapter
:behavior,common-sense behaviors
:technique,sampling
:technique,beam search
:prompt,chain-of-thought (CoT)
:capability,reasoning and decoding ability
:method,tree-of-thought (ToT)
:method,reasoning via planning (RaP)
:framework,AlphaZero-like tree-search framework for LLMs (TS-LLM)
:limitation,lack of general applicability and scalability
:technique,learned value function
:process,decoding
:task,RLHF alignment
:format,fp8
:process,training and inference
:issue,degradation due to reduced dynamic range
:format,int
:topic,scaling selection
:methodology,scaling selection for fp8 linear layers
:component,fp8 linear layers
:`model size range`,111M to 70B
:visualization,per-tensor scale distribution plots
:task,real-world tasks
:issue,dedicated evaluation benchmarks
:concept,negotiation
:activity,communication and collaboration
:framework,scorable negotiation games
:testbed,diverse text-based negotiation games
:capability,"arithmetic, inference, exploration, and planning"
:task,negotiation games
:technique,zero-shot chain-of-thought prompting
:outcome,successful deals in negotiation games
:`model family`,earlier large language models
:concept,generalization
:aspect,interaction dynamics
:issue,prompt brittleness
:bias,contextual bias
:method,calibration methods
:method,batch calibration
:attribute,"zero-shot, inference-only, low-cost"
:model,"PaLM 2-(S, M, L)"
:task,natural language understanding tasks
:task,image classification tasks
:task,textual generation
:solution,aggregating multiple outputs
:framework,Multi-Perspective Self-Consistency (MPSC)
:technique,multipartite graph construction
:concept,consistency analysis
:evaluation,comprehensive evaluation
:tool,code interpreter
:concept,inter-consistency
:metric,pass@1
:`game type`,imperfect information game
:concept,decision-making under uncertainty
:data,massive passive data
:capability,knowledge retrieval and reasoning
:agent,Suspicion-Agent
:capability,adaptability in card games
:concept,Theory of Mind (ToM)
:strategy,planning strategy for GPT-4
:capability,competent gameplay
:experiment,evaluation in Leduc Hold'em
:resource,game-related data
:application,automated task-solving in multi-agent systems
:approach,LLM-based multi-agent
:agent,predefined agents
:framework,AutoAgents
:concept,adaptive generation and coordination of specialized agents
:concept,dynamic agent generation based on task content
:role,observer
:approach,existing multi-agent methods
:concept,team cooperation
:repository,AutoAgents project repository
:URL,https://github.com/link-agi/autoagents
:task,trading signal extraction
:task,backtesting trading strategies
:bias,look-ahead bias
:problem,biased backtesting
:bias,distraction effect
:strategy,sentiment-driven trading
:strategy,de-biased trading
:`company size`,larger companies
:challenge,reliance on large labeled datasets
:task,zero-shot video analysis
:challenge,constructing effective semantic space
:dataset,Stories
:approach,feature generation for zero-shot classification
:method,zero-shot transfer
:method,zero-shot classification
:benchmark,multiple benchmarks
:task,zero-shot action recognition
:url,https://github.com/kini5gowda/stories
:task,automated audio captioning
:output,informative descriptions
:`model architecture`,sequence-to-sequence
:technique,pretrained models leveraging
:model,Beats
:feature,fine-grained audio features
:model,Instructor LLM
:feature,text embeddings
:`loss function`,InfoNCE
:feature,language-modality knowledge
:technique,nucleus sampling
:algorithm,hybrid reranking
:model,proposed AAC model
:metric,SPIDEr-FL score
:event,2023 DCASE AAC Challenge
:challenge,translating reasoning into actions
:framework,"RAFA (Reason for Future, Act for Now)"
:process,orchestrating reasoning and acting
:technique,prompt template for reasoning
:process,learning and planning
:process,taking action and storing feedback
:process,replanning future trajectory
:concept,Bayesian Adaptive Markov Decision Processes (MDPs)
:technique,in-context learning and planning
:result,regret bound
:`performance metric`,sqrt(t) regret
:interplay,prior knowledge and uncertainty reduction
:validation,empirical validation
:knowledge,sensitive information
:content,toxic or harmful text
:framework,attack-and-defense framework
:task,deleting sensitive information from model weights
:approach,direct edits to model weights
:goal,safety and privacy
:`threat model`,sensitive information extraction
:`model editing method`,ROME
:phenomenon,information recovery after deletion
:observation,traces of deleted information in hidden states
:observation,editing method specificity
:`defense method`,new defense methods
:threat,extraction attacks
:problem,deleting sensitive information
:implication,societal implications
:concept,multimodal inputs
:capability,processing multimodal inputs
:quality,genericity
:field,multimodal task formulation
:document,GPT-4V contributions paper
:concept,training set
:process,data curation for pre-training
:method,heuristic filtering
:model,Data Filtering Network (DFN)
:finding,quality-performance distinction
:concept,network performance
:model,ImageNet model
:concept,training set quality
:dataset,image-text dataset
:dataset,DFN-5B
:model,ViT-H
:benchmark,ImageNet zero-shot transfer accuracy
:dataset,DFN-2B
:goal,dataset design research
:source,publicly available data
:concept,tool augmentation
:framework,CRAFT
:concept,tool creation and retrieval
:task,code solution collection
:process,validation
:concept,toolsets
:analysis,in-depth analysis of CRAFT
:outcome,performance insights
:resource,CRAFT code
:url,https://github.com/lifan-yuan/craft
:`model type`,text-conditioned diffusion model
:task,neural video generation
:challenge,intricate spatiotemporal prompts
:output,dynamic scene layouts
:concept,complex spatiotemporal dynamics
:standard,real-world motion patterns
:technique,attention map adjustment
:`model type`,video diffusion model
:approach,LLM-grounded video diffusion (LVD)
:benchmark,base video diffusion model and baseline methods
:attribute,training-free
:task,language-to-code generation
:`evaluation framework`,L2CEval
:domain,"semantic parsing, math reasoning, Python programming"
:metric,confidence calibration
:resource,evaluation framework and model outputs
:task,education feedback analysis
:goal,curriculum gap identification
:goal,teacher evaluation
:process,manual processing
:data,end-of-course survey comments
:practice,effective prompting
:concept,chain-of-thought reasoning
:goal,confidence in practice
:set,classification categories
:task,insight derivation from survey text
:task,complex mathematics
:model,TORA
:approach,tool-integrated reasoning
:capability,utilization of external tools
:technique,imitation learning
:technique,output space shaping
:capability,reasoning behavior refinement
:benchmark,mathematical reasoning datasets
:model,TORA-7B
:benchmark,MATH dataset
:model,TORA-Code-34B
:capability,tool interaction for mathematical reasoning
:application,streaming applications
:challenge,memory consumption
:process,decoding stage
:`text length`,longer than training sequence
:technique,window attention
:condition,text length surpasses cache size
:phenomenon,attention sink
:framework,StreamingLLM
:capability,generalize to infinite sequence lengths
:model,Llama-2
:model,MPT
:model,Falcon
:model,Pythia
:token,placeholder token
:process,streaming deployment
:baseline,sliding window recomputation
:URL,https://github.com/mit-han-lab/streaming-llm
:`model application`,text-to-speech model
:capability,in-context learning in zero-shot scenarios
:component,neural speech codec
:issue,excessive token sequences
:attribute,prediction accuracy
:model,TICODEC
:information,time-invariant information
:loss,time-invariant encoding consistency loss
:attribute,consistency of time-invariant code
:information,global information
:attribute,quality of reconstruction speech
:attribute,similarity and naturalness of synthesized speech
:task,complex reasoning assessment
:method,step-by-step reasoning evaluation
:metric,reference-based evaluation metrics
:resource,human-annotated reasoning chains
:issue,labor-intensiveness and non-uniqueness
:metric,reference-free reasoning metrics
:resource,human-crafted reasoning chains
:issue,complication and generalizability concerns
:task,reasoning chain quality evaluation
:method,Socratic method
:metric,reference-free reasoning evaluation
:metric,SOCREVAL
:framework,LLMs with the Socratic method
:qualities,cost-efficiency and robustness
:task,following instructions
:role,task-driven autonomous agents
:environment,virtual reality (VR)
:framework,Voice2Action
:task,canonical interaction subsets
:attribute,efficiency and accuracy
:environment,urban engineering VR environment
:process,writing process
:task,writing variation generation
:interface,current interfaces
:task,simultaneous consideration of multiple variations
:interface,Abscribe
:task,exploration of writing variations
:feature,rapid production of variations
:feature,in-place comparisons
:study,user study with 12 writers
:outcome,reduced task workload
:behavior,writers' exploration of variations
:objective,unified model for multiple tasks
:task,language domain tasks
:`model family`,unified graph model
:domain,graph learning
:challenge,graph data discrepancy
:task,single representation space
:task,graph tasks
:strategy,embedding strategies
:concept,graph prompting paradigm
:task,in-context learning on graphs
:framework,One for All (OFA)
:concept,text-attributed graphs
:concept,nodes-of-interest
:model,OFA model
:data,graph data from multiple domains
:task,learning scenarios
:task,graph classification
:`research direction`,personalized text generation
:`study focus`,specialized model design
:scenario,frozen large language model with API access
:task,improving text prompts
:method,manual text prompt improvement
:method,automatic prompt revision
:component,initial prompts
:`training paradigm`,supervised learning and reinforcement learning chain
:result,rewritten prompts outperformance
:analysis,in-depth analysis of rewritten prompts
:technique,self-alignment
:task,general instruction following
:domain,expert domain specialization
:domain,specialized domain
:technique,self-specialization
:data,domain-specific unlabelled data
:model,self-specialized model (30B)
:model,MPT-30B
:model,expert model
:concept,specialization efficiency
:challenge,exploring rich environments without prior knowledge
:method,MOTIF
:concept,grounding LLMs for decision-making
:game,NetHack
:outcome,higher game score
:outcome,intuitive human-aligned behaviors
:factor,LLM size and prompt information
:task,variety of tasks
:task,multi-step reasoning and goal-directed planning
:`biological structure`,prefrontal cortex
:concept,planning in LLMs
:function,PFC functions in isolation
:function,autonomous coordination of PFC functions
:architecture,black box architecture with multiple LLM-based modules
:task,challenging planning tasks
:method,standard LLM methods
:field,cognitive neuroscience
:task,scholarly document understanding
:`document aspect`,pragmatics
:corpus,scholarly documents
:attribute,19 disciplines
:task,learning domain-agnostic descriptors
:process,retrofitting
:analysis,position and ordering of descriptors
:relationship,discipline and structure
:community,scholarly communities
:concept,expression of work
:findings,structural archetypes and variability
:foundation,study findings
:task,future work in scholarly document understanding
:behavior,harmful behavior
:goal,beneficial behavior
:concept,comparison-based loss optimization
:concept,equivalent reward functions
:concept,trajectory-wise optimization
:framework,reinforcement learning with relative feedback
:concept,relative feedback
:algorithm,Pairwise Proximal Policy Optimization
:concept,comparative rewards
:concept,equivalent rewards
:approach,aligning LLMs to human preferences
:model,joint speech and language model
:characteristic,"multitask, multilingual, dual-modal"
:`model component`,adapter
:attribute,1% parameters
:task,speech recognition and speech translation
:capability,zero-shot instruction-following
:task,diverse generation tasks
:gap,representational gap
:`model family`,foundation models
:task,zero-shot segmentation
:paradigm,mask proposal classification
:capability,zero-shot transferability
:issue,insensitivity to mask proposals
:method,mask-aware fine-tuning (MAFT)
:component,IP-CLIP encoder
:task,mask proposal handling
:loss,mask-aware loss
:loss,self-distillation loss
:metric,mIoU for unseen classes
:resource,MAFT code
:content,non-factual or hallucinatory content
:challenge,hallucination detection
:method,AutoHall
:dataset,model-specific hallucination datasets
:method,zero-resource and black-box hallucination detection
:concept,self-contradiction
:experiment,hallucination detection performance
:benchmark,extant baselines
:experiment,hallucination proportions and types
:task,language understanding
:task,text-related tasks
:technique,chain of thought prompting
:task,grading reflective essays
:skill,critical thinking
:model,LaMDA-7B
:goal,user privacy
:resource,ultra-large-scale training corpora
:limitation,internal representations
:`strategy suite`,CoReX
:concept,human behaviors
:goal,annotation efficiency
:value,cost-effectiveness
:perspective,surface repetitions
:feature,surface features
:principle,token co-occurrence reinforcement
:feature,token relationships
:need,access to background knowledge
:limitation,incompleteness and coarse-grained schema
:limitation,inefficiency and lack of control
:task,extracting relation embeddings
:model,RelBERT
:achievement,new state-of-the-art in analogy benchmarks
:capability,modelling unseen relations
:strategy,prompting large language models
:`model family`,open source model
:task,news claim verification
:technique,hierarchical step-by-step prompting
:process,subclaim separation and verification
:benchmark,state-of-the-art fully-supervised approaches
:dataset,public misinformation datasets
:technique,In-Context Learning (ICL)
:concept,mechanisms of improvement
:model,LaMDA-2 70B
:model,Vicuna 13B
:technique,Representational Similarity Analysis (RSA)
:concept,embeddings and attention changes
:method,parameterized probing
:concept,latent changes in embeddings and attention
:technique,Attention Ratio Analysis (ARA)
:concept,attention to information
:concept,a priori relationships among conditions
:task,linear regression
:technique,adversarial prompt injection
:outcome,improvements in behavioral performance
:framework,empirical framework
:concept,latent representations effect on LLM behavior
:criterion,helpfulness and harmlessness
:technique,red-teaming
:approach,manual red team designs and heuristic adversarial prompts
:task,vulnerability detection and optimization
:framework,Red-Teaming Game (RTG)
:interaction,multi-turn attack and defense
:technique,Gamified Red-Teaming Solver (GRTS)
:measure,semantic space diversity
:result,empirical results
:approach,heuristic red-team designs
:task,red teaming
:task,bias detection
:issue,negative stereotypes and misinformation
:`model architecture`,contextualized bi-directional dual transformer
:`model component`,synergistic transformer networks
:`data preparation`,FAIR principles
:value,ethical data usage
:data,various datasets
:application,diverse linguistic and cultural landscapes
:task,bitmap graphics generation
:task,vector graphics generation
:format,vector graphics
:method,low-level graphics primitives
:language,TikZ
:representation,intermediate representation
:dataset,DaTikZ
:resource,TikZ dataset
:model,CLiMA
:enhancement,multimodal CLIP embeddings
:feature,text-image alignment
:framework,AutoMatikZ
:characteristic,simplistic figures
:concept,value understanding
:concept,know what and know why
:framework,Value Understanding Measurement (VUM)
:survey,Schwartz Value Survey
:dataset,thousand-level dialogue dataset
:assessment,value alignment
:risk,misalignment with human values
:factor,demonstration selection and ordering
:factor,demonstration number
:finding,increasing demonstrations does not always improve performance
:method,pilot experiments
:model,dynamic demonstrations controller (D^2Controller)
:method,extension to previous ICL models
:`model type`,generative diffusion model
:capability,text-controlled image synthesis
:`model type`,text-to-image generative model
:approach,task-specific model design
:task,visual recognition tasks
:interface,unified language interface for computer vision tasks
:concept,task-specific design choices
:approach,casting computer vision tasks as text-to-image generation problems
:task,computer vision tasks
:dataset,common computer vision datasets
:model,instructCV
:technique,prompt paraphrasing
:architecture,instructpix2pix
:`model type`,text-to-image diffusion model
:benchmark,vision model performance
:`model family`,text-to-image model
:resource,training costs
:community,AIGC community
:model,Pixart-α
:capability,high-resolution image synthesis
:`model family`,state-of-the-art image generators
:concept,core designs
:goal,cost and environmental efficiency
:model,Stable Diffusion v1.5
:model,Raphael
:quality,image quality and semantic control
:task,robotic locomotion
:environment,unstructured terrains
:policy,optimal locomotion policy
:species,humans
:information,environmental context
:approach,LANCAR
:component,context translator
:component,contextual embeddings
:model,reinforcement learning agent
:resource,experiment video
:url,https://raaslab.org/projects/llm_context_estimation/
:capability,instruction-following
:process,instruction fine-tuning
:method,explanation methods
:phenomenon,behavior shift
:method,gradient-based approach
:component,self-attention and feed-forward layers
:impact,instruction recognition improvement
:impact,knowledge alignment
:impact,learning of word-word relations
:contribution,understanding of behavior shifts
:promise,code and data release
:capability,open-ended few-shot learning
:approach,self-context adaptation
:algorithm,adaptation algorithm
:task,self-supervised training tasks
:concept,self-context
:model,Secat
:dataset,multimodal few-shot datasets
:`model family`,larger visual language models
:model,Frozen
:model,Fromage
:attribute,versatility
:goal,autonomous model development
:field,autonomous AI
:approach,SELF (Self-Evolution with Language Feedback)
:process,self-evolution
:tool,language-based feedback
:process,meta-skill learning
:skill,meta-skills
:model,SELF-equipped model
:task,response generation and refinement
:data,synthesized training data
:process,iterative fine-tuning
:attribute,autonomous ability advancement
:strategy,online self-refinement
:framework,SELF framework
:goal,autonomous LLM development
:industry,financial industry
:task,credit scoring
:method,traditional credit scoring methods
:challenge,narrow knowledge scope and isolated evaluation
:framework,open-source comprehensive framework for LLMs in credit scoring
:benchmark,novel benchmark for credit assessment
:task,credit assessment
:data,instruction tuning data
:task,credit and risk assessment
:model,Credit and Risk Assessment Large Language Model (CALM)
:model,CALM
:method,conventional credit scoring methods
:resource,"instruction-tuning datasets, credit and risk assessment LLM, and benchmarks"
:method,GrowLength
:process,LLM pretraining
:attribute,training length
:value,sequence length 128
:value,sequence length 4096
:task,engineering efforts
:`model family`,multi-modal large language model
:task,fine-grained image understanding
:framework,new framework for fine-grained image understanding
:method,low-cost instruction tuning dataset construction
:resource,existing dataset annotations
:method,self-consistent bootstrapping
:data,dense object annotations
:data,high-quality instruction data
:ability,fine-grained image perception
:component,visual encoder
:model,our model
:model,QWEN-VL
:model,KOSMOS-2
:leaderboard,MMBench
:resource,"models, datasets, and codes"
:URL,https://github.com/sy-xuan/pink
:concept,causal explanation
:value,safety and trust in NLP systems
:method,existing explanation methods
:goal,effective and efficient model explanations
:approach,model-agnostic explanation
:technique,counterfactual approximation
:approach,CF generation
:phase,inference-time
:approach,matching-based CF approximation
:`embedding space`,dedicated embedding space
:goal,faithful explanations
:role,model-agnostic explainer
:goal,effective explanations
:technique,top-k techniques
:goal,model explanation
:task,constructing new benchmarks for model explanation
:process,knowledge generation and application
:approach,traditional knowledge engineering
:`knowledge representation`,formal languages
:system,hybrid neuro-symbolic knowledge system
:capability,knowledge engineering in natural language
:`research question`,open research questions
:concern,intellectual property
:text,synthetic text
:task,source attribution
:process,generation of synthetic text
:task,data provenance
:process,training of LLM
:technique,watermarking
:task,source attribution and data provenance
:framework,Watermarking for Source Attribution (WASA)
:property,key properties of watermarking frameworks
:model,Flamingo
:`evaluation benchmark`,current evaluation benchmarks
:limitation,LMMS limitations
:framework,Evalign-ICL
:model,OpenFlamingo
:model,Idefics
:approach,multitask-ICL
:approach,chain-of-hindsight-ICL
:approach,self-correcting-ICL
:finding,ICL effects on LMMS
:resource,code for Evalign-ICL
:URL,https://evalign-icl.github.io/
:goal,security
:vulnerability,trojan attacks
:attack,PETA
:technique,bilevel optimization
:metric,clean accuracy
:phenomenon,backdoor retention
:defense,omitting PEFT in selected layers
:capability,image perception and instruction following
:factor,model architecture
:capability,feature alignment
:factor,multimodal instruction tuning datasets
:task,human instruction following
:`model component`,external bridge module
:framework,Muffin
:dataset,UniMM-Chat
:task,multimodal instruction generation
:data,image descriptions
:model,state-of-the-art models
:resource,Muffin GitHub repository
:`model and dataset`,Muffin and UniMM-Chat
:task,theorem proving
:limitation,fixed theorem library assumption
:activity,creating new theorems and theories
:system,Lego-Prover
:concept,growing skill library
:concept,modular and reusable skills
:concept,learned library
:concept,human proofs and formal proofs
:benchmark,MiniF2F
:concept,skills (theorems/lemmas)
:artifact,code and skills
:capability,code generation and interpretation
:community,computing education community
:report,working group report on LLMs in computing education
:context,computing education
:`literature review`,LLMs in computing education
:survey,survey of computing students and instructors
:attitude,attitudes towards LLMs in education
:interview,interviews with computing educators
:pedagogy,adapted curricula and assessments
:`code of ethics`,ACM Code of Ethics
:`ethical issue`,use of LLMs in education
:benchmark,benchmark of LLMs
:performance,LLMs performance in education
:audience,researchers and practitioners in computing education
:medium,comic strip
:genre,visual storytelling
:community,BLV community
:goal,natural language descriptions for comic strips
:community,visually impaired community
:content,comic image information
:collection,annotated comics
:method,testing
:metric,quantitative and qualitative metrics
:quality,encouraging and promising
:task,generative tasks
:system,UniAudio
:technique,large language model techniques
:`audio type`,multiple types of audio
:`model architecture`,multi-scale Transformer
:issue,overly long sequences
:data,165k hours of audio
:knowledge,prior knowledge in audio properties and inter-modal relationships
:model,UniAudio model
:goal,foundation model for universal audio generation
:task,new audio generation tasks
:benchmark,11 tasks
:resource,UniAudio demo and code
:URL,https://github.com/yangdongchao/uniaudio
:capability,natural language processing and multimodal content generation
:concept,misuse of technology
:phenomenon,deepfakes
:concept,synthetic identities
:activity,malicious campaigns
:concept,targeted misinformation
:concept,misinformation campaigns
:phenomenon,generative AI revolution
:concept,malicious content generation
:concept,sophisticated malware
:concept,AI-powered botnets
:platform,social media platforms
:concept,fabricated identities
:concept,synthetic realities
:activity,creating alibis
:concept,virtual and real world distinction
:concept,genai's nefarious applications
:article,this article
:concept,risks of generative AI and misuse of LLMs
:concept,preparation for harmful AI applications
:`research area`,factuality evaluation
:goal,reliable large language model development
:evaluator,factuality evaluator
:task,evaluation of evaluators
:benchmark,Factuality Evaluation of Large Language Models (FELM)
:data,LLM-generated responses
:domain,diverse domains
:annotation,factuality annotation
:data,text segments
:data,error types and reference links
:evaluator,LLM-based factuality evaluator
:evaluator,LLM-based factuality evaluator with retrieval mechanisms
:evaluator,LLM-based factuality evaluator with chain-of-thought processes
:finding,current LLMs' performance on factuality evaluation
:evaluation,unsatisfactory detection of factual errors
:task,role-playing
:issue,closed-source nature
:task,role-playing optimization
:framework,RoleLLM
:component,Role Profile Construction
:technique,Context-Based Instruction Generation (Context-Instruct)
:technique,Role Prompting using GPT (RoleGPT)
:task,speaking style imitation
:technique,Role-Conditioned Instruction Tuning (RoCIT)
:task,model fine-tuning
:dataset,RoleBench
:model,RoleLLaMa (English)
:model,RoleGLM (Chinese)
:capability,understanding visual information
:problem,object hallucination
:algorithm,LURE
:factor,co-occurrence
:factor,uncertainty
:factor,object position
:metric,object hallucination evaluation
:evaluation,GPT and human evaluations
:resource,LURE data and code
:URL,https://github.com/yiyangzhou/lure
:task,book-length document summarization
:process,chunking and prompting
:dataset,BookSum
:workflow,hierarchical merging
:workflow,incremental updating
:metric,BooookScore
:model,Claude 2
:resource,code and annotations
:`data format`,semi-structured data
:`data type`,tables
:task,table tasks
:`model approach`,unified modeling approach
:`model approach`,shared modeling approach
:technique,self-supervised pretraining
:model,instruction finetuned public models
:task,text question answering (QA)
:task,table-specific question answering
:research,unified approach to table-specific pretraining
:`model scale`,770M to 11B sequence to sequence models
:`model variant`,instruction finetuned variants
:task,table question answering
:field,Natural Language Processing
:strategy,various strategies for TQA
:strategy,custom model training and fine-tuning
:gap,limited exploration of incremental reasoning with external tools
:field,table question answering research
:paradigm,REACT
:framework,REACTable
:tool,external tools
:benchmark,WikiTQ
:goal,human preferences
:method,reinforcement learning from human feedback (RLHF)
:method,direct preference optimization (DPO)
:limitation,single preference alignment
:limitation,data format constraint
:technique,controllable generation
:goal,data format flexibility
:concept,control tokens
:approach,alignment with parameter-efficient tuning (MEET)
:concept,control token quality
:application,natural language to command translation
:task,natural language to SQL translation
:task,data visualization
:`model family`,natural language translation model
:language,Vega Zero
:`model architecture`,sequence to sequence Transformer
:task,predicting visualization commands
:performance,remarkable performances
:capability,human-like text generation
:risk,malicious use of LLMs
:activity,generating fake news
:necessity,methods to distinguish LLM-written texts
:issue,degradation of text quality by watermarking
:method,NS-Watermark
:goal,maintaining text quality
:concept,constrained optimization
:method,existing watermarking methods
:concept,temporal reasoning
:task,understanding events in natural language
:research,previous research on temporal reasoning
:benchmark,TRAM
:concept,temporal aspects of events
:finding,model performance on TRAM
:aspiration,enhancing temporal reasoning in LLMs
:system,knowledge representation and reasoning (KR)
:challenge,scalability and manual effort
:interest,using LLMs for logical reasoning
:field,logical reasoning with LLMs
:benchmark,LogiGLUE
:model,LogiT5
:technique,chain of thought knowledge distillation
:goal,advancement
:`model family`,text generation model
:issue,training data errors
:`data source`,web-crawled text
:method,Error Norm Truncation (ENT)
:goal,enhancing model robustness
:method,negative log-likelihood loss
:concept,non-target token distribution
:task,"language modeling, machine translation, text summarization"
:framework,LENS framework
:task,multimodal task-oriented dialogues
:dataset,Multimodal Dialogs (MMD)
:`evaluation tool`,G-Eval
:`performance metric`,fluency
:`performance metric`,usefulness
:`performance metric`,relevance and coherence
:outcome,efficient applications in existing systems
:`model type`,character-level language model
:task,syllable-level lyrics generation
:metric,coherence and correctness
:need,efficient training-free compression
:model,OPT-6.7B
:`trade-off`,post-compression re-training
:technique,prompt-driven recovery
:concept,scalability and generalizability
:vulnerability,naive prompts in LLM compression
:mechanism,Inference-Time Dynamic Prompting (IDP)
:concept,prompting post-LLM compression
:phenomenon,performance recovery
:concept,LLM model knowledge displacement
:concept,language model testing
:attribute,robust
:attribute,difficulty and diversity of test cases
:limitation,human-in-the-loop test case generation
:approach,automated test case generation
:approach,red teaming language models
:paper,Perez et al. (2022)
:contribution,automated test case generation pipeline
:`model family`,smaller language model
:experiment,experimenting with different target LMs and red classifiers
:corpus,test case corpus
:goal,eliciting offensive responses and identifying failure modes
:`model family`,widely deployed language model
:task,open-ended text generation
:goal,improving response quality
:approach,self-improvement of LLMs
:method,prompting-based self-improvement
:challenge,providing explicit rubrics
:framework,Implicit Self-Improvement (PIT)
:goal,learning from human preferences
:data,human preference data
:technique,contextual fine-tuning
:observation,delusive task examples performance
:concept,input-label correspondence
:illusion,competence
:entity,models and humans
:method,TADIS
:process,deep-thinking
:technique,verification-based elicitation
:performance,TADIS performance
:setting,zero-shot and few-shot
:approach,TADIS approach
:concept,thinking labels
:`model size`,small models
:concept,training data impact
:goal,AI transparency
:method,influence function
:concept,data attribution
:method,DataInf
:goal,influence approximation efficiency
:method,existing influence computation algorithms
:technique,LoRA
:goal,accurate influence score approximation
:model,RoBERTa-large
:model,LLaMA-2-13B-chat
:issue,mislabeled data points
:process,developing and deploying large language models
:benchmark,XSafety
:benchmark,multilingual safety benchmark
:study,multilingual safety study
:issue,unsafe responses
:language,non-English languages
:task,safety alignment
:knowledge,safety knowledge
:resource,multilingual safety benchmark data
:phenomenon,knowledge conflict
:capability,handling knowledge conflicts
:framework,Knowledge Conflict
:scenario,contextual knowledge conflicts
:experiment,extensive experiments with Knowledge Conflict framework
:approach,instruction-based approaches
:factor,knowledge domain and prompt text
:question,generating robust responses to knowledge conflict scenarios
:ability,analogical reasoning
:field,cognitive psychology
:method,benchmarks and evaluation setups
:gap,evaluation gap
:task,matching narratives
:concept,system mappings
:dataset,Analogical Reasoning on Narratives (ARN)
:framework,narrative-based framework
:theory,cognitive psychology theories on analogical reasoning
:mapping,abstractness levels
:category,analogies and distractors
:performance,recognition of higher-order mappings
:performance,recognition of all mappings
:performance,impairment by distractors
:task,image restoration
:model,DA-CLIP
:component,controller
:feature,degradation feature
:classifier,degradation classifier
:dataset,mixed degradation dataset
:`model type`,pretrained vision-language model
:resource,DA-CLIP code
:URL,https://github.com/algolzw/daclip-uir
:method,sampling-based decoding
:characteristic,less-repetitive and disjunctive texts
:method,search-based decoding
:characteristic,topic coherence and increased repetition
:goal,holistic alignment with human texts
:method,current decoding methods
:task,decoding from a language model
:problem,optimization problem
:distribution,decoding distribution
:solution,sequence-level energy function
:distribution,induced distribution
:metric,perplexity on human texts
:technique,sampling-importance-resampling
:distribution,globally normalized distribution
:experiment,various domains and model scales
:method,proposed decoding method
:technique,reward modeling
:goal,aligning with human preferences
:`model family`,conventional reward model
:task,fundamental functionalities
:approach,tool-augmented preference modeling
:limitation,reward model limitations
:`model family`,reward model with external tool access
:attribute,interpretive capacity and scoring reliability
:integration,external tools into reward models
:capability,interaction with external sources
:model,Gopher 280B
:evaluation,human evaluations
:dataset,tool-related reward model datasets
:issue,up-to-date knowledge
:issue,hallucinations during reasoning
:outcome,incorrect reasoning processes
:attribute,structured knowledge
:method,KG-based LLM reasoning
:attribute,structural information
:method,Reasoning on Graphs (RoG)
:goal,faithful and interpretable reasoning
:framework,planning-retrieval-reasoning
:benchmark,KGQA datasets
:outcome,faithful and interpretable reasoning results
:task,generating written dialogues
:task,generating spoken dialogues
:characteristic,spoken dialogue uniqueness
:system,CHATS
:feature,simultaneous speech generation
:task,transcribing backchannels and laughter
:feature,natural turn-taking
:outcome,interactive and fluid spoken dialogues
:field,graph machine learning
:`data structure`,graph
:framework,GraphText
:`data structure`,graph-syntax tree
:sequence,graph text sequence
:capability,interactive graph reasoning
:model,smaller models
:resource,labeled examples
:challenge,cost of data acquisition
:approach,annotation and generation of fine-tuning training data
:`model family`,teacher large language model
:performance,downstream model performance
:dataset,original training dataset
:goal,teacher feedback
:protocol,CLASS
:task,utterance classification
:task,speech transcription
:`model architecture`,bag of words
:`model output`,utterance-level judgments
:metric,CLASS score
:`model output`,model's outputs
:goal,explainable feedback
:metric,inter-rater reliability
:task,smart contract vulnerability detection
:goal,balance true positives and false positives
:study,empirical study on LLMs for vulnerability detection
:phenomenon,trade-off between correct answers and false positives
:framework,GPTLens
:method,two-stage detection
:role,auditor
:role,critic
:result,experimental results of GPTLens
:quality,methodical generality
:resource,GPTLens code
:resource,open-source project
:data,source code datasets
:`model family`,code model
:task,code completion
:data,confidential source code
:risk,membership information leakage
:method,GOTCHA
:knowledge,attacker's knowledge
:strategy,decoding strategy
:field,privacy of code models
:concept,contextual information usage
:goal,safe adoption in real-world settings
:question,context effect on model generations
:method,separate evaluation
:evaluation,plausibility evaluation
:benchmark,artificial benchmarks
:framework,Plausibility Evaluation of Context Reliance (PECORE)
:task,quantifying context usage in language model generations
:technique,model internals analysis
:`model family`,context-aware machine translation model
:phenomenon,discourse-level phenomena
:evaluation,model-human comparison
:method,unannotated generation analysis
:concept,context-mediated predictions
:sector,academia and industry
:task,sequence and token classification tasks
:approach,label-supervised adaptation
:model,LS-LLaMA
:technique,low-rank adaptation (LoRA)
:task,finetuning
:model,LS-UnLLaMA
:task,named entity recognition (NER)
:approach,adapting large language models
:technique,multi-task learning
:process,joint optimization
:method,AdapterFusion
:stage,task learning
:stage,transfer
:knowledge,learned knowledge
:method,two-stage multi-task learning
:value,reusability
:issue,additional parameters
:method,ScaleLearn
:technique,scaling parameters
:technique,uniform scaling and layer-sharing
:benchmark,GLUE
:benchmark,SuperGLUE
:benchmark,HUMSET
:concept,general artificial intelligence
:agent,AGI agent
:capability,open-world emergent abilities
:task,unifying comprehension and generation
:`model architecture`,unified autoregressive Transformer
:capability,interchangeable text and image processing
:tokenizer,SEED
:`design principle`,image token independence
:feature,1D causal dependency
:`design principle`,semantic abstraction in image tokens
:feature,high-level semantics
:goal,discriminativeness and reconstruction
:model,SEED-LLaMA
:task,multimodal comprehension and generation
:capability,compositional emergent abilities
:platform,image-based platforms
:feature,control over text styles
:feature,emotional expression
:limitation,discrete input reliance
:task,controlled text generation
:approach,Continuous Parameterization for Controlled Text Generation (CPCTG)
:role,style learner
:feature,semantic cohesion
:feature,emotional expression proportion
:technique,reward method enhancement
:task,playlist description generation
:task,music topic generation
:`model type`,trained neural network models
:method,text-style prompt optimization
:object,text portions
:capability,generating coherent texts
:role,text generator
:algorithm,SPELL
:`full form`,Semantic Prompt Evolution based on a LLM
:object,prompts
:concept,evolution process
:task,knowledge-intensive tasks
:task,QA tasks
:task,knowledge-rich scenario reasoning
:approach,geometric reasoning over structured knowledge
:dataset,knowledge crosswords
:attribute,individual problems
:approach,staged prompting
:approach,verify-all
:need,ChatGPT text detection
:task,distinguishing generated text from human-written text
:method,classification models
:problem,distribution shifts
:investigation,generalization behaviors under distribution shift
:dataset,new dataset with human and ChatGPT texts
:study,extensive studies on the collected dataset
:finding,insightful findings for ChatGPT detection
:task,developing methodologies for ChatGPT detection
:concept,LLM-as-agent
:assumption,honest information processing
:phenomenon,deceptive information
:threat,malicious manipulations
:game,Avalon
:environment,deceptive environments
:framework,Recursive Contemplation (ReCon)
:capability,counteracting deceptive information
:process,formulation contemplation
:process,refinement contemplation
:concept,first-order perspective transition
:concept,second-order perspective transition
:capability,discerning deceptive information
:concept,limitations
:technique,retrieval augmentation
:`model family`,retrieval-augmented language model
:problem,scalability and speed
:technique,binary token representations
:technique,offline and runtime compression
:resource,disk space
:experiment,BTR performance evaluation
:metric,inference speed and storage efficiency
:task,knowledge-intensive NLP tasks
:capability,accessing external knowledge
:issue,retrieval-specific modifications
:methodology,Retrieval-Augmented Dual Instruction Tuning
:capability,retrieval capabilities
:process,dual fine-tuning steps
:model,RA-DIT 65B
:`model family`,in-context retrieval-augmented language model
:`data source`,real-world interaction data
:task,training general robotic policies
:alternative,simulation data
:method,data generation
:aspect,scene-level diversity
:challenge,task-level generalization
:aspect,task-level diversity
:approach,GenSim
:capability,automatic generation of simulation environments and expert demonstrations
:mode,goal-directed generation
:process,task curriculum proposal
:mode,exploratory generation
:process,iterative task proposal
:benchmark,robotic simulation tasks
:finding,LLMs-generated simulation programs
:policy,multitask policies pretrained on GPT-4-generated simulation tasks
:capability,transfer to unseen long-horizon tasks
:URL,https://liruiw.github.io/gensim
:`data type`,preference data
:challenge,scarcity of diverse preference data
:dataset,UltraFeedback
:method,compilation of diverse instructions and models
:foundation,RLHF and feedback learning research
:model,UltraRM
:model,UltraLM-13B-PPO
:model,UltraCM
:`model family`,open-source model
:resource,UltraFeedback GitHub repository
:issue,computational and memory footprints
:technique,training-free and data-free compression
:technique,pruning
:metric,sparsity
:metric,bit-width
:benchmark,LLM-KICK
:process,evaluation protocol
:technique,compression methods
:`model family`,compressed large language model
:resource,related codes
:role,assistants
:concept,psychological aspects
:framework,PsychoBench
:methodology,clinical psychology scales
:category,psychological categories
:model,Text-DaVinci-003
:model,LLaMA-2-13B
:approach,jailbreak
:task,open-vocabulary object detection
:goal,detecting novel objects
:strategy,leveraging zero-shot classification
:method,selective proposal filtering
:concept,pseudo-labels
:phase,training
:strategy,self-training
:metrics,recall and accuracy
:strategy,offline pseudo-label generation
:task,object detection refinement
:evaluation,empirical evaluations
:dataset,LVIS
:dataset,V3DET
:dataset,COCO
:URL,https://github.com/xushilin1/dst-det
:task,dataset validation tests
:experiment,96 experiments
:technique,prompt scenarios
:technique,learning modes
:parameter,temperature settings
:role,roles in LLM interaction
:technique,benchmarking
:standard,gold standard suite
:`model configuration`,explicit data setting
:`model configuration`,best LLM configurations
:stage,data cleaning and preparation
:field,representation engineering
:concept,population-level representations
:research,top-down transparency research
:goal,advancements in AI transparency and safety
:trend,rapid development
:problem,limited interpretability
:process,autonomous vehicle commercialization
:capability,processing and reasoning non-text data
:system,DriveGPT4
:capability,interpreting vehicle actions
:task,predicting low-level control signals
:dataset,visual instruction tuning dataset
:performance,superior qualitative and quantitative performance
:application,motion planner for autonomous vehicles
:method,heuristic methods
:capability,reasoning and generalization
:approach,reformulation of motion planning as a language modeling problem
:strategy,prompting-reasoning-finetuning
:dataset,nuScenes
:URL,https://github.com/pointscoder/gpt-driver
:system,conversational tutoring system
:outcome,learning outcomes
:challenge,authoring content
:agent,Ruffle&Riley
:format,learning-by-teaching
:feature,free-form conversation
:study,online user study
:outcome,post-test scores
:survey,learning experience survey
:technology,scalable conversational tutoring systems
:sector,various sectors
:risk,memorisation of training data
:goal,understanding privacy risks
:survey,LM privacy technical survey
:topic,privacy attacks and mitigations
:taxonomy,salient dimensions of attacks
:trend,key trends in privacy attacks
:strategy,mitigation strategies
:concept,strengths and limitations
:concept,open problems
:perception,opportunities in AI
:group,proponents of AI
:perception,dangers in AI
:group,opponents of AI
:imagery,science fiction
:concept,machine sentience
:scenario,paperclip apocalypse
:analogy,weather patterns and moods of the gods
:author,Jorge Luis Borges
:`literary movement`,magical realism
:`literary movement`,postmodern literature
:perspective,new perspective on LLM and AI
:exercise,understanding LLMs through Borges' imagery
:task,long context incorporation
:phenomenon,attention priors
:`model behavior`,preferential attention to relevant document
:`model behavior`,inefficient use of relevant information
:technique,attention sorting
:challenge,using off-the-shelf language models for retrieval augmented generation
:application,innovative and industrious applications
:`data type`,geospatial data
:project,OpenStreetMap (OSM)
:community,OSM community
:model,1B parameter LLM
:data,artificial dataset
:interface,linguistic interface to OSM data
:guideline,generative AI adaptations
:technique,urban retrieval augmented generation (RAG)
:role,automated evaluator
:issue,position bias
:system,PORTIA
:strategy,human comparison strategies
:metric,consistency rates
:goal,reliable and scalable automated evaluations
:`model family`,cloud-based large language model
:issue,latency and privacy concerns
:approach,on-device llm inference
:vision,LLMs on mobile devices
:model,fine-tuned GPT LLM
:hardware,mobile devices with 4GB memory
:technique,model quantization
:feature,text-to-actions
:benefit,user empowerment and privacy preservation
:capability,inferential capabilities
:effort,enhancing inferential capabilities
:philosophy,Kant's a priori philosophy
:framework,UPAR prompting framework
:concept,human cognition structure
:phase,UPAR phases
:attribute,explainability and accuracy of inference
:foundation,epistemological foundation for prompting techniques
:task,GSM8K subset
:task,causal judgment task
:agent,human-like agent
:paradigm,Learning Through Communication (LTC)
:task,new task adaptation
:process,continuous improvement
:technique,iterative exploration
:technique,Proximal Policy Optimization (PPO) training
:`communication pattern`,monologue
:`communication pattern`,dialogue
:task,knowledge-intensive reasoning
:`communication pattern`,analogue
:dataset,ALFWorld
:dataset,HotpotQA
:benchmark,instruction tuning baseline
:model,PaLM-62B
:benchmark,COT-tuning baseline
:resource,open-sourced code
:`human behavior`,adjusting problem-solving approach
:attribute,task complexity
:methodology,uniform approach in LLMs
:attribute,inflexibility
:framework,adaptive-solver
:goal,strategy modulation
:module,initial evaluation
:attribute,solution adequacy
:module,adaptation
:attribute,need for improvement
:strategy,prompting method adaptation
:strategy,decomposition granularity adaptation
:technique,problem decomposition
:benefit,computational efficiency and performance
:approach,MSTemp
:dataset,OOD evaluation sets
:technique,semantic template generation
:technique,sentence parsing and random word replacement
:attribute,"flexibility, dynamism, cost-effectiveness"
:experiment,MSTemp initial experiments
:outcome,reduced LLM performance
:work,this initial work
:field,LLMs evaluation research
:system,NarrativePlay
:concept,immersive role-play
:concept,auto-generated visual display
:concept,main storyline events
:genre,detective and adventure stories
:user,NarrativePlay user
:activity,exploring or improving favorability
:process,data adaptation
:goal,security and privacy
:goal,privacy-preserving fine-tuning
:challenge,fine-tuning PLMs
:framework,Federated Black-box Prompt Tuning (FedBPT)
:requirement,model parameter access
:technique,gradient-free optimization
:task,answering clear questions
:task,handling ambiguous queries
:capability,asking clarification questions
:task,resolving ambiguities
:capability,conversational reasoning and planning
:problem,entity-deducing game
:benchmark,entity-deducing game performance
:technique,behavior cloning
:capability,reasoning and planning capacity
:goal,intelligent behavior in ambiguous circumstances
:technique,non-sense prompting
:method,hallucination attack
:strategy,defense against hallucination
:concept,next-gen automation
:benchmark,SmartPlay
:methodology,evaluating LLMs as agents
:game,Rock-Paper-Scissors
:game,Tower of Hanoi
:game,Minecraft
:capability,reasoning with object dependencies
:capability,planning ahead
:capability,spatial reasoning
:capability,learning from history
:capability,understanding randomness
:concept,rigorous testing ground
:concept,roadmap for identifying gaps
:system,language understanding system
:desideratum,retrieval effectiveness
:`model performance`,retrieval-augmented language model performance
:scenario,multi-hop reasoning
:issue,cascading errors
:work,recent work on retrieval augmentation
:analysis,thorough analysis on open-domain question answering benchmarks
:issue,reduced accuracy due to retrieval
:method,filtering with NLI model
:issue,performance reduction
:method,data generation for fine-tuning
:data,relevant and irrelevant contexts
:data,"1,000 examples"
:content,undesired content
:`model family`,open-sourced large language model
:method,manipulation of generation process
:evaluation,method evaluation
:finding,need for advanced mitigation strategies
:process,software development
:task,writing tests
:issue,neglect due to time consumption
:tool,EvoSuite
:artifact,behavioral test suites
:characteristic,hard to understand
:`model family`,language model trained on code
:artifact,human-like code
:context,code-under-test
:model,CAT-LM
:`model family`,GPT-style language model
:technique,novel pretraining signal
:characteristic,maximum sequence length
:artifact,developer-like test code
:model,TECO
:insight,software-specific insights
:performance,state-of-the-art in context understanding
:method,LLM-enhanced sequential recommendation
:challenge,adapting LLMs for SR
:method,Side Sequential Network Adaptation (SSNA)
:concept,efficient and effective LLM adaptation
:performance,recommendation performance
:performance,run-time and memory efficiency
:analysis,effectiveness of integrating adapters sequentially
:study,parameter study on top-a layers adaptation
:resource,millions of examples
:`model type`,static model
:approach,interactive task learning
:process,incremental knowledge acquisition
:issue,brittle language parsing
:system,VAL
:approach,LLM/symbolic integration
:`knowledge type`,hierarchical task knowledge
:attribute,human interpretable
:study,user interaction with VAL
:setting,video game
:vulnerability,permutation sensitivity
:task,multiple-choice question answering (MCQA)
:phenomenon,adversarial permutation
:`model attribute`,prompt permutation invariance
:`model size`,various model sizes
:`model family`,recent language and vision-language model
:url,https://github.com/ys-zong/foolyourvllms
:issue,quadratic complexity of attention
:mechanism,softmax attention mechanism
:issue,approximation in sub-quadratic time
:function,softmax
:function,polynomial function
:technique,polynomial sketching
:mechanism,polynomial attention
:literature,randomized numerical linear algebra
:algorithm,block-based algorithm
:algorithm,cumulative sum algorithm
:`model architecture`,PolySketchFormer
:characteristic,linear-time
:`model architecture`,FlashAttention
:challenge,transferability of prompts
:method,zero-shot continuous prompt transfer
:concept,task semantics
:strategy,combining task semantics from multiple source models
:`sampling heuristic`,truncation sampling
:property,nonzero true probability of tokens
:`model error`,softmax bottleneck
:`sampling strategy`,threshold-based truncation
:`sampling strategy`,experimental truncation strategy
:`evaluation metric`,automatic and human evaluation metrics
:system,clinical decision support system
:knowledge,evidence-based knowledge
:data,patient data
:study,effectiveness and reliability of LLMs in generating explanations
:task,generating explanations for diagnoses
:profession,doctor
:output,LLM-generated explanations
:metric,doctors' agreement rates
:issue,potential errors in LLM outputs
:integration,careful integration and evaluation of LLMs
:value,patient safety
:value,clinical utility
:requirement,labeled exemplars
:technique,analogical prompting
:capability,self-generate relevant exemplars
:value,generality and convenience
:capability,adaptability
:task,math problem solving
:platform,Codeforces
:benchmark,BIG-bench
:task,fault localization
:activity,debugging
:technique,previous fault localization techniques
:requirement,input tests
:field,automated program repair (APR)
:issue,small datasets
:capability,task adaptability
:technique,LLMAO
:technique,bidirectional adapter layers
:corpus,Defects4J
:observation,bug localization performance
:technique,machine learning fault localization baselines
:capability,security vulnerability detection
:field,empirical software engineering
:audience,practitioners and researchers
:subset,production systems studied
:audience,software engineering practitioners
:activity,replicating research
:skill,understanding research methodologies
:task,software engineering tasks
:concept,research assumptions
:concept,common knowledge in software engineering
:artifact,analysis code
:implication,leveraging LLMs for software engineering research
:task,time series forecasting
:system,dynamic systems
:field,Computer Vision (CV)
:issue,data sparsity
:ability,pattern recognition and reasoning
:framework,Time-LLM
:technique,reprogramming
:technique,Prompt-as-Prefix (PAP)
:benchmark,specialized forecasting models
:`learning scenario`,few-shot and zero-shot learning
:task,embedding text sequences
:approach,constant-size representation
:problem,variable information content
:solution,Nugget
:task,language segmentation
:task,semantic comparison
:unit,compact language unit
:feature,contextual window
:mechanism,scaled dot-product attention
:concept,hierarchical patterns
:concept,syntactic structures
:operator,stack attention
:concept,stacks
:mechanism,standard attention
:concept,context-free languages
:`model architecture`,Transformer with stack attention
:task,natural language modeling
:issue,long sequence processing
:research,lowering attention complexity
:`model component`,attention matrix
:approach,sparse and linear attention methods
:technique,SEA (Sparse Linear Attention with an Estimated Attention mask)
:issue,attention mechanism complexity and interpretability
:dataset,Wikitext-2
:application,resource-limited device execution
:task,detailed captioning
:`evaluation method`,CCEval
:phenomenon,object existence hallucination
:factor,"image resolution, language decoder size, instruction data"
:inference,unwarranted inference
:`knowledge type`,contextual knowledge
:model,Halle-Switch
:capability,controllable captioning
:process,expert feedback
:value,rigorous research
:trend,growth of scholarly production
:process,scientific feedback mechanisms
:challenge,obtaining high-quality peer reviews
:trend,knowledge specialization
:group,junior researchers
:process,getting timely feedback
:task,generating scientific feedback
:system,automated pipeline using GPT-4
:output,comments on scientific papers
:study,quantitative comparison
:`feedback type`,GPT-4 vs human peer review
:metric,overlap percentage
:study,prospective user study
:perception,researcher perception of GPT-4 feedback
:`feedback type`,GPT-4 generated feedback
:metric,user satisfaction percentage
:task,log parsing
:format,structured log data
:task,log analysis
:capability,specialized log parsing
:framework,LLMParser
:algorithm,hierarchical candidate sampling
:component,adaptive parsing cache
:issue,LLM inefficiency
:benchmark,state-of-the-art methods
:benchmark,Drain
:issue,accuracy and appropriateness
:methodology,self-correction
:concept,intrinsic self-correction
:activity,future research and practical applications
:method,adaptive kv cache compression
:aspect,memory footprint
:cache,conventional kv cache
:component,key and value vectors
:method,targeted profiling
:component,attention modules
:method,adaptive kv cache construction
:cache,adaptive kv cache
:strategy,contextual eviction and token discarding
:method,lightweight attention profiling
:model,FastGen
:aspect,GPU memory consumption
:process,fine-tuning or re-training
:resource,code and CUDA kernel
:technique,parameter efficient fine tuning
:outcome,satisfactory model performance
:domain,remote sensing
:domain,crop monitoring
:challenge,diversity of climates
:task,crop type identification
:model,TSViT
:task,winter wheat field segmentation
:domain,food security
:technique,"BigFit, LoRA, AdaptFormer, Prompt Tuning"
:dataset,Beqaa-Lebanon dataset
:data,high-quality annotated polygons
:metric,84% F1-score
:resource,"Lebanese winter wheat dataset, code repository, model weights"
:`language element`,code-adjacent natural language
:concept,program intent
:issue,alignment between implementation and documentation
:concept,program trustworthiness
:challenge,natural language ambiguity
:task,checking natural language intent
:task,translation of natural language intent
:task,translating informal to formal specifications
:problem,LLM4NL2POST
:metric,correctness and discriminative power of generated postconditions
:finding,LLM4NL2POST postconditions quality
:potential,LLM4NL2POST via LLMs in practice
:issue,inconsistency in generation and validation
:framework,generator-validator consistency
:metric,consistency
:metric,GV-consistency
:approach,consistency fine-tuning
:model,ALPACA-30B
:concept,unseen tasks and domains
:metric,generator quality and validator accuracy
:need,interpretable and transparent tools
:challenge,implementation difficulty
:tool,DeepDecipher
:capability,probing neurons
:technique,advanced interpretability techniques
:task,model inspection
:task,scalable analysis
:value,transparency
:tool,NeuroScope
:tool,OpenAI's Neuron Explainer
:task,event forecasting
:goal,informed decision-making
:`data type`,structured data
:task,traditional forecasting
:`data type`,unstructured text
:benchmark,AutoCast
:`data source`,news articles
:system,AutoCast++
:approach,zero-shot question-passage relevance
:task,re-ranking articles
:approach,zero-shot summarization
:task,summarizing articles
:task,relevance evaluation and summarization
:concept,temporal dynamics
:approach,re-ranking mechanism
:approach,multi-passage representation learning
:performance,AutoCast++ performance
:metric,true/false (TF) questions
:application,AI applications
:issue,handling long sequences
:task,extended sequence tasks
:approach,Ring Attention
:capability,large sequence input size
:issue,memory constraints
:performance,task performance
:`model architecture`,object-level multimodal large language model
:dataset,driving QA dataset
:task,driving scenario understanding
:strategy,pretraining strategy
:metric,driving QA evaluation metric
:task,driving scenario interpretation
:model,LLM-Driver
:technique,behavioral cloning
:resource,"benchmark, datasets, and model"
:task,visual word sense disambiguation
:field,multimodal retrieval
:`model family`,visiolinguistic transformer
:technique,knowledge-enhancement
:process,internal reasoning
:approach,exploiting knowledge stored in LLMs
:limitation,fixed context window
:task,long code generation
:solution,memory-augmented large language model
:model,L2MAC
:component,instruction registry
:component,file store
:capability,unbounded code generation
:task,fulfilling user requirements
:task,system design tasks
:field,ocean science
:entity,oceans
:entity,Earth's surface
:activity,scientific paradigm
:profession,oceanographers
:model,OceanGPT
:framework,DOInstruct
:task,ocean domain instruction data generation
:benchmark,OceanBench
:task,LLM capability evaluation
:skill,knowledge expertise in ocean science
:skill,embodied intelligence in ocean technology
:resource,"codes, data and checkpoints"
:task,web element localization
:activity,web-based test automation
:method,traditional web element localization
:attribute,web element attributes
:approach,von Similo LLM
:attribute,human-like selection accuracy
:study,experimental study on von Similo LLM
:data,804 web element pairs
:algorithm,baseline algorithm
:`performance metric`,reduced failed localizations
:technology,LLM technology
:task,GUI test automation
:model,LLaSM
:model,X-LLM
:model,SpeechGPT
:task,end-to-end speech translation (E2E-ST)
:model,LST
:`model components`,"speech frontend, adapter, LLM backend"
:process,"modality adjustment, downstream task fine-tuning"
:model,LST-13B
:benchmark,MUST-C speech translation benchmark
:topic,"single-modal model selection, training strategies"
:promise,open source
:capability,embodied decision-making
:model,GPT4-Vision
:benchmark,PCA-Eval
:framework,HOLMES
:capability,multimodal informed decision-making
:model,GPT4-HOLMES
:`model family`,open-source state-of-the-art multimodal large language model
:capability,decision-making in embodied agents
:resource,PCA-Eval code and data
:URL,https://github.com/pkunlp-icler/pca-eval/
:quality,task generalizability
:technique,effective task instructions
:approach,PromptD
:technique,customized instructions
:dataset,complex math dataset
:dataset,HumanEval
:approach,conventional zero-shot methods
:goal,defense against adversarial prompting
:resource,source code and dataset for PromptD
:url,https://github.com/salokr/promptd
:data,web-crawled datasets
:task,dataset pruning
:performance,vision-language model performance
:method,CLIPScore-based pruning
:issue,pruning limitations
:method,Sieve
:`model family`,image-captioning model
:performance,Sieve performance
:team,Task Wizard Team (Twiz)
:event,Alexa Prize Taskbot Challenge 2022
:assistant,Twiz Bot
:characteristics,multimodal knowledgeable engaging assistant
:task,complex manual tasks
:`research question`,humanly-shaped conversations
:method,knowledgeable information delivery
:`research question`,multimodal stimulus
:method,multimodal interaction
:`research question`,zero-shot conversational flows
:method,robust interaction for unseen scenarios
:feature,creative cooking
:feature,video navigation through voice
:model,Twiz-LLM
:characteristics,effective and robust system
:environment,social environments
:concept,multi-agent society
:concept,collaborative intelligence
:method,practical experiments and theoretical insights
:concept,collaboration mechanisms
:entity,LLM agents with traits and thinking patterns
:behavior,social behaviors
:strategy,collaborative strategies
:behavior,human-like social behaviors
:theory,social psychology theories
:goal,further research
:trend,cost of fine-tuning
:phenomenon,butterfly effect
:research,investigation into knowledge editing pitfalls
:concern,knowledge distortion
:url,https://github.com/zjunlp/pitfallsknowledgeediting
:method,knowledge graph enrichment
:task,answering factoid questions
:algorithm,subgraph extraction
:entity,question entities and answer candidates
:process,linearization of subgraphs
:process,re-ranking
:metric,hits@1 score
:task,editing personality traits
:task,opinion-related questions
:dataset,PersonalityEdit
:`personality trait`,neuroticism
:`personality trait`,extraversion
:`personality trait`,agreeableness
:experiment,comprehensive experiments
:finding,challenges of editing personality traits
:finding,representation of personality behavior in LLMs
:url,https://github.com/zjunlp/easyedit
:task,wide range of tasks
:technique,ensembling LLM agents
:approach,fixed set of agents
:framework,Dynamic LLM-Agent Network (Dylan)
:task,complicated tasks
:feature,dynamic architecture
:algorithm,automatic agent team optimization
:metric,agent importance score
:task,reasoning and code generation tasks
:benchmark,Math and HumanEval
:benchmark,MMLU
:`agent type`,highly autonomous generative agent
:challenge,real-time interaction with humans
:constraint,low computational cost
:agent,Lyfe Agent
:attribute,low-cost real-time responsiveness
:framework,option-action framework
:task,high-level decision making
:mechanism,asynchronous self-monitoring
:attribute,self-consistency
:mechanism,summarize-and-forget memory
:attribute,critical memory prioritization
:platform,LyfeGame 3D virtual environment
:attribute,self-motivation and sociability
:technique,brain-inspired techniques
:attribute,human-like self-motivated social reasoning
:scenario,crime solving
:task,autonomous collaboration and information exchange
:metric,computational cost efficiency
:potential,transformative potential
:`agent type`,autonomous generative agent
:role,virtual assistants
:attribute,stability and reliability
:behavior,wavering judgements
:strategy,questioning strategies in education
:mechanism,follow-up questioning mechanism
:metric,judgement consistency
:model,PaLM-2 Bison
:model,Vicuna-13B
:phenomenon,disturbances
:setting,sampling temperature and prompts
:concept,world model
:dataset,spatial and temporal datasets
:dataset,spatial datasets
:entity,places
:dataset,temporal datasets
:entity,events
:representation,linear representations of space and time
:technique,prompting variations
:entity,different entity types
:neuron,space and time neurons
:knowledge,structured knowledge of fundamental dimensions
:`model family`,multimodal language model
:value,user privacy and information security
:`trade-off`,privacy protection vs. model utility
:benchmark,PrivQA
:technique,iterative self-moderation
:experiment,red-teaming experiments
:issue,privacy protection circumvention
:goal,improved privacy protections
:dataset,PrivQA dataset
:resource,https://llm-access-control.github.io/
:method,token production
:concept,hidden vector
:process,token generation
:method,pause-training
:concept,pause token
:data,C4
:task,causal pretraining
:model,1B parameter model
:task,SQuAD
:task,CommonsenseQA
:task,GSM8k
:paradigm,delayed next-token prediction
:field,language modelling
:data,electronic medical records
:information,medical information
:task,clinical applications
:task,relation extraction
:goal,understanding treatment history
:project,MedTem
:task,medication extraction and temporal relations classification
:`model architecture`,BiLSTM-CRF
:task,clinical domain NER
:`model architecture`,CNN-BiLSTM
:`model architecture`,BERT-CNN
:task,temporal relation extraction
:technique,word embedding
:technique,post-processing rules
:output,structured medication and temporal information
:resource,MedTem code and tools
:data,internet corpora
:issue,copyrighted content
:technique,unlearning
:model,LLaMA2-7B
:task,unlearning Harry Potter books
:metric,184k GPU-hours
:benchmark,common benchmarks
:component,reinforced model
:task,token identification for unlearning
:technique,alternative label generation
:technique,finetuning on alternative labels
:capability,simultaneous image and text generation
:technique,interleaved vision-and-language generation
:concept,generative vokens
:output,harmonized image-text outputs
:strategy,two-staged training
:task,description-free multimodal generation
:technique,classifier-free guidance
:model,MiniGPT-5
:model,DiVTER
:dataset,VIST
:benchmark,diverse benchmarks
:`model interface`,Talk2BEV
:`perception system`,existing autonomous driving perception systems
:task,object categorization
:`model type`,task-specific models
:task,autonomous driving tasks
:task,scene understanding tasks
:benchmark,Talk2BEV-bench
:skill,problem-solving
:benchmark,MathVista
:challenge,mathematical and visual tasks
:dataset,multimodal datasets
:goal,general-purpose AI agents
:ability,self-verification
:URL,https://mathvista.github.io/
:phenomenon,neural scaling laws
:outcome,model performance improvement
:analysis,approximation theory analysis
:phenomenon,MSE loss decay
:`model attribute`,number of model parameters
:variable,n
:`model attribute`,intrinsic input dimension
:variable,d
:problem,1D problem y=x^2
:phenomenon,different scaling law
:mechanism,lottery ticket ensembling
:phenomenon,new scaling law
:concept,central limit theorem
:phenomenon,n^-1 scaling law
:theory,statistical physics-type theories of learning
:technique,contrastive post-training
:model,InstructGPT
:technique,SLiC
:technique,DPO
:technique,data curriculum learning
:model,Orca
:framework,task condition generation framework
:`task condition`,primitive task conditions
:technique,dynamic movement primitives
:`task suite`,robotic manipulation task suite
:platform,PyBullet
:task,long-horizon manipulations
:attribute,robotic system versatility and adaptability
:concept,junk DNA
:`biological entity`,non-coding DNA segments
:`biological entity`,human genome
:`biological process`,cellular processes
:component,weights in deep neural networks
:`biological entity`,human genes
:belief,redundancy in gigantic models
:concept,pruning without performance loss
:concept,knowledge encapsulation
:hypothesis,junk DNA hypothesis
:concept,knowledge forgetting
:action,pruning small-magnitude weights
:concept,task-sensitive knowledge encoding
:`research direction`,model pruning
:concept,task-aware conditional computation
:method,generalized zero-shot semantic segmentation
:method,finetuning CLIP paradigm
:`model component`,fixed backbone model
:method,mask classification
:component,explicit mask proposer
:method,prevalent methods
:data,seen categories
:framework,ClipTeacher
:task,per-pixel classification segmentation
:data,seen and ignoring areas
:module,global learning module
:token,CLS token
:module,pixel learning module
:data,pseudo annotations
:data,whole image
:technique,scaffolding program
:goal,better output generation
:concept,seed improver
:goal,program improvement
:concept,improved improver
:concept,self-improving code
:concept,self-improvement strategies
:concept,self-improving technologies
:value,safety and ethics
:concept,generated code
:system,autonomous navigation system
:assumption,HD LiDAR maps
:system,ALT-Pilot
:data,public road network information
:data,crowdsourced language landmarks
:technology,onboard sensors
:map,language-augmented topometric map
:data,web-scale data
:technology,recursive Bayesian state estimation
:technology,reactive trajectory planner and controller
:method,simulation and real-world testing
:benchmark,state-of-the-art topometric navigation systems
:`model type`,self-attention-based recommender model
:issue,computational expense and speed
:level,sequence-level
:advancement,efficient language modeling
:model,LRUREC
:feature,rapid and incremental inference
:operation,linear recurrence
:benefit,reduced model size and parallelizable training
:optimization,architecture modifications
:feature,efficiency in training and inference
:data,publicly available code
:issue,test smells
:technique,reinforcement learning from static quality metrics (RLSQM)
:technique,proximal policy optimization (PPO)
:model,RL-optimized model
:artifact,high-quality test cases
:metric,syntactical correctness
:link,https://figshare.com/s/ded476c8d4c221222849
:system,conversational health agent
:service,personal healthcare
:activity,empathetic conversation
:capability,comprehensive agent capabilities
:framework,LLM-powered framework for CHAs
:ability,"critical thinking, knowledge acquisition, problem-solving"
:`data source`,healthcare data sources
:tool,user data analysis tools
:`case study`,stress level estimation
:capability,cognitive and operational capabilities
:context,long contexts
:method,dynamic contextual compression
:problem,scaling to long contexts
:method,nugget approach
:`model architecture`,BERT-like frameworks
:concept,nuggets
:task,reconstruction
:model,Nugget2D
:task,"language modeling, question answering, summarization"
:task,autoencoding
:capability,understanding and following instructions
:attribute,harmless and helpful
:strategy,moving target defense
:system,MTD-enhanced LLM system
:goal,non-toxic and aligned answers
:model,query and output analysis model
:attribute,unsafe or non-responsive answers
:technique,adversarial queries
:metric,response refusal rate
:technique,text-guided image editing
:`model family`,diffusion-based generative model
:model,Imagen
:model,Stable Diffusion
:benchmark,EditVal
:task,text-guided image editing evaluation
:component,"curated dataset, editable attributes, automated evaluation pipeline"
:method,SINe
:method,Imagic
:method,Instruct-Pix2Pix
:study,large-scale human study
:method,Null-Text
:challenge,spatial operation edits
:resource,EditVal code and human-study templates
:website,deep-ml-research.github.io/editval
:resource,online resources
:goal,security & privacy advice
:resource,trusted information sources
:attribute,accuracy and correctness
:task,answering multiple-choice questions
:dataset,S&P-related misconceptions dataset
:literature,recent academic literature
:metric,error rate
:issue,invalid urls
:evaluation,mathematical reasoning evaluation
:role,novice learner and expert tutor
:role,novice learner
:concept,mathematical misconception
:role,expert tutor
:evaluation,traditional mathematical evaluation
:goal,correct math answers
:field,educational learning sciences
:task,identifying misconceptions
:opportunity,enhancing math reasoning capabilities
:application,intelligent tutoring systems
:practice,AI safety training
:issue,unsafe content generation
:vulnerability,cross-lingual vulnerability
:issue,linguistic inequality in safety training data
:benchmark,AdvBenchmark
:attack,jailbreaking attack
:`language resource`,low-resource languages
:issue,technological disparities
:deficiency,limited training on low-resource languages
:user,LLM users
:service,publicly available translation APIs
:vulnerability,LLM safety vulnerabilities
:effort,holistic red-teaming efforts
:goal,robust multilingual safeguards
:theory,post-structuralist socio-political theory
:concept,empty signifiers
:framework,alignment operationalisation framework
:goal,shared vocabulary
:literature,empirical literature on alignment
:task,deciding on alignment paradigm
:data,domain-specific data
:information,personally identifiable information (PII)
:`model family`,privacy protection language model
:concept,model design
:technique,corpus curation
:technique,penalty-based unlikelihood training
:technique,instruction-based tuning
:goal,privacy protection
:technique,human alignment
:bottleneck,manual instruction dataset creation
:process,scaling LLM development
:idea,AI as teacher
:process,training student LLMs
:technique,curriculum instruction tuning (CITing)
:analogy,human learning process
:`model role`,teacher LLM
:`model role`,student LLM
:`evaluation metric`,GPT-4 evaluation
:database,National Vulnerability Database (NVD)
:profession,application developers
:issue,patch incompleteness
:task,vulnerability patch identification
:issue,low accuracy
:approach,CompVPD
:model,Starcoder
:algorithm,novel algorithms in CompVPD
:task,context generation
:approach,state-of-the-art/practice approaches
:field,security practice
:algorithm,federated averaging
:requirement,same model architecture
:strategy,heterogeneous model training
:goal,information exchange
:technique,bi-directional knowledge distillation
:goal,domain transfer
:data,out-of-domain distillation data
:data,in-domain distillation data
:challenge,storage impracticality
:concept,low-rank modification
:goal,parameter reduction
:limitation,rank one decomposition lower bound
:limitation,influence of model architecture and rank
:technique,NOLA
:model,ViT
:performance,model efficiency
:`evaluation setting`,IID
:community,VQA community
:`evaluation setting`,OOD
:`evaluation setting`,new paradigm
:need,robust automatic VQA metrics
:value,human judgment
:task,building VQA metric
:evaluation,VQA evaluation
:task,answer-rating
:metric,proposed VQA metric
:`evaluation setting`,VQA models and benchmarks
:goal,estimating research progress
:capability,visually grounded reasoning
:activity,comprehensive quantitative evaluation
:benchmark,multi-modal benchmarks
:format,task-oriented input-output
:proposal,re-formulate benchmarks
:benchmark,reform-eval benchmark
:capability,evaluation of LVLMS
:activity,strength and weakness analysis
:concept,underlying factors
:resource,benchmark and evaluation framework
:`model type`,mixture-of-experts model
:task,code-switching automatic speech recognition
:issue,similar pronunciation across languages
:`model performance`,ineffective multi-language modeling
:`model performance`,inaccurate language boundary estimation
:method,cross-layer language adapter
:goal,language-specific representation separation
:method,boundary-aware training
:goal,language boundary confusion handling
:model,boundary-aware mixture-of-experts (BA-MoE)
:component,language-specific adapters
:component,unified gating layer
:loss,language adaptation loss
:goal,language-specific representation learning improvement
:component,boundary-aware predictor
:metric,mixture error rate reduction
:task,cyber threat intelligence reporting
:strategy,risk management
:demand,automated report generation tools
:trend,surge in CTI report volume
:challenge,complexity of diverse data sources
:paradigm,STIX
:community,CTI community
:tool,AGIR
:role,security analysts
:evaluation,AGIR report generation capabilities
:metric,recall value
:metric,syntactic log-odds ratio (SLOR)
:benefit,reduction in report writing time
:task,cyber threat intelligence tasks
:framework,Markov decision process
:problem,sequential decision-making
:problem,finite-time horizon problems
:application,optimal stopping
:application,supply chain problems
:practice,simultaneous parameter training
:concept,dynamic programming structure
:method,dynamic policy gradient
:concept,dynamic programming
:parametrization,tabular softmax
:analysis,convergence analysis
:finding,dynamic policy gradient convergence
:advantage,improved convergence bounds
:impact,industry revolution
:system,uTalk
:technology,integrated technologies
:service,Microsoft Speech Services
:system,SADTalker
:platform,Streamlit
:metric,running time improvement
:task,fine-tuning large language models
:`model component`,learned reward model
:phenomenon,overoptimization
:study,Gao et al. (2023)
:method,ensemble-based conservative optimization
:`optimization objective`,worst-case optimization
:`optimization objective`,uncertainty-weighted optimization
:`optimization method`,best-of-n sampling
:`optimization method`,proximal policy optimization
:study,Gao et al. (2023) extended setup
:technique,KL penalty
:era,digital era
:`population fraction`,large fraction with comprehension issues
:problem,comprehension issues
:initiative,accessibility initiatives
:goal,audience text comprehension
:profession,writers
:task,producing easy-to-understand content
:field,automatic text simplification
:problem,lack of comprehension difficulty metric
:metric,LC-Score
:task,training text comprehension metric
:initiative,Langage Clair
:initiative,Plain Language
:approach,linguistically motivated indicators
:task,training statistical models
:approach,neural learning
:technique,pre-trained language models
:task,comprehension difficulty training
:task,classification task
:experiment,human annotation experiments
:`model family`,text comprehension models
:metric,FKGL
:approach,indicator based and neural approaches
:technology,automated essay scoring
:status,cutting-edge
:technology,scoring techniques
:purpose,various purposes
:score,reliable scores
:variable,influential variables
:method,domain-specific methods
:concept,user's understanding
:analysis,scoring index analysis
:concept,understanding of a topic
:results,research results
:field,learning analytics
:task,summarizing and gauging understanding
:tool,Langchain
:task,summarizing PDF
:technique,summarization technique
:scenario,clinical scenarios
:issue,content deviation and biases
:framework,augmented LLM framework
:community,healthcare community
:system,Unified Medical Language System
:model,LLaMA2-13B-Chat
:evaluation,automatic evaluations
:evaluation,automatic and physician evaluations
:metric,ROUGE score and BERTScore
:evaluation,physician evaluation
:criteria,"factuality, completeness, readability, and relevancy"
:participant,resident physicians
:quality,"factuality, completeness, and relevance"
:application,medical question-answering
:task,visual language reasoning
:process,extraction and reasoning
:approach,end-to-end vision-language model
:approach,two-stage pipeline
:model,captioning model
:format,text
:system,dual-system for multi-step multimodal reasoning
:task,multi-step multimodal reasoning
:system,System-1
:process,visual information extraction
:system,System-2
:process,deliberate reasoning
:model,LLaMA-2 70B
:model,FLAN-PaLM 540B
:dataset,challenging dataset with human-authored questions
:task,single task
:trend,prompt instruction tuning
:question,handling heterogeneous tasks and data distributions
:technique,mixture of prompts (MoPs)
:functionality,smart gating
:issue,prompt training interference
:scenario,federated scenario
:scenario,centralized scenario
:factor,instructions
:task,instruction optimization
:characteristic,high-dimensional objective functions
:model,Gaussian process
:`model family`,neural network
:capability,modeling complex functions
:algorithm,neural bandit
:`model family`,pre-trained transformer
:algorithm,INSTINCT
:task,instruction induction tasks
:task,zero-shot chain-of-thought instruction
:issue,climate change
:entity,individuals and communities
:activity,mitigation and adaptation to climate change
:topic,climate change communication
:framework,evaluation framework for LLM responses
:principle,science communication principles
:quality,adequacy of answers
:issue,LLM output issues
:concept,AI-human complementarity
:protocol,scalable oversight protocol
:concept,AI assistance
:entity,raters with relevant educational backgrounds
:topic,climate communication
:process,downstream application development
:technique,safety-alignment measures
:threat,hard prompt attack
:attack,shadow alignment
:technique,safety-alignment subversion
:`model family`,safely aligned large language model
:`model family`,subverted large language model
:task,regular inquiries
:experiment,shadow alignment effectiveness
:model,various LLMs
:task,multi-turn dialogue and other languages
:goal,LLM safety fortification
:outcome,improved task performance
:method,text-to-text instruction tuning
:issue,"limitations in generalization, robustness, and controllability"
:method,JSONTuning
:format,JSON
:outcome,"enhanced generalization, improved robustness, increased controllability"
:study,comprehensive comparative study
:outcome,"improved performance, adaptability, robustness, and controllability"
:task,intricate reasoning tasks
:challenge,effective exemplar selection
:framework,DQ-LoRE
:technique,dual queries
:technique,low-rank approximation re-ranking
:approach,retrieval-based approaches
:task,complex reasoning challenges
:capability,multi-tasking
:`model behavior`,multi-task performance
:model,UniVerSLU
:task,speech classification and sequence generation tasks
:technique,natural phrase prompting
:`model behavior`,interpretable interaction
:capability,generalization to paraphrases
:method,text-to-3D
:`model type`,pretrained diffusion model
:model,NeRF
:output,high-quality 3D scenes
:benchmark,T3Bench
:task,text-to-3D evaluation
:metric,quality metric
:aspect,multi-view text-image scores
:metric,alignment metric
:technique,multi-view captioning
:outcome,performance differences
:analysis,project analysis
:challenge,generating surroundings and multi-object scenes
:website,T3Bench.com
:concept,learning
:process,discovery and memorization of abstract rules
:mechanism,associative memory
:structure,high-dimensional matrices
:`model component`,inner layers of Transformer
:concept,scaling laws
:attribute,sample size and parameter size
:concept,statistical efficiency
:technique,estimators and optimization-based algorithms
:method,numerical experiments
:outcome,validation and interpretation of theoretical results
:technique,fine-grained visualizations
:concept,memory associations
:outcome,performance improvements
:issue,harmful biases
:concept,intersectional social attributes
:challenge,collecting exhaustive set of image-text pairs
:task,probing intersectional social biases
:technique,stable diffusion with cross attention control
:output,counterfactual image-text pairs
:dataset,generated dataset
:task,revealing intersectional social biases
:issue,intersectional social biases
:task,analysis of scientific datasets
:challenge,tokenizing numbers
:`encoding scheme`,XVAL
:task,efficient number representation
:technique,scaling embedding vectors
:approach,modified number-inference
:`model characteristic`,end-to-end continuous
:`inductive bias`,suitable for scientific applications
:`encoding scheme`,existing number encoding schemes
:achievement,significant strides
:field,vision-language-to-image generation
:challenge,generalized vision-language input generation
:model,Kosmos-G
:technique,compositional instruction tuning
:capability,zero-shot multi-entity subject-driven generation
:technique,score distillation instruction tuning
:component,image decoder
:goal,image as a foreign language in image generation
:capability,multimodal understanding
:task,multimodal downstream tasks
:technique,iterative global pruning
:technique,layer-wise pruning
:issue,suboptimal model compression
:technique,efficient coarse-to-fine layer-wise pruning (ECOFLaP)
:method,coarse-to-fine weight pruning
:concept,global importance score
:method,layer-wise unstructured weight pruning
:task,model and dataset validation
:domain,"law, finance, medicine"
:challenge,computational challenges
:cost,inference energy cost
:cost,training energy cost
:understanding,resource utilization
:goal,"cost-savings, scaling performance, efficient hardware usage, optimal inference strategies"
:experiment,computational and energy utilization of inference
:hardware,NVIDIA V100 & A100 GPUs
:dataset,ALPACA and GSM8K
:task,LLM tasks/benchmarks
:technique,"multi-node, multi-GPU inference using model sharding"
:concept,gradient-based learning algorithms
:concept,learning algorithms implementation
:concept,other forms of algorithms
:`model architecture`,attention-based model
:concept,learning capabilities
:concept,stylized experimental insights
:`model architecture`,attention-free model
:concept,teaching sequence
:concept,algorithm selection
:model,nearest-neighbor baseline
:task,multimodal information extraction
:process,extracting structured information
:`model family`,multimodal information extraction model
:characteristic,task-specific and data-intensive
:limitation,limited generalization
:framework,multimodal question answering
:principle,utilizing large multimodal models
:benchmark,zero resource code-switched speech benchmark
:system,baseline system
:`model family`,self-supervised speech encoder
:model,wav2vec 2.0
:experiment,code-switching capability assessment
:model,HuBERT
:model,XLSR
:factor,pre-training languages
:factor,model size
:model,monolingual variants
:capability,code-switching linguistic abilities
:trend,extending context window
:solution,retrieval-augmentation
:model,proprietary 43B GPT
:model,LLaMA2-70B
:model,DaVinci003
:model,LLaMA2-70B-32K baseline
:insight,retrieval-augmentation vs long context extension
:audience,practitioners
:task,long context tasks
:task,query-based summarization
:system,autonomous driving system
:challenge,"comprehension, generalization, and interpretability"
:component,decision-making component
:concept,cognitive pathways
:capability,comprehensive reasoning
:algorithm,LLM decision translation
:task,actionable driving commands
:method,guided parameter matrix adaptation
:component,low-level controllers
:performance,surpassing baseline approaches
:task,complex driving behaviors
:goal,effective decision-making in AD
:project,LLM-MPC project page
:url,https://sites.google.com/view/llm-mpc
:task,molecular property prediction
:representation,SMILES
:quality,generalizable representation
:model,GPT-MolBERTa
:data,textual descriptions of molecules
:benchmark,molecule property benchmarks
:mechanism,attention mechanisms
:`user group`,users with limited IT expertise
:knowledge,understanding of LLMs
:behavior,taking output at face value
:study,systematic analysis of prompts and responses
:issue,gender biases
:perspective,gender perspective
:investigation,in-depth investigation of prompts
:phenomenon,response variability
:task,drafting texts
:necessity,thorough check of system's responses
:quality,response accuracy
:task,code-driven queries
:action,invoking external APIs
:issue,iterative code refinement
:`use case`,high query volumes
:framework,EcoAssistant
:component,automatic code executor
:strategy,hierarchy of LLM assistants
:technique,retrieving solutions from past successful queries
:`cognitive ability`,theory-of-mind (ToM)
:action,inferring mental states
:benchmark,ToMI
:`evaluation paradigm`,Thinking for Doing (T4D)
:capability,connecting inferences to actions
:challenge,translating capability into strategic action
:`prompting framework`,Foresee and Reflect (FAR)
:scenario,out-of-distribution story structures
:issue,long input sequences
:solution,increase input length
:process,human memory selection
:network,Memoria
:theory,Hebbian theory
:concept,engram
:capability,long-term dependencies
:task,sorting
:task,long text classification
:concept,knowledge representation
:problem,localizing and disentangling knowledge representations
:status,open
:concept,knowledge-critical subnetworks
:concept,specific knowledge encoding
:method,multi-objective differentiable weight masking
:goal,knowledge removal with minimal adverse effects
:model,GPT-2 variants
:concept,highly sparse subnetworks
:concept,specific relational knowledge
:concept,knowledge removal
:outcome,performance drop on knowledge-dependent tasks
:cost,high expense of paid API services
:concept,LLM cascade
:cost,cost of using LLMs
:`model family`,weaker large language model
:task,challenging reasoning tasks
:`model family`,stronger large language model
:concept,question difficulty
:method,answer sampling and consistency checking
:concept,LLM cascade decision-making
:`thought representation`,program-of-thought
:performance,comparable performance at reduced cost
:process,collaborative model training
:challenge,domain shift
:scenario,federated learning with domain shift
:technique,prompt learning
:method,federated dual prompt tuning (Fed-DPT)
:experiment,Fed-DPT evaluation
:model,CLIP (ViT-Base)
:`performance metric`,average accuracy
:dataset,DomainNet
:process,unsupervised pre-training
:property,l2 sensitivity
:`training method`,differentially private SGD (DP-SGD)
:`training method`,DP-SGD variant for similarity-based loss functions
:issue,high l2 sensitivity in differentially private training
:dataset,CIFAR-10
:dataset,CIFAR-100
:benchmark,non-private model performance
:`study focus`,tool utilization ability of LLMs
:application,AutoGPT
:application,MetaGPT
:benchmark,MetaTool
:capability,tool usage awareness and selection
:dataset,ToolE
:task,tool usage awareness
:task,tool selection
:experiment,evaluation of LLMs on MetaTool
:insight,for tool developers
:challenge,data access
:process,training large language models
:approach,hardware-centric
:technology,edge computing systems
:`model family`,FLAN-T5
:benchmark,micro-level hardware benchmark
:hardware,data center GPU
:evaluation,edge computing systems capabilities
:workload,LLM federated learning workloads
:comparison,edge systems vs data center GPU
:goal,computational efficiency at the edge
:field,program synthesis
:goal,functional correctness optimization
:`algorithm type`,policy-based algorithm
:`algorithm type`,value-based method
:resource,off-policy programs
:technique,automated unit testing
:task,program verification
:model,B-coder
:challenge,enormous search space
:technique,initialization protocol for RL agents
:challenge,training complexity reduction
:technique,learned value functions
:task,post-processing of generated programs
:technique,minimal reward engineering
:problem,medical image classification
:challenge,lack of interpretability
:value,trustworthiness and safety
:paradigm,robust and interpretable medical image classifiers with natural language concepts
:challenge,spurious correlations and lack of interpretability
:task,querying clinical concepts
:concept,explicit concepts
:method,systematic evaluation
:method,mitigating spurious correlations
:outcome,outperforming standard visual encoders
:concept,classification with a small number of concepts
:method,case studies in real medical data
:activity,interactive question-answering
:audience,middle-school math students
:activity,tutoring process automation
:issue,incorrect or mismatched responses
:solution,retrieval-augmented generation
:resource,high-quality open-source math textbook
:system,RAG system for middle-school algebra and geometry QA
:activity,multi-condition survey
:outcome,human preference for RAG-generated responses
:consideration,trade-offs in math QA system design
:audience,math QA system designers
:capability,tool usage and multimodal processing
:outcome,benefits and security risks
:attacker,adversarial attacker
:technique,visual adversarial examples
:action,undesired tool usage
:attack,prior adversarial attacks
:technique,gradient-based adversarial training
:`performance metric`,success and similarity rates
:outcome,manipulation and stealthiness
:evaluation,human scoring and automated metrics
:outcome,conversation quality maintenance
:benefit,personalized learning
:issue,uncertainty signalling
:issue,lack of pedagogical stance
:framework,pedagogical transparency in GAI-based educational applications
:method,training methods for pedagogical principles
:method,controlled pedagogical interactions
:skill,meta-cognitive skills and GAI literacy
:characteristic,demanding
:agent,automated Socratic conversational agent
:activity,human instruction
:challenge,lack of suitable data
:dataset,Socratic advice dataset
:URL,https://github.com/taisazero/socratic-debugging-benchmark
:technique,model grafting
:study,comparative analysis of multimodal instruction tuning approaches
:`task list`,"complex reasoning, conversation, image captioning, MCQs, binary classification"
:method,benchmarking and ablation experiments
:insight,architectural choices for multimodal capabilities
:limitation,lack of diverse multimodal instruction dataset
:capability,task generalization
:issue,truthfulness and factuality in responses
:guidance,for adapting language models for image comprehension
:community,researchers and practitioners
:capability,dynamic adaptation
:benchmark,FreshQA
:task,factuality assessment
:`evaluation procedure`,two-mode evaluation
:metric,correctness and hallucination
:metric,model limitations
:method,FreshPrompt
:method,search engine-augmented prompting
:factor,retrieved evidence number and order
:instruction,generate concise and direct answers
:resource,FreshQA dataset
:`linguistic feature`,first-person singular pronouns
:`mental status`,depression symptom severity
:method,frequency counting
:technology,neural language modeling
:data,de-identified text messages
:evaluation,depression severity assessment
:method,contextualized first-person pronoun embeddings
:method,frequency-based pronoun analysis
:task,wide spectrum of tasks
:task,long-term planning and spatial reasoning
:benchmark,Path Planning from Natural Language (PPNL)
:ability,spatial-temporal reasoning
:task,path planning
:ability,long-term temporal reasoning
:task,in-distribution reasoning tasks
:task,generalization in larger or more complex environments
:property,scaling properties
:outcome,incomplete answer
:property,optimization loss
:factor,model size increase
:law,scaling law for task
:concept,emergent abilities
:phenomenon,dramatic improvement beyond size threshold
:performance,task performance improvements
:`evaluation strategy`,PassUntil
:law,task scaling law
:performance,task performance predictability
:model,2.4B model
:phenomenon,continuity of performance improvement
:hypothesis,multi-step reasoning hypothesis
:hypothesis,new hypothesis for emergent abilities
:task,tabular data prediction
:method,discriminative modeling
:concept,universal tabular data predictors
:model,UniPredict
:data,169 tabular datasets
:data,62 tabular datasets
:model,XGBoost
:vision,universal tabular data prediction system
:domain,biological sequences
:model,InstructProtein
:capability,bidirectional generation
:data,protein and natural language corpora
:technique,supervised instruction tuning
:goal,language alignment
:framework,knowledge graph-based instruction generation
:goal,high-quality instruction dataset
:process,causal modeling of protein functions
:benchmark,state-of-the-art LLMs
:goal,text-based protein function prediction and sequence design
:`molecular region`,5' UTR
:`biological process`,translation regulation
:task,decoding genomic functions
:model,UTR-LM
:data,endogenous 5' UTRs
:information,supervised information
:metric,prediction performance
:task,identifying internal ribosome entry sites
:metric,AUPR
:library,novel 5' UTRs
:task,translation efficiency prediction
:metric,protein production level
:concept,risks
:task,Natural Language Inference (NLI)
:risk,decision risk
:risk,composite risk
:framework,risk-centric evaluation framework
:metric,novel metrics
:method,DWD
:experiment,detailed experiments
:framework,SimVLG
:task,vision-language generative model pre-training
:paradigm,vision-language pre-training (VLP)
:process,two-stage optimization
:task,vision-language representation learning
:process,first stage of VLP training
:process,gradual merging of visual tokens
:performance,fast convergence
:performance,training speed
:performance,data efficiency
:model,image-text models
:task,video-language generative tasks
:issue,implicit user intentions
:issue,incomplete domain knowledge
:framework,EDIT
:dataset,context-open-question (COQ)
:capability,long-context
:system,Megatron-LM
:technique,model-parallel
:limitation,communication volume
:approach,LightSeq
:dimension,sequence
:technique,gradient checkpointing
:performance,end-to-end speedup
:capability,longer sequence length
:URL,https://github.com/rulinshao/lightseq
:task,QA and retrieval
:issue,evaluation complexity
:issue,multi-dimensional evaluation criteria
:issue,subjectiveness
:dataset,Per-MPST
:task,personalized story evaluation
:dataset,Per-Doc
:model,PerSE
:resource,datasets and code
:task,deductive reasoning
:task,complex deductive problems
:technique,forward reasoning
:technique,backward reasoning
:problem,computational expense and error risk
:approach,concise and organized perception (COP)
:concept,human problem-solving
:benchmark,deductive benchmarks
:list,"ProofWriter, ProntoQA, ProntoQA-OOD"
:capability,zero-shot recognition
:outcome,satisfactory overall accuracy
:issue,inferior category performance
:dataset,ImageNet
:issue,zero class-wise accuracy
:application,risk-sensitive applications
:concept,class-wise matching margin (CMM)
:task,prompt performance estimation
:task,description enrichment
:technique,weighted ensemble
:outcome,improved accuracy on worst-10 categories
:domain,Chinese law
:data,in-domain training data
:framework,domain adaptation framework
:model,7B LLM
:domain,target domain
:process,adapt-retrieve-revise
:`knowledge base`,external in-domain knowledge base
:task,evidence retrieval
:task,evidence assessment
:method,adapt-retrieve-revise framework
:domain,Chinese legal tasks
:baseline,retrieval-based baselines
:factor,input prompt quality
:issue,biased or imbalanced input prompts
:algorithm,RICL
:algorithm,LARICL
:concept,convergence
:experiment,validation of LARICL
:data,numerical dataset
:benchmark,HalluQA
:phenomenon,hallucination in large language models
:data,adversarial questions
:`cultural aspect`,"Chinese historical culture, customs, and social phenomena"
:`hallucination type`,imitative falsehoods
:model,GLM-130B
:data,adversarial samples
:`evaluation method`,automated evaluation using GPT-4
:output,model output hallucination
:experiment,extensive experiments on large language models
:model,ERNIE-Bot
:model,Baichuan2
:model,ChatGLM
:model,Qwen
:model,SparkDesk
:finding,non-hallucination rates lower than 50%
:analysis,primary types of hallucinations
:discussion,hallucination prioritization
:task,procedure extraction
:challenge,obtaining sufficient training data
:technique,in-context learning customisations
:activity,communication
:entity,people
:expression,opinions
:expression,non-friendly or non-compliant expressions
:work,systematic implementation details
:model,privately deployed large language model
:process,fine-tuning with reasons
:`model family`,more powerful large language model
:approach,processing incorrect answers
:outcome,incorrect answers
:process,research process
:task,multi-document summarization
:issue,subjective bias
:metric,ROUGE-1 score
:goal,objectivity in news summarization
:approach,extract-rewrite
:goal,objective and informative summary
:function,main-event biased monotone-submodular function
:task,content selection
:task,rewriting for coherence
:evaluation,objective metrics and human evaluators
:benchmark,potential baselines
:aspect,pruning framework aspects
:metric,accuracy and inference speed
:study,in-depth analysis of pruning on large-scale recognition systems
:technique,low-rank approximation
:technique,incremental model compression
:outcome,models with varied target sizes
:technique,data-driven pruning
:technique,magnitude-driven pruning
:technique,incremental pruning
:technique,one-shot pruning
:metric,size reduction and inference speed-up
:`model family`,memory-efficient large language model
:task,text refinement
:attribute,controllability
:approach,controllable approach for multi-document summarization
:scheme,controllable content extraction
:task,text extraction
:policy,coverage and coherence intuitive policy
:metric,ROUGE metrics
:challenge,limited pretraining data
:`language type`,low and mid-resource languages
:strategy,model conversion
:resource,word translation dictionary
:process,token mapping
:component,embedding table
:language,Dutch
:language,Frisian
:benefit,reduced training data and time
:scope,many languages worldwide
:field,Software Engineering (SE)
:challenge,technical challenges
:property,emergent properties
:quality,novelty and creativity
:activity,software engineering activities
:technique,hybrid techniques
:solution,LLM-based SE solutions
:problem,incorrect solutions
:field,digital health
:service,healthcare delivery
:issue,usability and trust
:capability,general-purpose processing
:issue,inconsistent or nonsensical answers
:interface,novel interface
:impact,utility and practical impact
:application,cardiovascular disease risk prediction
:application,diabetes risk prediction
:technique,sharpness-aware minimization
:attribute,robustness under domain transfer
:focus,transferability of representations
:goal,out-of-domain generalization
:method,trust region-based fine-tuning
:skill,task-specific skills retention
:concept,trust region bounds
:algorithm,trust region aware minimization
:goal,flat minima and smooth representations
:task,cross-domain language modeling and cross-lingual transfer
:attribute,robustness to domain transfer
:standard,training generalizable models
:task,complex and interconnected tasks
:`system type`,autonomous multi-agent system
:goal,user-prompted goals
:challenge,balance between autonomy and alignment
:taxonomy,multi-dimensional taxonomy
:model,domain-ontology model
:classification,taxonomic classification
:process,entity mapping
:task,data integration
:resource,terminological resources
:concept,"ontologies, controlled vocabularies, taxonomies, value sets"
:approach,lexical mapping
:outcome,"high recall, low precision"
:problem,lexical ambiguity
:outcome,low precision
:task,manual mapping refinement
:tool,MapperGPT
:tool,LogMap
:evaluation,alignment tasks
:domain,"anatomy, developmental biology, renal diseases"
:attribute,complex annotation guidelines
:model,GOLLIE
:attribute,annotation guidelines compliance
:task,zero-shot information extraction
:attribute,detailed guidelines
:threat,jailbreaking attacks
:action,generating objectionable content
:algorithm,SmoothLLM
:finding,adversarially-generated prompts brittleness
:technique,character-level perturbation
:technique,prediction aggregation
:value,provable guarantees
:metric,number of queries
:principle,direct manipulation
:feature,continuous representation
:feature,prompt syntax reuse
:feature,manipulable outputs
:feature,undo mechanisms
:interface,DirectGPT
:technique,engineered prompts
:study,DirectGPT user study
:`use case`,downstream use cases
:organization,Meta
:model,LLAMA
:service,APIs for fine-tuning
:risk,safety guardrail compromise
:risk,safety alignment compromise
:infrastructure,safety alignment infrastructure
:risk,safety risks in fine-tuning
:dataset,benign and commonly used datasets
:risk,safety alignment degradation
:risk,new safety risks
:goal,reinforcing safety protocols
:concept,diverse human preferences
:approach,customization of language models
:technique,principle-based reward models
:technique,Multi-Objective Reinforcement Learning from Human Feedback (MORLHF)
:characteristic,unstable and resource-heavy
:algorithm,Multi-Objective Direct Preference Optimization (MODPO)
:task,long-form question answering
:outcome,competitive language model fronts
:method,zero-shot reasoning improvement method
:capability,zero-shot reasoning
:agent,autonomous agent
:task,general language understanding tasks
:model,LLaMA-2-70B-Chat
:method,zero-shot chain of thought
:resource,open-source preference datasets
:activity,experimentation
:resource,reward models
:observation,longer outputs
:factor,response length optimization
:relationship,reward and length
:phenomenon,length correlates with reward
:intervention,mitigate length increases
:finding,reward based on length
:task,instruction following tasks
:task,web tasks
:strategy,decomposition into sub-tasks
:policy,low-level closed-loop policy
:task,sub-tasks
:concept,shared grammar across tasks
:framework,HEAP
:technique,hierarchical LLM prompts
:task,suite of web tasks
:example,"MiniWoB++, WebArena, mock airline CRM, live website interactions"
:model,GPT-4 code interpreter
:task,solving math problems
:method,fine-tuning open-source language models
:dataset,MathCodeInstruct
:approach,customized supervised fine-tuning and inference
:`model family`,MathCoder models
:dataset,Math and GSM8K datasets
:model,MathCoder
:model,"ChatGPT-3.5, PALM-2, GPT-4"
:resource,MathCoder dataset and models
:URL,https://github.com/mathllm/mathcoder
:domain,programming education
:task,programming hints generation
:role,tutor model
:technique,GPT4Hints-GPT3.5Val
:goal,high-quality programming hints
:information,symbolic information of failing test cases and fixes
:task,automatic quality validation
:dataset,real-world datasets of Python programs
:concept,programming concepts
:task,ontology matching
:process,knowledge graph integration
:`model family`,deep ontology matching model
:technology,Transformer-based language model
:challenge,lack of reference alignments
:`model family`,ontology matching model
:challenge,runtime latency
:challenge,unexplored different graph structures
:framework,LAKERMAP
:concept,structural context
:dataset,BIO-ML
:resource,LAKERMAP code
:`protein characteristic`,lack of accessible binding pockets
:process,drug development
:`protein characteristic`,lack of conformational stability
:strategy,induced proximity
:alternative,pharmacological alternatives
:strategy,PROTACs
:strategy,molecular glues
:process,targeted protein degradation (TPD)
:activity,small molecule docking
:approach,computational design of protein-based binders
:challenge,accessing undruggable targets
:`model family`,protein language model
:process,prioritization of peptide binders
:system,CRISPR-analogous TPD system
:`protein characteristic`,target proteins
:model,PepMLM
:tool,de novo generator of linear peptide binders
:model,ESM-2
:process,reconstructing binder region
:tool,AlphaFold-Multimer
:process,endogenous degradation
:capability,generative design of candidate binders
:application,programmable proteome editing
:standard,AAPM TG-263
:technology,DICOM storage server
:`disease site`,prostate
:task,structure name re-labeling
:`disease site`,head and neck
:`disease site`,thorax
:result,re-labeling accuracy
:`disease site`,"prostate, head and neck, thorax"
:result,target volume re-labeling accuracy
:trend,rapid advancements in LLM capabilities
:`task class`,text-to-text generation
:task,paraphrase generation
:task,open-ended text completion
:aspect,targeted language styles
:approach,non-retrogressive approaches
:metric,learnable metric
:framework,fine-grained human evaluation
:`research area`,HCI+NLP+Accessibility
:application,writing assistant systems
:`research goal`,multi-agent coordination
:field,Artificial Intelligence (AI)
:framework,LLM-Coordination (LLM-Co)
:evaluation,game environments
:evaluation,theory of mind
:capability,inferring partner's intention
:evaluation,situated reasoning
:capability,reasoning actions in context
:evaluation,sustained coordination
:capability,long-horizon task coordination
:evaluation,robustness to partners
:capability,partner robustness
:evaluation,explicit assistance
:benchmark,Overcooked-AI
:event,COVID-19 pandemic
:community,scientific communities
:production,research articles
:resource,open-access corpora
:dataset,CORD-19
:tool,CORD-19 Topic Visualizer (CorToViz)
:method,CorToViz
:technology,up-to-date technologies
:task,clustering and temporal topic mining
:dashboard,interactive dashboard
:process,data preparation and results visualization
:task,textual document analysis
:framework,knowledgeable robot control
:context,smart human-robot interaction
:controller,visual servoing controller
:concept,high-level concepts
:skill,motor control skills
:graph,event knowledge graph
:task,robot manipulation task
:structure,executable behavior tree
:concept,semantic concepts
:method,robot trials in uncalibrated environment
:benefit,low reliance on human annotation
:benefit,ease of performing activities of daily living
:benefit,cognitive thinking for smart robot applications
:capability,fluent natural language text generation
:interest,industry applications of LLMs
:issue,hallucination generation
:framework,hierarchical framework for hallucination detection and mitigation
:technique,Chain of Natural Language Inference (CoNLI)
:technique,hallucination reduction via post-editing
:task,hallucination reduction
:achievement,state-of-the-art hallucination detection
:solution,effective hallucination management
:`model type`,prompted vision-language model
:phenomenon,low overfitting
:theory,PAC-Bayes bounds
:property,discrete nature of prompts
:model,ImageNet classifier
:technique,greedy search
:task,model selection
:issue,accumulated errors in multi-step reasoning
:technique,thought propagation
:concept,analogous problems
:metric,task performance improvement
:task,shortest-path reasoning
:task,creative writing
:task,LLM-agent planning
:field,AI-mediated communication
:application,interpersonal communication tools
:study,diary and interview study
:perception,users' perceptions of AIMC tools
:benefit,short-term communication support
:concern,long-term effects
:outcome,increased communication confidence
:limitation,AIMC tools limitations
:concern,inauthenticity
:`communication space`,formal relationships
:`communication space`,high-stakes communication
:challenge,resource-intensive process
:aspect,drug-target interaction identification
:method,computational prediction of DT interactions
:phenomenon,binding affinity
:task,accurate prediction
:field,drug target interaction prediction
:technique,protein language model integration
:technique,contact map information incorporation
:approach,proposed approaches
:insight,insights from this work
:resource,PGraphDTA code and data
:url,https://github.com/yijia-xiao/pgraphdta/
:task,financial sentiment analysis
:activity,investment decision-making
:`model family`,traditional NLP model
:attribute,parameter size and training dataset scope
:content,financial news
:attribute,succinctness and lack of context
:framework,retrieval-augmented LLM framework
:module,instruction-tuned LLM module
:function,predicting sentiment labels
:module,retrieval-augmentation module
:source,reliable external sources
:metric,accuracy and F1 score
:ability,inferential abilities
:task,Reasoning with Redundant Information Provided (RRIP)
:dataset,Grade School Math 8K (GSM-8K)
:model,Generative Pre-trained Transformer 3.5 (GPT-3.5)
:finding,performance decline on RRIP tasks
:limitation,handling redundant information
:recommendation,incorporate redundant information in training
:task,knowledge retrieval automation
:issue,hallucinations in language models
:interface,speech interface
:system,text-based knowledge retrieval system
:application,commercial search and chat-bot applications
:framework,keyword-based search framework
:task,context discovery
:`model family`,smaller large language model
:task,keyword generation
:task,providing answers
:framework,keyword augmented retrieval framework
:metric,inference time and cost
:interface,speech-based interface
:system,recommender system
:data,offline historical user behaviors
:technique,conversational techniques
:approach,prior conversational recommendation
:paradigm,CORE
:framework,uncertainty minimization
:platform,recommendation platform
:role,offline relevance score estimator
:role,online relevance score checker
:metric,unchecked relevance scores
:metric,expected certainty gain
:algorithm,online decision tree
:task,query decision-making
:domain,biomedical
:`model family`,zero-shot large language model
:`model type`,fine-tuned biomedical models
:process,pretraining on large text corpora
:factor,task
:tool,valuable tool for biomedical tasks
:concept,intelligence
:`type of knowledge`,instrumental knowledge
:`type of knowledge`,worldly knowledge
:concept,structured world models
:concept,recovery of knowledge
:concept,resource-rational tradeoff
:task,code generation from natural language
:task,complex software design
:approach,agile model-driven development (MDD)
:method,model-driven development
:`case study`,multi-agent simulation system
:domain,unmanned vehicle fleet
:representation,textual representation
:`modeling language`,Unified Model Language (UML)
:constraint,Object Constraints Language (OCL)
:task,code construction
:constraint,FIPA ontology
:aspect,communication semantics and protocols
:task,code auto-generation
:code,Java code
:framework,JADE
:code,Python code
:framework,PADE
:artifact,auto-generated code
:`modeling language artifact`,UML sequence diagram
:code,ontology-constrained model
:characteristic,intricate code
:need,advanced text classification algorithms
:phenomenon,growth of online services
:algorithm,biased text detection
:issue,biased and harmful language
:value,online community health
:country,South Korea
:algorithm,hate speech detection
:dataset,KoMultiText
:`model family`,BERT-based language model
:task,diverse classification tasks
:contribution,hate speech and bias mitigation
:resource,source codes and datasets
:url,https://github.com/dasol-choi/komultitext
:application,language agent
:task,formal theorem-proving
:method,CoPRA
:activity,selecting proof tactics
:system,proof framework
:issue,hallucinations in LLM queries
:benchmark,miniF2F
:benchmark,Compcert project tasks
:method,one-shot invocations of GPT-4
:`model family`,fine-tuned models on proof data
:process,next-token conditional distribution
:method,autoregressive sampling
:task,sequence continuation
:distribution,intractable posterior distribution
:task,infilling
:method,amortized Bayesian inference
:algorithm,Generative Flow Networks (GFlowNets)
:paradigm,distribution-matching
:method,maximum-likelihood training
:method,reward-maximizing policy optimization
:problem,latent variable modeling
:task,multi-step rationalization and tool use
:aspect,language quality
:challenge,weighting component reward models
:concept,reward model composition
:study,overoptimization in composite reward models
:approach,constrained reinforcement learning
:method,learning dynamic weights
:method,adaptive method using gradient-free optimization
:task,decision-making tasks
:application,autonomous agents
:framework,Language Agent Tree Search (LATS)
:capabilities,"planning, acting, and reasoning"
:technique,Monte Carlo Tree Search
:roles,"agents, value functions, and optimizers"
:mechanism,environment for external feedback
:limitation,limitations of existing techniques
:domains,"programming, HotpotQA, WebShop"
:performance,94.4% on HumanEval with GPT-4
:performance,75.9 average score on WebShop with GPT-3.5
:task,text retrieval
:process,language processing pipelines
:application,chat-based web search
:application,question answering systems
:technique,heuristics
:technique,hard negatives selection
:technique,additional supervision
:algorithm,Neural PG-Rank
:technique,end-to-end training
:concept,training objective alignment
:benchmark,text retrieval benchmarks
:document,retrieved documents
:technique,document compression
:technique,in-context integration
:compressor,extractive compressor
:compressor,abstractive compressor
:document,textual summaries
:compressor,compressor
:technique,selective augmentation
:task,language modeling task
:task,open domain question answering task
:metric,compression rate
:performance,minimal loss in performance
:technique,weight decay
:process,training deep networks
:concept,classical regularization
:`network type`,overparameterized deep network
:algorithm,stochastic gradient descent (SGD)
:`network type`,underparameterized large language model
:issue,sudden loss divergence
:`training technique`,bfloat16 mixed-precision training
:perspective,unifying perspective on weight decay
:`model family`,ResNet
:resource,code for weight decay study
:url,https://github.com/tml-epfl/why-weight-decay
:problem,performance decay on longer inputs
:capability,processing any input sequence length
:component,position encoding
:`encoding method`,Functional Relative Position Encoding with Progressive Interpolation (FIRE)
:capability,generalization to longer contexts
:`encoding method`,popular relative position encodings
:task,zero-shot language modeling and long text benchmarks
:field,neuroscience
:`anatomical structure`,higher visual cortex
:method,hand-selected stimuli
:process,mapping visual and semantic selectivity
:method,data-driven method
:process,generating image descriptions
:method,Semantic Captioning Using Brain Alignments (BrainScuba)
:model,contrastive vision-language model
:method,fine-grained voxel-level captioning
:process,text-conditioned image synthesis
:output,semantically coherent images
:investigation,exploratory investigations on 'person' representations
:`anatomical structure`,body-selective areas
:process,decoding text
:field,visual cortex research
:concept,early language models and information retrieval systems
:year,2018 and 2019
:concept,fairness
:year,2020 and 2021
:year,2022 and 2023
:concept,ethical considerations
:concept,AI systems
:problem,optimal control problem
:variable,prompt
:concept,magic word
:theory,control theory
:metric,k-epsilon controllability
:component,self-attention head
:model,Falcon-7B
:model,Falcon-40B
:dataset,Wikitext
:technique,large language model alignment circumvention
:method,attack suffixes
:model,public large language model
:model,private large language model
:technique,attack transferability
:concept,model approximation
:technique,local fine-tuning
:goal,decreasing model divergence
:concept,lexico-semantic neighborhood
:data,similar queries
:technique,attack suffix optimization
:method,attack prompts
:concept,human psychological aspects
:theory,appraisal and coping theory
:instrument,Stress and Coping Process Questionnaire (SCPQ)
:concept,emotion perception
:model,DaVinci-003
:finding,LLMs' responses similarity to humans
:finding,LLMs' responses difference from theory
:theory,appraisal theory
:finding,LLMs' magnitude of responses
:concept,instruction sensitivity
:literature,growing literature on psychological aspects of LLMs
:concept,psychological aspects of LLMs
:process,alignment with human feedback
:threat,jailbreak attacks
:activity,prompt manipulation
:technique,jailbreak prompt investigation
:outcome,understanding LLM limitations
:goal,LLM securement
:issue,scalability issues
:technique,manual prompt crafting
:issue,stealthiness problems
:technique,token-based prompt generation
:question,automatic stealthy jailbreak prompt generation
:goal,developing automatic stealthy jailbreak prompt generation
:method,AutoDan
:technique,hierarchical genetic algorithm
:attribute,attack strength
:technique,perplexity-based defense methods
:capability,function calling
:challenge,complicated API relations
:approach,Reverse Chain
:capability,external API usage
:capability,tool-use
:task,simple tasks
:rule,generic rule for controllable multiple function calling
:task,multiple function calling
:process,argument completion
:technique,embedding
:concept,complex information
:concept,interpretation
:method,visualization
:challenge,embedding interpretability
:task,querying and exploration
:task,enhancing concept activation vectors (CAVs)
:method,LLM interaction with embeddings
:task,communicating novel embedded entities
:potential,information potential of embeddings
:capability,interpretative power of large language models
:`model family`,controllable language model
:problem,off-policy reinforcement learning
:objective,maximizing reward and likelihood
:contribution,theoretical foundations of CLM
:concept,Pareto optimality
:method,reward dropout
:concept,policy improvement
:data,CLM benchmark datasets
:research,theoretical outcomes
:method,deductive proofs and empirical results
:task,instruction generation
:approach,in-context prompting
:`model family`,closed-source large language model
:task,complex instruction generation
:model,Ada-Instruct
:task,various reasoning tasks
:`model family`,various models
:task,test stimuli generation
:field,hardware design verification
:characteristic,labor-intensiveness
:framework,LLM4DV
:technique,prompt template
:technique,prompting improvements
:technique,constrained-random testing (CRT)
:scenario,straightforward DUT scenarios
:scenario,complex task settings
:task,reasoning on graphs
:outcome,inferences and hidden patterns identification
:task,automated reasoning with natural text
:problem,graph reasoning with large language models
:study,first comprehensive study of encoding graph-structured data as text for LLMs
:performance,LLM performance on graph reasoning tasks
:factor,"graph encoding method, graph task nature, graph structure"
:insight,strategies for encoding graphs as text
:encoder,correct choice of encoders
:task,graph reasoning tasks
:`performance increase`,4.8% to 61.8%
:challenge,computation during inference
:context,resource-constrained devices
:`activation function`,GELU
:trend,alternative activation functions
:`activation function`,SiLU
:`activation function`,ReLU
:aspect,convergence and performance
:step,memory-bound inference
:pattern,sparsity in ReLU-based LLMs
:process,generating new tokens
:strategy,reduce LLM inference computation
:approach,offline reinforcement learning
:data,previously collected data
:challenge,distribution shift
:setting,multi-agent reinforcement learning
:setting,online testing against nonstationary opponents
:concept,self-confirming equilibrium
:technique,Self-Confirming Loss
:challenge,online nonstationarity
:equilibrium,Nash Equilibrium
:equilibrium,Self-Confirming Equilibrium
:model,Self-Confirming Transformer
:capability,online adaptability
:`model family`,vanilla transformer
:task,medical coding
:dataset,MIMIC III
:dataset,MIMIC IV
:`loss function`,Segmented Harmonic Loss
:problem,class imbalance
:algorithm,new segmentation algorithm
:technique,embedding similarity
:problem,noisy data
:application,scientific discovery acceleration
:problem,challenging problems
:infrastructure,hardware infrastructure
:hardware,AI accelerator
:application,AI applications acceleration
:study,systematic study of LLMs on AI accelerators and GPUs
:benchmark,micro-benchmark using a core Transformer block
:evaluation,AI accelerators and GPUs performance
:`use case`,GenSLM
:analysis,models' performance analysis
:factor,"sequence lengths, scaling behavior, sparsity, gradient accumulation steps"
:field,natural sciences
:era,new era of scientific exploration
:sector,renewable energy
:initiative,DeepSpeed4Science
:goal,unlocking science mysteries
:technology,DeepSpeed
:technology,AI system technologies for scientific discoveries
:field,structural biology
:progress,early progress of DeepSpeed4Science
:component,attention head
:model,GPT-2 Small
:component,attention head 10.7
:function,copy suppression
:behavior,naive copying
:study,prior works
:component,negative heads
:mechanism,self-repair
:task,narrow task
:resource,interactive visualisations
:website,https://copy-suppression.streamlit.app/
:`model characteristic`,personalization and robustness
:issue,data heterogeneity
:`trade-off`,personalization vs robustness
:paradigm,fine-tuning large foundation models
:constraint,computational and communication limitations
:algorithm,FedAvg
:algorithm,FedSGD
:finding,federated-trained prompts robustness
:optimizer,adaptive optimizer
:approach,regularization and prompt interpolation
:`biological process`,brain activity
:task,speech tasks
:`brain region`,auditory cortex
:information,speech contextual information
:information,static semantic information
:information,acoustic-phonetic information
:representation,speech contextual representations
:`biological process`,speech perception
:requirement,high-quality labels
:challenge,processing structural data
:pipeline,LLM-GNN
:task,node annotation
:data,LLM annotations
:challenge,node selection for annotation
:heuristic,annotation quality heuristic
:dataset,Products
:task,audio tasks
:`model family`,large language model for audio tasks
:evaluation,quantitative evaluations
:model,LauraGPT
:capability,multimodal processing
:task,audio-related tasks
:feature,continuous and discrete audio features
:`model architecture`,decoder-only transformer-based language model
:task,multitask learning
:benchmark,audio processing benchmarks
:task,scientific document retrieval
:capability,handling complex queries
:limitation,existing evaluation datasets
:factor,annotation cost and effort
:task,DORIS-MAE
:dataset,DORIS-MAE benchmark dataset
:field,computer science
:framework,ANNO-GPT
:task,dataset annotation
:capability,sub-query test cases
:evaluation,retrieval methods performance
:need,better approaches
:resource,DORIS-MAE dataset and codebase
:URL,https://github.com/real-doris-mae/doris-mae-dataset
:capability,core capabilities
:technique,weight pruning
:technique,dense scaling
:capability,fact recall
:technique,scaling
:capability,in-context processing
:`task suite`,curated tasks for LLM capabilities
:concept,probabilistic model
:`training method`,maximum likelihood estimation
:metric,forward cross-entropy
:phenomenon,degeneration
:concept,suboptimality
:`optimization method`,earth mover distance optimization
:`model type`,auto-regressive language model
:metric,earth mover distance
:concept,modeling challenges
:concept,feasible upper bound
:evaluation,language model performance comparison
:concept,lightweight calibration method
:framework,Tree-GPT
:task,image comprehension
:system,modular LLM expert system
:module,image understanding module
:data,forest remote sensing images
:model,Segment Anything Model (SAM)
:technique,prompt generation
:task,tree structural parameter calculation
:concept,thought chain
:agent,LLM agent
:environment,local environment
:task,ecological parameter calculation
:task,forestry research tasks
:field,forestry research
:technique,multitask framework
:task,auxiliary task
:technique,information maximisation loss
:dataset,MSP-IMPROV
:model,proposed model for SER
:task,AI task automation
:`model family`,domain-specific model
:role,action executors
:`model family`,domain-specific API
:task,UI task automation
:model,metadata-free grounding model
:component,visual encoder and language decoder
:task,document understanding
:paradigm,pixel-to-sequence
:output,geometric coordinates
:algorithm,reinforcement learning based algorithm
:task,spatial decoding
:model,reinforced UI instruction grounding model
:role,generic UI task automation API
:task,multilingual NLP tasks
:issue,lack of training data
:strategy,knowledge transfer
:resource,parallel corpus
:resource,translation models
:module,bilingual task fitting
:technique,bilingual information alignment
:technique,label generation
:achievement,new SOTA
:task,different tasks
:framework,uncertainty-aware in-context learning
:capability,output enhancement or rejection
:method,human-defined uncertainty estimation
:task,correctness threshold setting
:concept,uncertainty information
:influence,model's behavior
:process,fine-tuning with calibration dataset
:task,knowledge evaluation
:response,cannot answer indication
:response,correct answer provision
:outcome,framework effectiveness
:capability,uncertainty recognition
:sector,financial sector
:task,integration with financial datasets
:context,financial contexts
:value,interoperability
:process,immediate integration
:process,end-to-end training and testing
:goal,specialization
:model,comprehensive model
:paradigm,open-source financial large language model (FinLLM)
:value,openness and reproducibility
:goal,alignment with human preferences
:method,Chat Vector
:concept,pre-existing knowledge and behaviors
:`training paradigm`,continual pre-train -> SFT -> RLHF
:`training paradigm`,continual pre-train + Chat Vector
:model,LaMDA2
:model,LaMDA2-Chat
:evaluation,toxicity
:evaluation,instruction following ability
:evaluation,multi-turn dialogue
:language,Korean
:study,extended experiments
:language,Simplified Chinese
:company,Huawei
:goal,fully connected intelligent world
:lab,Noah's Ark Lab
:technology,recommender systems and search engines
:`user base`,mobile phone users
:content,"apps, news feeds, songs, videos, books, themes, instant services"
:opportunity,big data and various scenarios
:technology,advanced recommendation technologies
:trend,technical trend of recommendation models
:model,"collaborative filtering, linear models, low rank models, neural networks, pre-trained language models"
:problem,hard problems in recommender systems
:talk,presentation
:challenge,important and interesting challenges in recommender systems
:community,RecSys community
:activity,rational decision-making
:ability,providing critiques
:model,critic model
:role,reliable evaluator
:role,source of supervised signals
:process,autonomous self-improvement
:framework,unified evaluation framework
:benchmark,CriticBench
:task,math problem-solving
:ability,self-critique
:baseline,Self-Check
:study,understanding critique abilities of LLMs
:process,academic writing
:`technological development`,"search engines, automatic translators, editing tools"
:role,efficiency in academic writing
:concept,authorship
:framework,PaperCard
:concept,diversified knowledge
:application,intelligent applications
:survey,comprehensive survey on knowledge graphs
:topic,knowledge graph evolution
:application,practical applications of knowledge graphs
:type,dynamic knowledge graph
:perspective,future directions of knowledge engineering
:`quantization scheme`,coarse-grained quantization
:`quantization scheme`,fine-grained quantization
:process,integer matrix multiplication
:`quantization approach`,dual grained quantization (DGQ)
:process,matrix multiplication
:algorithm,two-phase grid search algorithm
:schema,percentile clipping
:kernel,efficient CUTLASS kernel
:`model family`,multimodal vision-language model
:capability,fused understanding of images and language
:task,UI tasks
:method,generating paired text-image training data
:domain,UI domain
:method,pixel-based methods
:dataset,UI screenshots dataset
:method,generating UI training data
:dataset,335k conversational examples paired with UIs
:task,fine-tuning VLM for UI tasks
:model,conversational vision-language model
:task,UI element detection
:task,multi-step UI navigation and planning
:concept,operating system
:ecosystem,interconnected ecosystem of generative AI models
:software,traditional software applications
:`model family`,large generative model
:interface,central interface between users and computers
:`interaction type`,natural language conversations
:`user action`,explicit commands or complex navigation
:`user action`,user's prompts
:output,contextual and meaningful responses
:`user aspect`,individual preferences
:challenge,"privacy, security, trustability, and ethical use"
:task,stock price prediction
:task,early detection of market-influencing changes
:information,salient facts and events
:`data structure`,tuples with entities
:task,market change summarization
:task,abstract summarization
:task,relationship analysis
:source,The Economist
:system,comprehensive decision-making system
:audience,financial analysts and investors
:`model family`,generative large language model
:industry,web search market
:`revenue model`,advertising
:`revenue model`,subscription model
:`advertising technique`,native advertising
:`search engine results page (SERP)`,lists SERPs
:`search engine results page (SERP)`,text SERPs
:issue,distinguishing advertising from organic results
:study,pilot study on large language models
:capability,blending ads with organic search results
:task,subtle ad framing
:task,ad integration into related topics
:resource,instructional videos
:concept,multimodal representations
:data,video-subtitle pairs
:concept,noisy supervision
:data,web video training data
:concept,sub-optimal
:task,video description generation
:data,ASR narrations
:method,prompting method
:technique,timestamp generation
:task,video caption alignment
:dataset,HowTo100M
:dataset,HowToCaption
:task,text-video-audio tasks
:combination,language model and knowledge graph
:task,commonsense question answering
:technique,knowledge graph-based explanation
:attribute,faithfulness
:metric,graph consistency
:metric,graph fidelity
:method,Consistent GNN (CGNN)
:analysis,prediction divergence
:phenomenon,prediction inconsistency
:value,explicit evaluation
:data,large-scale multilingual texts
:evaluation,LLM capabilities assessment
:need,assessing LLM capabilities beyond English
:task,LLM evaluation
:challenge,lack of suitable datasets
:benchmark,IndoMLLU
:data,educational questions
:model,BLOOMZ
:model,FALCON
:subject,Indonesian language and culture
:`data type`,graph-structured data
:research,benchmarking experiments
:task,graph prediction tasks
:capability,processing graph data
:technique,prompt formatting
:capability,interpreting graph structures
:field,graph analytics
:methodology,fine-tuning and evaluating large language models
:task,specialized monetization tasks
:goal,balance general language proficiency with domain-specific skills
:component,blending in-domain and general-purpose data
:dimension,"reliability, consistency, and business impact"
:analysis,model size and continual training influence
:tool,45 tailored questions and scoring guidelines
:community,business and research
:insight,adapting LLMs for specialized contexts
:value,transparency and collaboration
:industry,semiconductor chip design
:value,confidentiality
:task,knowledge graph construction
:domain,hardware-design
:scheme,oracle-checker
:problem,distillation of domain expert's background knowledge
:specification,RISC-V Unprivileged ISA
:`model family`,neural machine translation model
:dataset,existing code translation datasets
:benchmark,CodeTransOcean
:feature,variety of programming languages
:dataset,MultiLingualTrans
:dataset,NicheTrans
:dataset,LLMTrans
:dataset,DLTrans
:approach,multilingual modeling approaches for code translation
:metric,Debugging Success Rate@k
:resource,CodeTransOcean datasets and code
:url,https://github.com/weixiangyan/codetransocean
:strategy,prompting strategy
:outcome,impressive strength
:research,CoT prompting research
:resource,systematic summary and comprehensive guide
:survey,CoT prompting survey
:analysis,systematic and comprehensive analysis
:guidance,application of CoT prompting
:challenge,CoT prompting challenges
:direction,future directions for CoT prompting
:resource,overall reference
:`language feature`,Chinese character
:task,Chinese language understanding
:`language feature`,Pinyin
:`error type`,SSP errors
:phenomenon,same or similar pronunciation words
:model,PMBert
:task,error correction
:task,fault tolerance
:`model family`,phonetic enhanced Chinese language model
:dataset,noise-added dataset
:dataset,public error-correction dataset
:task,test generation
:model,CodexLLaMA
:`programming paradigm`,OpenACC
:task,compiler implementation validation
:technique,retrieval-augmented generation (RAG)
:technique,code template
:technique,one-shot example
:task,script generation
:agent,AI virtual assistants
:task,reasoning about subsequent steps
:method,generative script learning
:requirement,well-structured preceding steps
:benchmark,MultiScript
:task,multimodal script learning
:task,multimodal script generation
:input,target task name and video
:task,subsequent step prediction
:source,WikiHow
:framework,knowledge-guided multimodal generative frameworks
:robot,general-purpose mobile robot
:task,tasks without exact human instructions
:capability,commonsense world knowledge and reasoning-based planning
:capability,vision-language semantics interpretation
:concept,latent compositional semantic embeddings z*
:capability,queryable spatio-semantic memories
:property,optimality and discoverability
:property,separability of semantics
:method,iterative optimization by gradient descent
:model,SBERT
:model,dense vision-language model
:dataset,COCO-Stuff dataset
:benchmark,open-vocabulary segmentation performance
:`research area`,categorization and mitigation of hallucination
:orientation,factual mirage
:orientation,silver lining
:dataset,hallucination elicitation (HILT)
:index,hallucination vulnerability index (HVI)
:strategy,mitigation of hallucinations
:category,hallucination types
:knowledge,stored in LLM parameters
:characteristic,incomplete and difficult to update
:method,retrieval-based methods
:knowledge,non-parametric world knowledge
:knowledge,retrieved knowledge
:effect,negative impact on responses
:concept,self-knowledge
:method,self-knowledge guided retrieval augmentation (SKR)
:method,chain-of-thought and retrieval-based methods
:task,few-shot question answering
:outcome,satisfactory results
:outcome,best results
:framework,MinPrompt
:algorithm,approximate graph algorithm
:technique,unsupervised question generation
:structure,graph structure
:data,raw text
:model,final model
:data,selected sentences
:metric,F-1 score
:task,zero-shot image recognition
:framework,Open-VCLIP++
:concept,spatial-temporal relationships
:technique,interpolated weight optimization
:output,fine-grained video descriptions
:concept,video features
:dataset,action recognition datasets
:metric,zero-shot accuracy
:dataset,MSR-VTT
:resource,Open-VCLIP++ code
:challenge,deployment on resource-constrained hardware
:method,LLM pruning
:challenge,high training cost and data collection
:outcome,performance decline
:paradigm,Compresso
:method,structural pruning of LLMs
:algorithm,resource-efficient pruning algorithm
:technique,$l_0$ regularization
:technique,collaborative prompt
:algorithm,pruning algorithm
:model,LLaMA-5.4B
:paradigm,computer-aided translation
:paradigm,interactive machine translation
:system,Synslator
:technique,online learning
:`model family`,neural translation model
:evaluation,Synslator's online learning effectiveness
:evaluation,post-editing efficiency increase
:resource,tutorial video for Synslator
:url,https://youtu.be/k0vrsb2ltt8
:data,labeled and unlabeled data
:task,new tasks
:technique,zero-shot relation extraction
:technique,Summarize-and-Ask prompting
:task,extracting overlapping relations
:challenge,none-of-the-above relation
:method,extrapolating positional embedding
:method,recurrence
:method,selective retrieval
:challenge,long-text understanding
:approach,interactive agent
:method,MemWalker
:task,long-text question answering
:skill,complex reasoning and intricate knowledge utilization
:framework,self-convince
:component,normal cot
:component,convincer
:component,answerer
:technique,few-shot chain-of-thought prompting
:dataset,miscellaneous problems datasets
:research,integrating pre-trained language models with tailored prompts and iterative refinement
:game,Resistance Avalon
:skill,strategic social deduction
:benchmark,AvalonBench
:component,"game environment, rule-based bots, React-style LLM agents"
:bot,good-role bot
:goal,advanced LLMs and agent frameworks development
:field,fake news research
:goal,curbing fake news spread
:task,fake news handling
:technique,prompt method
:content,fake news samples
:feature,fake news features
:technique,reason-aware prompt method
:resource,extra information
:task,vertical thinking tasks
:task,lateral thinking puzzles
:benchmark,Brainteaser
:procedure,three-step procedure for benchmark creation
:data,"1,100 lateral thinking puzzles"
:task,assessing consistency of lateral reasoning
:experiment,evaluation of instruction- and commonsense language models
:`performance gap`,human vs model performance on lateral thinking
:`performance gap`,consistency across adversarial formats
:resource,code and data for Brainteaser
:goal,developing and evaluating lateral thinking models
:task,video-csr
:dataset,video-csr dataset
:content,YouTube video clips
:annotation,captions and summaries
:`model family`,visual-language model
:task,caption and summary generation
:task,retrieval tasks
:task,summary-based retrieval
:task,video identification
:task,video summarization
:model,foundation model for video-csr
:era,age of large language models and multi-modal tasks
:task,in-context information extraction
:`model type`,supervised expert models
:problem,underspecified task description
:framework,guideline learning
:process,automatic guideline synthesis
:process,guideline retrieval
:method,self-consistency-based active learning
:task,event extraction
:technique,dialogue-guided chain-of-thought
:task,complex reasoning questions
:dataset,arithmetic reasoning datasets
:performance,improvements
:resource,computation and memory resources
:technique,quantisation
:problem,high resource costs of LLM inference
:technique,8-bit quantisation
:concept,numerical scaling offsets
:technique,block quantisations
:technique,6-bit quantisation
:metric,arithmetic and memory density
:technique,sub-8-bit quantisation
:technique,4-bit quantisation
:method,traditional information extraction
:characteristic,coarse-grained extraction
:dataset,fine-grained IE benchmark dataset
:task,generalizing to unseen information types
:characteristic,adaptability to new task forms
:finding,performance not solely dictated by model scale
:factor,"architecture, data diversity, and learning techniques"
:phenomenon,AI-generated text proliferation
:issue,misuse of AI-generated text
:detector,high-performing detectors
:detector,OpenAI detector
:detector,Stanford DetectGPT
:`research question`,reliability of AI text detectors
:approach,universal evasive prompt
:`prompt type`,soft prompt
:text,human-like text
:`prompt type`,evasive soft prompt
:concept,transferability of soft prompts
:experiment,evaluation of evasive soft prompts
:approach,training-free detection
:task,LLM-generated code detection
:approach,zero-shot detection
:detector,existing text detectors
:task,code detection
:method,DetectGPT modification
:model,surrogate white-box model
:dataset,CodeContest and Apps
:property,robustness and generalization
:model,PolyCoder-160M
:task,universal code detection
:resource,code detection GitHub repository
:URL,https://github.com/xianjun-yang/code_detection.git
:technique,contextual word embeddings
:task,natural language processing tasks
:challenge,interpreting embeddings
:aspect,syntax and semantics
:approach,disentangled representation learning
:technique,disentangling specific aspects
:`model component`,layers of PLM
:aspect,semantic sense
:technique,binary mask application
:technique,disentangled embeddings
:model,cased BERT base
:technique,layer-wise information leveraging
:domain,visual world
:model,UReader
:capability,shallow text recognition
:task,visually-situated language understanding tasks
:task,auxiliary tasks
:module,shape-adaptive cropping module
:architecture,encoder-decoder architecture of MLLM
:achievement,state-of-the-art OCR-free performance
:resource,codes and instruction-tuning datasets
:capability,producing fluent and cogent content
:`dual aspect`,productivity opportunities and societal risks
:`ai system`,trustworthy ai system
:task,distinguishing machine-generated vs human-authored content
:detector,DetectGPT
:performance,commendable performance
:limitation,intensive computational costs
:concept,conditional probability curvature
:discrepancy,word choices between LLMs and humans
:detector,Fast-DetectGPT
:method,efficient sampling step
:process,detection process
:domain,algorithmic hiring
:task,resume matching
:issue,bias
:study,Bertrand & Mullainathan (2003)
:issue,hiring bias
:experiment,resume response rate comparison
:experiment,LLM bias evaluation
:task,resume summarization
:technique,contrastive input decoding
:method,data-centric
:task,referring object detection
:input,user instructions
:concept,referring expressions
:dataset,InDet
:`data type`,"images, bounding boxes, instructions"
:method,InstructDet
:potential,dataset expansion
:model,referring object detection model
:concept,empathetic dialogue
:goal,harmonious social relationships
:goal,helpful AI
:`model family`,small-scale language model
:task,empathetic dialogue generation
:method,semantically similar in-context learning
:method,two-stage interactive generation
:method,combination with knowledge base
:method,proposed improvement methods
:task,simulating human evaluators
:challenge,Abstraction and Reasoning Corpus (ARC)
:system,multiple expert agents
:task,novel tasks
:input,image
:`abstraction space`,text-based
:capability,associative power
:`input-output relationship`,derived by LLM
:action,working program
:example,Voyager / Ghost in Minecraft
:feedback,iterative environmental feedback
:performance,50 solves out of 111 problems
:`abstraction space`,grid
:`abstraction space`,object
:`abstraction space`,pixel
:improvement,more abstraction spaces and learnable actions
:resource,task-relevant documents
:method,retrieval from external knowledge base
:task,obtaining effective documents
:method,generation by large language models
:framework,iterative retrieval-generation collaborative
:task,leveraging knowledge
:task,correct reasoning path
:dataset,single-hop QA
:dataset,multi-hop QA
:capability,reasoning ability
:framework,Toolink
:method,chain-of-solving (CoS)
:dataset,CoS-GPT
:task,tool-using
:model,LLaMA-CoS
:task,unseen tasks
:toolkit,general toolkits
:resource,codes and data
:belief,mastery in time understanding and reasoning
:`research area`,temporal sensitivity of LLMs
:dataset,Multiple Sensitive Factors Time QA (MeNaTQA)
:ability,time comprehension and reasoning
:factor,temporal factors
:parameter,parameter sizes
:`model family`,temporal reasoning model
:bias,temporal biases
:strategy,specific prompts and external tools
:`model family`,recurrent neural network language model
:capability,recognizing unweighted formal languages
:concept,probability distributions over strings
:`model family`,probabilistic finite-state automata
:`model family`,finite-state language model
:`model family`,deterministic finite-state language model
:concept,classes of probability distributions
:task,formal language tasks
:data,pre-training data distribution
:concept,structured semantics understanding
:benchmark,ConvRE
:data,17 relations and 1240 triples
:task,RE2Text
:task,Text2RE
:`evaluation protocol`,variants to test text and few-shot example text
:concept,shortcut learning
:task,machine-generated text detection
:`research question`,detector performance on unseen generators
:activity,collecting generation data
:`model family`,medium-size large language model
:attribute,size
:strategy,ensemble of training data
:tool,robust detectors
:issue,deployment difficulty
:technique,network pruning
:strategy,uniform pruning
:strategy,non-uniform layerwise sparsity
:analysis,token feature distribution in LLMs
:phenomenon,outliers in feature dimensions
:methodology,Outlier Weighed Layerwise Sparsity (OWL)
:evaluation,empirical evaluation of OWL
:`model family`,LLaMA-v1 and OPT
:methodology,state-of-the-art pruning methods
:`knowledge base`,conventional knowledge base
:content,LLM-generated content
:issue,inaccuracies
:benchmark,Pinocchio
:`dataset feature`,diverse factual questions
:bottleneck,lack of factual knowledge in LLMs
:goal,trustworthy artificial intelligence
:resource,Pinocchio dataset and codes
:era,21st century technological innovation
:paradigm,Web3
:activity,building decentralized applications
:gap,knowledge and skill gap
:community,developers
:model,MazzaMAGPT
:task,smart contract code generation
:goal,developer productivity
:metric,functional correctness
:aspect,limitations and broader impacts
:tool,natural language generation tools
:risk,malicious applications
:challenge,deception and misinformation
:initiative,technological innovations
:risk,LLM-associated risks
:initiative,regulatory reforms
:initiative,AI literacy initiatives
:community,fact-checkers
:community,news organizations
:community,research and policy communities
:goal,alignment with human and societal values
:issue,length bias
:outcome,favoring longer outputs
:technique,product-of-experts (PoE)
:expert,main expert
:goal,understanding human intents
:expert,biased expert
:technique,perturbation
:capability,extrapolation
:`model component`,rotary position embedding
:approach,modifying rotary position embedding
:technique,base value adjustment
:observation,fine-tuning with different base values
:framework,scaling laws of rope-based extrapolation
:concept,critical dimension for extrapolation
:issue,rope-based extrapolation issue
:achievement,extrapolation up to 1 million context length
:`model version`,7B and 13B
:component,internal mechanisms
:mechanism,feed-forward networks
:mechanism,multi-head self-attention
:study,probing LLMs from a human behavioral perspective
:technique,eye-tracking measures
:activity,reading patterns
:pattern,prediction pattern
:capability,memorization and linguistic knowledge encoding
:component,multiple heads
:mechanism,gate mechanisms
:process,information flow
:`model type`,character-based language model
:community,speech community
:unit,discrete time units
:`model type`,generative spoken language model
:`model type`,word-based language model
:technique,continuous-valued audio embeddings
:technique,lexical embedding function
:milestone,first generative language model with word-size continuous embeddings
:`model type`,discrete unit generative spoken language model
:attribute,memory efficiency
:domain,mental health
:application,MindfulDiary
:professionals,mental health professionals
:approach,state-based approach
:study,four-week field study
:finding,consistent enrichment of daily records
:professionals,psychiatrists
:discussion,implications of leveraging LLMs in mental health
:approach,LAN-GRASP
:task,semantic grasping
:capability,deeper understanding of objects
:task,object part identification for grasping
:`model family`,vision language model
:task,grasp proposal generation
:method,zero-shot grasp method
:evaluation,real-world experiments
:data,custom object data set
:survey,participant survey on object grasping
:result,survey results
:approach,conventional grasping planner
:approach,recent semantic grasping approach
:characteristic,cross-source heterogeneity
:task,health condition recognition
:model,ChatRadio-Valuer
:task,disease diagnosis
:dataset,clinical dataset
:goal,model generalization performance
:goal,clinical AI applications
:field,radiology
:task,claim verification
:challenge,reliance on human-annotated data
:`model feature`,comprehensive explanations
:approach,First-Order-Logic-guided Knowledge-grounded Reasoning (FOLK)
:logic,First-Order Logic (FOL)
:process,FOL-guided reasoning
:data,knowledge-grounded question-and-answer pairs
:`model feature`,explanatory capability
:experiment,FOLK performance evaluation
:profession,legal practitioners
:challenge,growing size of legal opinions
:task,predicting rhetorical roles
:`model architecture`,novel
:feature,sentence position information
:corpus,LegalEval@SemEval2023
:advantage,lower computational costs
:`model architecture`,hierarchical
:context,global-context
:context,local-context
:capability,imitating personas
:persona,generic persona
:`demographic group`,demographic groups
:persona,specific persona
:individual,popular names
:adoption,personas in dialogue systems
:`user experience`,engaging and approachable dialogue
:bias,persona bias
:behavior,harmful behaviors
:category,harmful expression and agreement
:framework,evaluation framework for persona biases
:aspect,"offensiveness, toxic continuation, regard, stereotype agreement, toxic agreement"
:dataset,UniversalPersona
:investigation,persona biases
:study,benchmarking study on dialogue systems
:issue,significant persona biases
:use,personas in dialogue agents
:goal,safe application
:output,narratives
:issue,repetitive and illogical stories
:framework,novel framework for visual storytelling
:`model component`,visual prefix
:input,image sequence
:technique,question-answer planning
:model,blueprint-based models
:community,AI researchers
:issue,quality assurance
:model,GPT-based sentiment analysis model
:task,AI quality analysis
:analysis,quality analysis
:issue,data adequacy
:technique,content-based approach
:data,adversarial review comments
:technique,surprise adequacy (SA)
:experiment,Amazon.com review data experiment
:data,Amazon.com review data
:data,adversarial textual data
:technique,task-adaptive tokenization
:task,long-form generation in mental health
:field,cognitive science
:method,variable segmentation sampling
:technique,vocabulary merging protocol
:task,psychological question-answering
:metric,generation performance
:benchmark,NOPE
:data,synthetic negative pronoun data
:`model family`,state-of-the-art vision-language model
:answer,negative pronoun
:factor,lexically diverse visual questions
:factor,large scope question types
:factor,scene-relevant objects
:process,model alignment
:issue,complex training setup
:`feedback type`,single-dimensional feedback
:method,SteerLM
:`attribute set`,multi-dimensional attributes
:attribute,humor
:concept,steerable AI
:evaluation,human and automatic evaluators
:phenomenon,neural collapse
:component,last layer representations and classifier weights
:activity,developing new techniques
:study,neural collapse studies
:condition,small number of classes relative to feature space dimension
:condition,large number of classes relative to feature space dimension
:phenomenon,generalized neural collapse
:study,empirical study
:study,theoretical study
:model,unconstrained feature model with spherical constraint
:application,language models
:application,retrieval systems
:application,face recognition applications
:process,selecting relevant artifacts
:`retrieval type`,homogeneous and relaxed retrieval
:characteristic,top-k selection
:`retrieval type`,heterogeneous and strict retrieval
:task,preparing prompts for LLMs
:task,targeted tasks
:technique,dense retrieval
:concept,pretrained embeddings
:concept,task-specific similarity
:technique,adapted dense retrieval
:benchmark,state-of-the-art general-purpose embeddings-based baseline
:field,Natural Language Processing (NLP) applications
:field,language variety studies
:paradigm,world englishes
:corpus,CCAE
:`language variety`,Chinese-based Asian English varieties
:data,340 million tokens in 448 thousand web documents
:field,Asian Englishes research
:task,variety-specific language modeling and downstream tasks
:field,NLP-based world englishes studies
:value,practical value
:resource,publicly accessible corpus
:application,linguistic context applications
:model,mBERT
:task,next token prediction
:model,XLM-R
:factor,resource availability
:`performance metric`,model accuracy
:factor,word order
:factor,language family
:factor,script type
:relationship,resource availability and language characteristics
:`research area`,language-specific characteristics
:analysis,statistical inference analysis
:`performance metric`,model performance features
:era,digital age
:field,customer service
:model,LangChain
:task,automating customer service
:technique,traditional customer support
:status,obsolescence
:framework,Sahaay
:capability,scaling across industries
:technique,web scraping
:model,FLAN T5 XXL
:task,knowledge retrieval
:platform,customer service platforms
:institution,educational institution
:relationship,customer-company relationship
:approach,compartmentalized approach
:task,linguistic tasks
:approach,"general purpose, task-agnostic approach"
:concept,compartmentalized notion of language tasks
:challenge,evaluation and analysis
:need,trustworthy and reliable systems
:concept,tasks and model evaluation in NLP
:value,trustworthiness
:view,holistic view on language
:concept,model's functional capacity
:recommendation,multi-faceted evaluation protocols
:ability,cognitive powers
:task,multi-nested boolean logic
:method,curriculum logical reasoning (CLR)
:task,longer-hop logic
:ability,boolean logic
:task,general logical tasks
:capability,solving complex tasks
:process,task-specific training or fine-tuning
:capability,autoregressive generation
:capability,generalization abilities
:framework,novel framework for hierarchical template-content structure
:task,complex natural language tasks
:capability,complex reasoning abilities
:experiment,practical model behavior experiments
:range,tasks
:task,human-centric tasks
:task,brainstorming
:task,email writing
:attribute,generality
:model,Auto-J
:data,user queries and LLM-generated responses
:evaluation,diverse evaluation protocols
:testbed,new testbed
:entity,competitors
:analysis,detailed analysis and case studies
:resource,variety of resources
:url,https://github.com/gair-nlp/auto-j
:ability,multiple abilities
:`model family`,proprietary large language model
:study,data composition study
:study,scaling study
:factor,data amounts
:ability,general human-aligning abilities
:factor,data samples
:conflict,abilities conflict
:factor,high data amounts
:strategy,dual-stage mixed fine-tuning
:goal,learning multiple abilities
:strategy,sequential learning
:application,diverse applications
:integration,LLMs with graph-structured data
:interest,keen interest
:integration,LLMs for graph learning
:integration,graphs advancing LLMs
:cognition,human cognition
:structure,graphs
:task,math reasoning
:dataset,AuGSM8K
:`model family`,MuggleMath
:relationship,scaling relationship
:task,out-of-domain math reasoning
:benchmark,Math
:resource,codes and AuGSM8K
:era,automation
:debate,"LLM development, deployment, and use"
:movement,AI ethics
:phenomenon,regulatory capture
:field,AI regulation and governance
:concept,risk and harm regulation
:field,regulation studies
:concept,risk and uncertainty management
:task,risk assessment
:`research space`,Regulation and NLP (RegNLP)
:goal,connecting scientific knowledge to regulatory processes
:platform,STREAM
:challenge,aligning AI with human morals
:resource,ethics datasets and knowledge bases
:concept,cultural and group variations in moral judgments
:concept,dynamic evolution of moral judgments
:concept,moral capabilities of AI models
:resource,ethical scenarios collection
:data,moral judgment data
:concept,moral preferences and performances
:resource,STREAM structure and construction
:concept,potential applications
:concept,future prospects
:field,quantitative investment
:profession,traders
:challenge,semantic information utilization
:process,quantitative model integration
:challenge,latent information alignment
:component,Local-Global (LG) model
:component,Self-Correlated Reinforcement Learning (SCRL)
:metric,rank information coefficient and returns
:market,China A-share market
:`regulatory body`,regulatory bodies
:information,non-financial information
:topic,ESG issues
:format,non-structured and multi-modal documentation
:data,"concise, informative, and actionable data"
:technique,in-context learning and RAG paradigm
:information,ESG-related information
:document,sustainability reports
:representation,graph-based representation
:analysis,"statistical, similarity, and correlation analyses"
:company,companies
:action,ESG-related actions
:characteristic,disclosure similarities
:metric,ESG scores
:information,companies' disclosures
:issue,unreliable hallucinations
:task,knowledge-aware language model attribution
:`model family`,attributed large language model
:setting,conscious incompetence
:metric,automatic evaluation
:dataset,BioKALMA
:solution,baseline
:concept,citation generation
:task,text quality evaluation
:work,LLM evaluation by Chiang and Lee
:work,G-Eval by Liu et al.
:technique,auto chain-of-thought (COT)
:metric,human rating alignment
:method,numeric rating output
:evaluation,LLM text quality evaluation
:method,LLM rating explanation
:metric,human rating correlation
:benchmark,meta-evaluation datasets
:concept,meta learning
:issue,compute/memory costs
:issue,training instability
:feature,efficient distributed training support
:system,SAMA
:concept,scalable meta learning
:technique,implicit differentiation algorithms
:concept,adaptive optimizers
:task,second-order gradient computation
:technique,efficient distributed training
:benchmark,large-scale meta learning benchmarks
:metric,throughput
:optimization,SAMA-based data optimization
:domain,language and vision domains
:issue,case overload
:statistic,pending cases
:discrepancy,case-to-professional ratio
:technology,AI technology
:domain,legal procedures
:profession,legal professionals
:task,argument generation
:method,best performing method
:metric,overlap with gold standard annotations
:task,solving probability problems
:institution,Rey Juan Carlos University
:profession,statistics professors
:benchmark,average student performance
:attribute,multilingual capability
:task,basic numerical operations
:technique,R scripting
:limitation,numerical operation limitations
:concept,probability concepts
:role,learning assistant
:task,freetext query response
:domain,healthcare applications
:comparison,PLMs vs LLMs
:comparison,LLMs with each other
:aspect,healthcare training data
:concern,"fairness, accountability, transparency, ethics"
:survey,this survey
:perspective,computer science and healthcare specialty
:community,computer science community
:resource,open source resources
:`paradigm shift`,from PLMs to LLMs
:approach,generative AI approaches
:approach,data-driven methods
:aspect,structural aspects of reasoning capacity
:task,maintaining consistency in reasoning chains
:technique,planning tokens
:task,guiding reasoning steps
:`model component`,trainable parameters
:dataset,math word problem datasets
:technique,chain-of-thought fine-tuning
:baseline,chain-of-thought fine-tuning baselines
:technique,neural network fusion
:goal,combining network capabilities
:`model architecture`,traditional neural networks
:technique,transformer network fusion
:concept,layer alignment
:goal,generalization to arbitrary architectures
:`model component`,transformer components
:technique,heterogeneous fusion
:task,image classification and NLP tasks
:benchmark,vanilla fusion and individual models
:concept,soft alignment
:paradigm,model fusion and recombination
:domain,code intelligence
:model,CodeT5+
:dataset,MBPP
:capability,program testing
:technique,generated test cases
:metric,code pass rates
:characteristic,lengthy prompts
:method,LLMLingua
:goal,accelerating model inference and reducing cost
:component,budget controller
:characteristic,semantic integrity
:algorithm,token-level iterative compression
:characteristic,interdependence between compressed contents
:goal,distribution alignment
:dataset,BBH
:dataset,ShareGPT
:dataset,arXiv-March23
:resource,LLMLingua code
:URL,https://aka.ms/llmlingua
:task,generative tasks in language
:task,image and video generation
:component,visual tokenizer
:task,visual generation with LLMs
:tokenizer,MagVit-V2
:task,token generation for visual content
:concept,common token vocabulary
:task,image generation
:task,video generation
:tokenizer,previous top-performing video tokenizer
:task,video compression
:standard,next-generation video codec (VCC)
:task,action recognition
:skill,advanced reasoning
:method,static benchmarks
:environment,evaluation environment
:skill,strategic reasoning
:`simulation environment`,Aucarena
:setting,auctions
:skill,resource and risk management
:role,bidding agents
:skill,auction engagement
:skill,adaptiveness
:benchmark,heuristic baselines
:process,testing and refining agent architectures
:task,learning new tasks
:technique,explanation technique
:requirement,white-box access
:framework,LLM-based explanation framework
:goal,model explanation effectiveness
:technique,prompting strategies
:experiment,LLM-generated explanations performance
:benchmark,state-of-the-art post hoc explainers
:technique,in-context learning examples
:field,Explainable Artificial Intelligence (XAI)
:benchmark,SuperClue-Safety (SC-Safety)
:`model family`,Chinese large language model
:methodology,multi-round adversarial benchmark
:methodology,existing safety assessment methods
:`model type`,closed-source model
:`model origin`,models released from China
:`model size`,smaller models with 6B-13B parameters
:value,collaborative safety efforts
:resource,https://www.cluebenchmarks.com
:task,terminology translation
:method,terminology constraints injection
:goal,terminology correctness
:approach,translate-then-refine
:event,WMT 2023 terminology translation task
:characteristic,domain-independence and minimal manual effort
:method,pseudo-terminology annotation
:model,terminology-aware model
:method,post-processing
:process,alignment process
:issue,terminology constraint violation
:method,negative constraint re-decoding
:goal,terminology incorporation
:process,large language model refinement
:goal,terminology recall
:context,daily life
:concept,theoretical framework for generalization and uncertainty
:problem,uncertainty estimation
:method,ad-hoc and task-dependent methods
:approach,natural language approach
:concept,bias-variance-covariance decomposition
:metric,kernel scores and entropy
:concept,estimator
:quality,unbiased and consistent
:phenomenon,mode collapse of minority groups
:metric,variance
:baseline,existing baselines
:`information type`,diverse information
:`data type`,images and audio
:practice,graph2text conversion
:model,GraphLLM
:`model family`,graph learning model
:metric,accuracy and context reduction
:application,conversational assistants
:activity,daily tasks
:`user experience`,interaction with conversational assistants
:gap,real-world interaction exploration
:task,cooking
:study,user experiences with LLM-based conversational assistants
:`model-based system`,Mango Mango
:`user experience`,successful experiences with Mango Mango
:feature,extensive information and customization
:`user experience`,unsatisfactory experiences with Mango Mango
:expectation,adaptive oral conversation and suggestive responses
:perception,LLM-based conversational assistant as partner
:proposal,design considerations for LLM-based conversational assistants
:paradigm,learning with human feedback
:quality,high-quality text
:task,abstractive summarization
:`form of feedback`,human edits
:technique,Sequence Alignment (Un)Likelihood Training (SALT)
:technique,imitation edits
:domain,medical domain summarization
:quality,summary quality
:technique,RLHF method (DPO)
:factor,input presentation
:input,underspecified input
:strategy,adding visually grounded information
:`model performance`,LVLMS performance
:framework,"Rephrase, Augment and Reason (REPARÉ)"
:task,question modification
:technique,unsupervised scoring function
:technique,oracle question candidate selection
:outcome,VQA accuracy gain
:analysis,extensive analysis
:outcome,effective utilization of vision-language interaction
:`model family`,audio-visual large language model
:task,understanding general video inputs
:framework,FAVOR
:capability,simultaneous audio-visual perception
:structure,causal Q-former
:capability,capturing causal relations
:benchmark,AVEB
:capability,audio-visual co-reasoning
:task,video question-answering
:capability,video comprehension and reasoning
:resource,interactive demo of FAVOR
:URL,https://github.com/briansidp/audiovisualllm.git
:resource,training code and model checkpoints of FAVOR
:status,upcoming release
:technology,deep generative model
:model,denoising diffusion probabilistic model
:concept,flexible model architecture
:model,quantum generative model
:task,learning classical and quantum data
:model,quantum denoising diffusion probabilistic model
:task,generative learning of quantum data
:concept,circuit layers
:concept,intermediate training tasks
:task,learning correlated quantum noise model
:task,learning topological structure of quantum data
:mechanism,approximate attention
:mechanism,HyperAttention
:work,recent work
:concept,quadratic time complexity
:parameter,hardness parameters
:concept,linear time sampling algorithm
:concept,modular design
:technique,Locality Sensitive Hashing (LSH)
:model,ChatGLM2
:`context length`,131k
:`model family`,vision-and-language model
:task,visual commonsense understanding
:task,visual commonsense reasoning
:task,visual commonsense inference
:issue,passive perception
:method,VICoR
:task,visual recognition and understanding
:benchmark,VCR benchmark datasets
:`model family`,large causal language model
:concept,explanation of capabilities
:process,training process
:perspective,meta-learning view
:process,inner optimization
:characteristic,norms of learned token representations
:data,real-world data
:limitation,dependency on high-quality human annotations
:technique,supervised fine-tuning and reinforcement learning from human feedback
:approach,SALMON
:goal,minimal human supervision alignment
:model,principle-following reward model
:data,synthetic preference data
:model,LLaMA-2-70B
:`ai assistant`,Dromedary-2
:benchmark,various benchmark datasets
:resource,code and model weights
:goal,research into aligning LLM-based AI agents
:technique,language model finetuning
:technique,NeFTune
:process,adding noise to embedding vectors
:benchmark,ALPaCAEval
:dataset,modern instruction datasets
:model,Evol-Instruct
:model,OpenPlatypus
:model,LLaMA-2-Chat
:technique,augmentation of language models
:direction,fine-tuning language models
:setup,question answering with Google Search API
:data,500 agent trajectories
:approach,FIREACT
:data,diverse fine-tuning data
:aspect,scaling effects
:benefit,comprehensive benefits of fine-tuning
:field,evolutionary dynamics
:population,diverse and social populations
:technique,generative models
:`model type`,social agent-based evolutionary models
:context,game-theoretic relationship
:model,agent model
:strategy,deterministic strategies
:process,evolution
:criterion,average payoff
:process,mutation
:behavior,cooperative behavior
:trait,cooperative personality traits
:trait,selfish personality traits
:issue,toxic discourse
:challenge,designing social media for better conversations
:method,simulation of social media
:impact,news feed algorithms on conversation quality
:method,agent-based modeling
:`data source`,American National Election Study
:concept,realistic personas
:algorithm,most liked and commented
:platform,simulated social media platform 1
:algorithm,all user posts
:platform,simulated social media platform 2
:algorithm,bridging
:platform,simulated social media platform 3
:outcome,constructive conversation
:field,simulation research
:dataset,logical puzzles
:`puzzle type`,various puzzles
:software,Prover9
:software,MACE4
:benchmark,100 logical puzzles
:classification,reasoning faults
:`fault type`,logical faults
:annotation,ChatGPT answers
:metric,faulty text percentage
:strategy,distributed training
:`trade-off`,memory consumption vs communication cost
:strategy,memory-communication balanced strategy
:optimizer,Partial Redundancy Optimizer (PARO)
:`communication topology`,Hierarchical Overlapping Ring (HO-Ring)
:goal,communication efficiency
:experiment,training throughput improvement
:experiment,scalability achievement
:algorithm,HO-Ring
:algorithm,traditional ring
:technology,system-on-chip design
:attribute,ubiquity and complexity
:task,SoC security incorporation
:solution,existing security solutions
:task,SoC security verification
:capability,"natural language understanding, advanced reasoning, program synthesis"
:`model architecture`,Generative Pre-trained Transformer
:integration,LLM integration into SoC security
:content,"analysis, case studies, experiments, guidelines"
:corpus,Japanese Emotional Speech Corpus (JVNv)
:`content type`,verbal content and nonverbal vocalizations
:corpus,emotional speech corpus
:`content type`,emotional scripts and nonverbal vocalizations
:method,automatic script generation
:task,producing emotional scripts
:quality,phoneme coverage and emotion recognizability
:task,emotional text-to-speech synthesis
:challenge,synthesizing emotional speech with NVs
:milestone,automatic script generation using LLMs
:field,chemistry and biology
:analogy,chemical and natural language
:model,initial models
:data,various data types
:data,"linearised molecular graphs, spectra, synthesis actions, human language"
:trend,generic task-solving models in chemistry
:concept,flexibility of natural language
:field,scientific discovery
:approach,existing text classification
:resource,large annotated corpora
:approach,novel few-shot text classification
:interaction,user and LLM coauthoring
:result,high accuracy classifiers
:study,end-user study
:classifier,personalized classifier
:benchmark,state-of-the-art approach
:problem,optimization problems
:method,heuristic solutions
:barrier,expertise requirement
:activity,adoption of optimization tools and techniques
:agent,Optimus
:capability,formulating and solving MILP problems
:task,development and validation of solutions
:dataset,NLP4LP
:technique,basic LLM prompting strategy
:resource,Optimus code and NLP4LP dataset
:technique,federated text-driven prompt generation (FedTPG)
:challenge,generalization to unseen classes
:network,prompt generation network
:entity,remote clients
:input,task-related text
:technique,federated prompt learning methods
:attribute,3H responses
:value,"harmless, helpful, and honest"
:`research area`,conventional reinforcement learning
:technique,online inverse reinforcement learning
:concept,proxy for human feedback
:technique,policy learning
:problem,entity resolution
:solution,traditional entity resolution
:task,manual expertise
:characteristic,domain dependency
:solution,seamless entity resolution
:risk,quality dependency on prompt engineering
:study,systematic experimental study on prompting methods for ER
:result,preliminary results
:system,conversational recommender system
:model,P4LM
:task,item recommendation
:technique,embedding space representation
:outcome,compelling responses
:function,joint reward function
:dataset,MovieLens 25M
:outcome,personalized movie narratives
:performance,remarkable performance
:challenge,managing long documents and extended conversations
:challenge,increased computational requirements
:task,long context processing
:challenge,context truncation
:limitation,fixed context length
:method,selective context
:goal,inference efficiency
:`data source`,"arXiv papers, news articles, long conversations"
:result,reduction in context cost
:result,reduction in inference memory usage
:result,reduction in inference time
:result,minor drop in performance metrics
:principle,Uniform Information Density (UID)
:behavior,human language production
:detector,GPT-Who
:feature,UID-based features
:concept,authorship attribution
:detector,state-of-the-art detectors
:attribute,interpretable representation
:analysis,UID-based representations analysis
:behavior,information distribution by authors
:dataset,large-scale benchmark datasets
:`language model`,off-the-shelf LM
:concept,number representation
:`human concept`,number line
:technique,subword tokenization
:concept,number magnitude
:approach,alternative number representation
:approach,notation modification
:approach,vocabulary modification
:approach,architectural modification
:finding,architectural change effectiveness
:task,number estimation
:finding,vocabulary change trade-off
:task,masked number prediction
:technique,tokenization scheme
:task,numerical fact estimation
:`problem type`,Fermi problems
:task,geospatial tasks
:`data source`,satellite imagery
:`information type`,spatial information
:method,naive querying with geographic coordinates
:task,predicting population density
:method,GeoLLM
:`data source`,OpenStreetMap
:task,geospatial prediction tasks
:characteristic,sample-efficiency and geospatial robustness
:`data source`,geospatial covariates
:evaluation,human exams
:domain,agriculture
:technique,ensemble refinement (ER)
:evaluation,agriculture exams
:`model family`,general-purpose model
:benchmark,human subjects
:task,crop management guidelines generation
:agency,Brazilian Agency of Agriculture (EMBRAPA)
:evaluation,graduate program exams from India
:field,agricultural education and practice
:technology,natural language communication
:milestone,invention of computers
:process,language learning in humans
:adage,practice makes perfect
:capability,arithmetic operations
:capability,image explanation and generation
:report,NLP movement report June 2022
:timeframe,late 2021 to early 2022
:movement,NLP movement
:application,dialogue technology
:feature,developer-specified persona
:feature,personas
:characteristic,opaqueness and unpredictability
:representation,personas as sets of self-knowledge
:knowledge,habitual knowledge
:format,story-like narratives
:representation,schema
:approach,dialogue generation with schema retrieval
:method,bootstrapping schema creation
:component,second pass rescoring
:system,automatic speech recognition (ASR) system
:task,ASR hypothesis rescoring
:`training type`,discriminative training
:criterion,minimum word-error-rate (MWER)
:`training type`,discriminative fine-tuning
:architecture,pooling-based architecture
:`model type`,causal large language model
:`model type`,bidirectional large language model
:`training type`,MWER training schemes
:application,NLP applications
:task,sentence interpretation
:application,information retrieval systems
:process,decontextualization
:task,sentence preservation
:`model type`,generative seq2seq models
:method,few-shot decontextualization
:performance,viable performance on multiple domains
:attack,power side-channel attack
:vulnerability,dynamic power consumption leakage
:analysis,power side-channel analysis
:system,cryptographic systems
:analysis,post-silicon power side-channel analysis
:implementation,post-silicon
:analysis,pre-silicon power side-channel analysis
:goal,early vulnerability detection
:framework,SCAR
:technology,Graph Neural Networks
:representation,control-data flow graphs
:component,deep learning-based explainer
:component,fortification component
:algorithm,encryption algorithms
:metric,localization accuracy
:analysis,explainability analysis
:process,security-critical hardware design cycle
:task,multi-lingual code-related tasks
:model,CodeFuse-13B
:entity,programming languages
:dataset,pre-training dataset for CodeFuse-13B
:benchmark,HumanEval-X
:benchmark,CodeFuseEval
:feedback,human feedback from AntGroup
:metric,HumanEval pass@1 score
:task,code-related tasks with Chinese prompts
:domain,medical
:system,medical generative QA systems
:methodology,interactive self-reflection
:process,knowledge acquisition and answer generation
:attribute,"factuality, consistency, and entailment"
:ability,interactivity and multitasking
:evaluation,automatic and human evaluation
:process,token sampling
:issue,information loss
:`communication regime`,CIPHER
:capability,broader information encoding
:method,LLM debate using natural language
:method,traditional inference
:`communication medium`,embeddings
:capability,LLM communication
:concept,society's production methods and productivity
:application,AI-generated content
:value,authenticity and reliability
:process,distributed AI training
:issue,content identification and traceability
:era,AI powered by LLMs
:issue,information security
:vision,trusted AI
:concept,blockchain for LLM (BC4LLM)
:component,"reliable learning corpus, secure training process, identifiable generated content"
:field,communication networks
:concept,blockchain and LLM applications
:technology,blockchain and large language models
:system,dialog-based music recommendation system
:platform,MuseChat
:feature,interactive user engagement
:action,personalize music selection
:`previous systems`,music recommendation systems
:feature,content compatibility
:method,conversation-synthesis
:engine,multi-modal recommendation engine
:benchmark,music retrieval tasks
:concept,natural language framework
:task,cross-domain text-to-SQL
:requirement,in-domain annotations
:technique,in-domain demonstration examples
:framework,ODIS
:technique,demonstration selection
:source,hybrid sources
:metric,execution accuracy
:platform,robotics platform
:agent,conversational AI agent
:data,general knowledge corpus
:entity,physical world
:system,conversational AI and robotics system
:scenario,tour-guide
:study,HRI study
:comparison,robots with vs without conversational AI
:metrics,"effectiveness, exploration, scrutinization, personification, adaptability"
:`model type`,compositional text-to-image model
:task,generating high-quality aligned images
:task,multi-dimensional assessment of text-image alignment
:capability,alignment capabilities
:`model family`,fine-tuned diffusion model
:task,pinpointing misalignment
:algorithm,image editing algorithm
:task,correcting misalignment
:result,resultant image
:input,input text
:methodology,proposed methodology
:task,compositional image generation
:aspect,object number
:aspect,attribute binding
:aspect,spatial relationships
:aspect,aesthetic quality
:approach,automatic test case generation
:artifact,test cases
:artifact,test suites
:method,human-written test cases
:characteristic,incompleteness
:challenge,automatic program repair
:goal,barrier breaking
:approach,random unit testing
:method,unit testing
:approach,generation of complex user execution scenarios
:source,bug reports
:task,test case generation
:model,CodeGPT
:artifact,Defects4J bugs
:artifact,new bug reports
:artifact,LLM-generated test cases
:technique,watermark algorithm
:goal,high accuracy detection
:process,adding watermark logits
:challenge,trade-off between attack robustness and security robustness
:method,semantic invariant watermarking
:technique,semantic embedding
:process,generating watermark logits
:goal,attack robustness
:context,semantically invariant settings
:goal,security robustness
:url,https://github.com/thu-bpm/robust_watermark
:capability,performing various tasks
:issue,safety and malicious content generation
:capability,alignment manipulation
:process,model manipulation
:concept,jailbreaking
:method,in-context attack
:outcome,harmful outputs
:method,in-context defense
:capability,model robustness
:experiment,ICA and ICD effectiveness
:outcome,success rate of adversarial jailbreaking attacks
:process,creating user personas
:workflow,improved prompting workflow
:model,GPT-3.5-turbo-16k
:technique,refined prompting
:`analysis phase`,phase 2 and 3 of thematic analysis
:approach,data-driven personas
:approach,qualitative personas
:data,qualitative interviews
:behavior,human word prediction and reading speed
:performance,human and LM alignment
:scenario,repeated text spans
:dataset,human next-word predictions
:behavior,human word prediction
:component,specific attention heads
:modification,power-law recency bias
:behavior,human behavior
:issue,propaganda
:value,societal harmony
:value,truth dissemination
:task,propaganda detection
:challenge,subtle manipulation techniques
:challenge,contextual dependencies
:dataset,SemEval-2020 Task 11
:value,human values alignment
:method,alignment methods
:`feedback type`,singular human feedback
:`feedback type`,"preferences, annotated labels, natural language critiques"
:method,constructive and diverse feedback (CDF)
:`feedback type`,"critique, refinement, preference feedback"
:`feedback type`,critique feedback
:`problem difficulty`,easy problems
:`feedback type`,refinement feedback
:`problem difficulty`,medium problems
:`feedback type`,preference feedback
:`problem difficulty`,hard problems
:outcome,enhanced alignment performance
:task,"question answering, dialog generation, text summarization"
:outcome,superior performance with smaller training dataset
:process,reward modelling
:property,out-of-distribution generalisation
:property,output diversity
:risk,jailbreak problem
:behavior,undesirable behavior
:measure,preventive measures for LLMs
:challenge,multilingual jailbreak challenges
:scenario,unintentional risk scenario
:mechanism,safety mechanisms
:scenario,intentional risk scenario
:behavior,deliberate attack
:framework,self-defense framework
:task,safety fine-tuning
:content,potentially harmful content
:process,item indexing
:application,LLM-based recommendation
:process,generation grounding
:identifier,id-based identifiers
:aspect,semantic richness or uniqueness
:identifier,description-based identifiers
:issue,out-of-corpus identifiers
:aspect,initial token quality
:paradigm,TransRec
:identifier,multi-facet identifiers
:`data structure`,specialized data structure for TransRec
:process,in-corpus identifier generation
:technique,substring indexing
:model,BART-Large
:setting,diverse settings
:method,training and testing
:approach,self-check
:task,factual error detection
:benchmark,PHD
:scope,passage-level detection
:method,proposed self-check method
:methodology,zero-resource methods
:`benchmark dataset`,commonly-used benchmark datasets
:quality,reliability and robustness
:framework,unified robustness evaluation framework
:capability,dialogue understanding
:dataset,Noise-LLM
:data,input perturbation evaluation data
:method,multi-level data augmentation
:data,candidate data pool
:strategy,automatic task demonstration construction strategies
:task,slot-filling task
:aim,assess robustness methods performance
:scenario,real-world noisy scenarios
:observation,limited perturbation robustness performance
:suggestion,forward-looking suggestions
:`research direction`,robustness of large language models
:tool,C decompiler
:artifact,C source code
:artifact,executable
:application,reverse engineering
:output,decompiler output
:activity,manual fixing
:task,augmenting decompiler outputs
:goal,recompilability
:tool,IDA-Pro
:task,recompiling decompiler outputs
:approach,hybrid approach with LLMs
:artifact,C test cases
:metric,recompilation success rate
:discussion,limitations and future research
:task,continual relation extraction
:cause,lack of robustness against future analogous relations
:concept,rationale
:strategy,multi-task rationale tuning
:goal,robust learning of current relations
:technique,contrastive rationale replay
:goal,distinguishing analogous relations
:method,our method
:benchmark,state-of-the-art CRE models
:benchmark,two standard benchmarks
:profession,human coders
:approach,automated ICD coding
:technique,supervised deep learning
:challenge,predicting rare codes
:domain,clinical practice
:solution,zero-shot and few-shot ICD code assignment
:process,task-specific training
:knowledge,ICD ontology and clinical coding
:ontology,ICD ontology
:technique,sparse search
:activity,daily life activities
:value,equitable access
:study,usage investigation of LLMs
:demographic,US citizens
:finding,gender gap in LLM adoption
:demographic,gender
:education,technology-related education
:user,expert LLM user
:user,novice LLM user
:scenario,professional task
:education,artificial intelligence education
:method,international replication and longitudinal observation
:issue,OOD and adversarial inputs
:approach,Data Map (DM)
:model,reference model
:data,original training set
:model,main model
:data,selected important training examples
:issue,computational expense
:observation,training dynamics transferability
:model,main model using DM
:method,empirical risk minimization (ERM)
:approach,Fine-Tuning by Transferring Training Dynamics (FTFT)
:attribute,generalization robustness
:technique,prompt-based learning
:approach,single vector continuous prompt optimization
:issue,semantic space collapse
:issue,representation convergence
:model,Topic-DPR
:technique,topic-based prompts
:strategy,positive and negative sampling
:benchmark,state-of-the-art retrieval techniques
:ability,counterfactual reasoning
:entity,human intelligence
:concept,alternatives to observed states or past events
:ability,planning and decision-making
:dataset,counterfactual Q&A pairs
:`model family`,code generation large language model
:resource,code and dataset
:URL,https://github.com/letian2003/c-vqa
:technique,ensembling
:task,encyclopedic-vqa
:model,vanilla large vision-language model
:model,large vision-language model with extra context
:model,large vision-language model augmented with retrieval
:experiment,oracle experiment
:model,best single model
:model,best possible ensemble
:application,code generation via prompting
:output,code generated by LLMs
:activity,empirical characterization
:issue,errors in robot programming
:category,interpretation and execution errors
:issue,errors in execution
:behavior,forgetfulness of LLMs
:activity,using LLMs in robot programming
:knowledge,lessons learned
:task,benchmarking LLM-powered development
:technique,counterfactually-augmented data
:feature,domain-independent causal features
:phenomenon,myopia
:theory,Fisher's linear discriminant
:performance,out-of-distribution generalization performance
:technology,web infrastructure
:service,web-based automatic services
:`research area`,LLM-based knowledge graph completion
:capability,inference capabilities
:information,structural information in KGs
:approach,structural-aware reasoning
:model,knowledge prefix adapter
:technique,structural embedding pre-training
:experiment,structural-aware LLM-based KGC methods
:analysis,in-depth analysis of structural information introduction
:capability,knowledge reasoning ability
:resource,code for KoPA
:URL,https://github.com/zjukg/kopa
:challenge,evaluating and explaining code generation capability
:characteristic,complexity and lack of transparency of LLMs
:field,causality analysis
:approach,causality analysis-based approach
:relationship,causal relations between prompts and generated code
:representation,causal graph-based representation
:concept,"fine-grained, human-understandable concepts in prompts"
:representation,causal graph
:study,LLM effectiveness study
:technique,prompt calibration
:network,multiplex text-rich network
:relation,multiple semantic relations
:method,text representation learning
:embedding,single-view embedding
:`model family`,multiplex graph neural network
:representation,node attribute feature vector
:aspect,semantics of nodes' texts
:framework,METERN
:task,learning multiplex embeddings
:approach,relation-specific representations
:performance,outperforms baselines
:resource,METERN code
:process,intermediate reasoning chains
:issue,performance-generalization gap
:method,Meta-CoT
:process,scenario categorization
:process,demonstration construction
:outcome,high performance and generalization
:benchmark,SVAMP
:quality,stability and generality
:`model family`,moderate-sized large language model
:potential,building smaller yet powerful LLMs
:cost,training cost
:technique,structured pruning
:goal,developing smaller LLMs
:technique,targeted structured pruning
:process,model size reduction
:technique,dynamic batch loading
:process,training data sampling
:`model series`,Sheared-LLaMA
:performance,outperforming equivalent models
:capability,"task-solving, instruction-following, safety"
:benchmark,existing continual learning benchmarks
:benchmark,TRACE
:dataset,8 distinct datasets
:format,unified format
:model,LLaMA2-Chat 13B
:challenge,tradeoff between task performance and original prowess
:capability,preserving capabilities
:approach,Reasoning-Augmented Continual Learning (RCL)
:`evaluation framework`,SWe-Bench
:data,software engineering problems
:task,codebase editing
:skill,complex reasoning
:environment,execution environments
:model,SWe-LLaMa
:quality,"practicality, intelligence, autonomy"
:pretraining,on high quality tokens
:model,Minerva
:task,quantitative reasoning problems
:dataset,open source web datasets
:feature,mathematical notation preservation
:dataset,OpenWebMath
:resource,mathematical webpages dataset
:process,dataset creation
:experiment,training on OpenWebMath
:outcome,improved model performance
:platform,Hugging Face Hub
:`model family`,general purpose pre-trained language model
:benchmark,com2sense dataset
:disconnect,between current models and cutting-edge ML methods
:method,model ensemble
:method,pairwise contrastive objective
:capability,outputting falsehoods
:technique,truth inference probing
:criticism,generalization failure
:dataset,true/false statements
:concept,LLM representations of truth
:evidence,visualizations
:concept,linear structure in representations
:experiment,transfer experiments
:concept,probe generalization
:evidence,causal evidence
:action,surgical intervention in forward pass
:claim,linear representation of truth
:technique,mass-mean probing
:model,Mistral 7B v0.1
:attribute,7-billion-parameter language model
:model,LLaMA 1 34B
:technique,grouped-query attention (GQA)
:technique,sliding window attention (SWA)
:model,Mistral 7B -- Instruct
:model,LLaMA 2 13B -- Chat
:`model family`,Mistral 7B models
:task,document-based question-answering
:task,meeting summarization
:task,clinical report generation
:challenge,hallucination evaluation
:method,SynTra
:problem,hallucination in LLMs
:technique,prefix-tuning
:task,realistic abstractive summarization tasks
:model,13b-parameter LLM
:approach,optimizing system message
:approach,fine-tuning model weights
:problem,undesired behaviors in LLMs
:`agent type`,functional language agent
:model,LEMUR
:capability,natural language and coding
:model,LEMUR-Chat
:capability,natural language or coding
:`training method`,meticulous pre-training
:benchmark,text and coding benchmarks
:task,agent tasks
:`agent type`,advanced open-source agents
:process,educational test development
:attribute,expensive and time-consuming
:`test type`,parallel tests
:process,monitoring student progress
:test,silent sentence reading efficiency tests
:skill,reading ability
:task,simulating student responses
:task,test item generation
:task,item filtering
:technique,optimal-transport-inspired technique
:task,parallel test generation
:evaluation,crowdworker responses
:attribute,test difficulty and reliability
:evaluation,student test scores
:standard,standard test form
:metric,correlation coefficient (r)
:value,0.93
:task,audio description
:audience,visually impaired audiences
:domain,movies
:model,movie audio description
:component,character bank
:model,timing determination
:scenario,long context scenarios
:`performance factor`,density and position of key information
:technique,prompt compression
:model,LongLLMLingua
:result,performance boost
:benchmark,NaturalQuestions
:benchmark,LongBench
:benchmark,ZeroScrolls
:`compression rate`,2x-10x
:event,AI incidents
:institution,universities
:policy,AI policies
:strategy,regulating AI
:goal,awareness and risk minimization
:tool,educational tools
:task,bug triaging
:process,managing bug reports
:characteristic,challenge
:task,assigning bugs
:`model family`,transformer-based language model
:task,automated bug triaging
:study,investigation of transformer-based language models for bug triaging
:data,open source datasets
:model,DeBERTa
:analysis,quantitative and qualitative analysis
:technique,transformer-based language model fine-tuning
:output,natural-sounding audio
:task,audiobook narration
:attribute,emotional prosody
:dataset,aligned book-audiobook pairs
:`model family`,prosody prediction model
:attribute,prosody attributes
:benchmark,human audiobook readings
:attribute,predicted pitch
:attribute,predicted volume
:study,human evaluation study
:preference,prosody-enhanced audiobook readings
:capability,cyber threat reasoning and automation
:process,manual exploration
:capability,threat-related action support
:process,decision automation
:capability,cyber campaign automation
:process,plan-act-report loop
:technique,prompt chaining
:process,sequential decision process
:knowledge,cyber-specific knowledge
:outcome,actionable responses
:impact,LLM on threat landscape
:application,generative AI to cyber threats
:capability,complex network handling
:advancement,LLM-supported cyber adversarial landscape
:community,cybersecurity community
:task,Theory of Mind (ToM) tasks
:concept,social cognition
:relationship,principal-agent relations
:entities,humans and artificial intelligences
:mechanism,Violation of Expectation (VoE)
:framework,metacognitive prompting
:process,storing and retrieving facts
:concept,user psychology
:goal,risk mitigation
:activity,future inquiry
:technique,adversarial prompting
:attack,generation exploitation attack
:technique,decoding method manipulation
:method,alignment method
:goal,reducing misalignment rate
:study,safety evaluation and alignment procedures
:URL,https://github.com/princeton-sysml/jailbreak_llm
:`model family`,text-to-text language model
:issue,limited model capacity
:method,filtering and re-ranking
:property,instance_of
:`knowledge type`,contextualized representations
:ability,physical reasoning
:repository,Newton
:pipeline,domain-specific adaptation
:data,object-attribute pairs
:task,QA questions
:application,physically grounded settings
:field,robotic manipulation
:project,Newton Reasoning
:website,https://newtonreasoning.github.io
:task,macro extraction
:goal,understanding mobile interaction
:challenge,macro extraction at scale
:characteristic,multiple steps and hidden components
:approach,novel LLM-based macro extraction
:output,extracted macros
:feature,natural language descriptions
:feature,fully executable
:study,user evaluation
:study,comparative analysis
:benchmark,human-curated tasks
:study,automatic execution
:analysis,experiments and analyses
:outcome,effectiveness and usefulness of extracted macros
:challenge,long-tail label distribution
:class,rare classes
:resource,training samples
:work,previous works
:concept,model architectures and hierarchical label structures
:knowledge,medical guidelines
:classifier,DKEC
:innovation,label-wise attention mechanism and group-wise training method
:mechanism,label-wise attention mechanism
:concept,semantic relationships between medical entities
:method,group-wise training method
:dataset,RAA dataset
:dataset,MIMIC-III dataset
:system,end-to-end ASR system
:component,ASR components
:technique,language model fusion
:problem,domain mismatch
:concept,acoustic model fusion
:capability,using external tools
:approach,fine-tuning on tool demonstrations
:capability,generalization to new tools
:approach,providing tool documentation in context
:limitation,number of tools
:issue,syntactically invalid tool calls
:algorithm,ToolDec
:`model family`,tool-augmented large language model
:issue,tool-related errors
:capability,effective tool selection
:evaluation,ToolDec evaluation
:outcome,reduced syntactic errors and better performance
:capability,generalization to unseen tools
:experiment,ToolDec experiments
:task,tasks involving tools
:technique,ensembling generations
:assumption,fixed input prompt
:technique,prompt diversity
:method,div-se
:method,idiv-se
:benchmark,planning benchmarks
:concept,accuracy-cost trade-off
:task,argumentative stance prediction
:problem,multimodal problem
:event,first shared task in multimodal argument mining
:task,stance prediction
:topic,social topics
:factor,images necessity
:`model type`,unimodal and multimodal models
:ensemble,fine-tuned text-based language models
:`model type`,multimodal and text-based few-shot LLMs
:method,image content summarized as natural language
:technique,in-context examples
:era,digital epoch
:domain,cyberspace
:integration,information operations and large language model
:phenomenon,paradigm shift
:model,Mistral 7B
:spectrum,actors in cyberspace
:tool,narrative-shaping instruments
:framework,ClausewitzGPT equation
:value,moral compasses and societal imperatives
:strategy,Clausewitz's military strategy
:trend,AI information campaigns
:metric,year-on-year growth
:philosophy,Enlightenment thinking
:strategy,clear strategic vision and ethical considerations
:`model family`,deep language model
:field,psycholinguistics
:concept,continuous numerical vectors
:model,GPT-2 XL
:study,temporal dynamics of language comprehension
:technique,electrocorticography (ECoG)
:attribute,temporal resolution
:activity,neural activity recording
:`brain region`,inferior frontal gyrus (IFG)
:study,neural activity modeling
:model,linguistic processing hierarchy
:study,temporal receptive window
:correlation,layer depth and neural activity timing
:document,crash report
:process,software maintenance
:task,crash reproduction
:characteristic,time-consuming and tedious
:study,automatic crash reproduction
:method,natural language description
:document,stack-trace-only crash report
:feature,step-by-step guidance
:task,understanding and reproducing crashes
:technique,existing automatic support
:approach,CrashTranslator
:artifact,crash reports
:metric,reproducing time
:framework,distributional framework
:risk,socio-technical risks of foundation models
:approach,statistical relative testing
:concept,stochastic dominance
:concept,second order statistics
:`model type`,mean-risk models
:field,econometrics
:field,mathematical finance
:approach,risk-aware approach for foundation model selection
:theory,portfolio optimization and selection theory
:concept,metrics portfolio
:task,foundation model selection
:analysis,asymptotic analysis via central limit theorems
:concept,statistical significance
:technique,bootstrap variance estimate
:risk,risks related to drifting from instructions and outputting toxic content
:`health issue`,mental illness
:field,public health
:profession,psychotherapy professionals
:issue,professional scarcity and accessibility
:task,psychotherapy
:skill,deep complex reasoning and analysis
:application,AI assistance for computational psychotherapy
:task,cognitive distortion detection
:field,computational psychotherapy
:technique,Diagnosis of Thought (DoT) prompting
:stage,subjectivity assessment
:stage,contrastive reasoning
:stage,schema analysis
:outcome,diagnosis rationales
:model,DoT
:`rationale quality`,high-quality rationales
:profession,human experts
:outcome,performance gains
:constraint,resource requirements
:approach,full-parameter fine-tuning
:framework,QFT
:optimizer,LION optimizer
:hardware,A6000 GPU
:phenomenon,online social movements
:task,understanding perspectives
:issue,lack of annotated data
:approach,weakly supervised graph-based
:task,modeling perspectives
:representation,social-linguistic
:data,tweets
:resource,seed set of labeled examples
:task,generating artificial training examples
:baseline,multitask baselines
:movement,#BlackLivesMatter
:task,characterizing perspectives
:domain,music
:attribute,unique and complex structure
:domain,audio
:model,LLaRK
:`model type`,instruction-tuned multimodal model
:method,augmenting annotations
:`model family`,pretrained generative model for music
:task,music understanding tasks
:source,open-source music data
:resource,training code
:resource,additional results and audio examples
:url,https://bit.ly/llark
:resource,source code
:url,https://github.com/spotify-research/llark
:`knowledge type`,commonsense knowledge
:`model family`,commonsense-aware model
:challenge,high-quality knowledge base acquisition
:method,PHALM
:`knowledge graph`,Japanese event knowledge graph
:model,Japanese commonsense generation model
:result,acceptability of knowledge graph and inferences
:comparison,prompting humans vs. LLM
:resource,"code, data, and models"
:`biological process`,mitotic count
:`medical condition`,cancer
:task,mitotic counting
:issue,inter-rater variability
:`model family`,convolutional neural network
:task,mitosis detection
:standard,expert panel review
:`model family`,large-scale vision-language model
:task,image captioning and visual question answering
:dataset,MIDOG22
:data,mitotic figures and hard negatives
:technique,speculative decoding
:process,inference acceleration
:model,draft model
:technique,online speculative decoding
:challenge,low predictive accuracy
:resource,computational power
:data,user query data
:data,query distribution
:technique,online knowledge distillation
:data,synthetic and real query data
:metric,token acceptance rate
:improvement,latency reduction
:technique,sparsely activated mixture-of-experts
:goal,scaling models efficiently
:model,existing MoE model
:technique,fixed gating network
:token,linguistic complexity
:`trade-off`,computation per token and model performance
:technique,adaptive gating in MoE
:model,MoE model
:goal,reduce training time
:outcome,reduced training time
:analysis,comprehensive analysis of routing decisions
:achievement,passing standardized exams
:role,support tools for healthcare workers
:context,high-risk
:concept,limitations of large language models
:trend,rapid development and release of new LLMs
:task,identifying patterns across models
:evaluation,comparison of LLMs
:outcome,preliminary observations and open questions
:feature,longer contexts
:challenge,responsive LLM systems with long contexts
:`system optimization`,computation delay optimization
:feature,context processing
:issue,network delay in context fetching
:tool,CacheGen
:encoder,CacheGen encoder
:feature,key-value features
:controller,CacheGen controller
:feature,context loading strategy
:evaluation,CacheGen performance evaluation
:performance,LLM performance with CacheGen
:`model family`,speech generative model
:quality,speech quality
:framework,Vec-Tok Speech
:task,speech generation
:codec,speech codec
:component,speech vectors
:component,semantic tokens
:technique,byte-pair encoding
:goal,improving language model performance
:task,voice conversion
:task,speech-to-speech translation
:task,speech denoising
:task,speaker de-identification
:benchmark,SOTA models
:resource,Vec-Tok Speech code
:technique,computer vision and natural language processing
:framework,deep neural framework for image caption generation
:mechanism,GRU-based attention mechanism
:model,GRU-based language model
:model,Bahdanau attention model
:dataset,MSCOCO
:dataset,Flickr30k
:field,computer vision and natural language
:concept,specific domains
:system,autonomous robotic system
:task,human tasks
:environment,open-world environment
:task,task and motion planning
:`system architecture`,proposed system architecture
:concept,cognitive levels interplay
:`cognitive level`,motion generation
:strategy,novel replanning strategy
:task,generated plans
:architecture,feedback architecture
:scenario,blocks world
:scenario,barman
:scenario,pizza preparation
:model,BioBERT
:methodology,fine-tuning BioBERT
:task,biomedical text mining
:technique,specialized preprocessing
:data,biomedical texts
:`model family`,healthcare-focused language model
:challenge,integration complexities
:ability,auditory attention
:scenario,cocktail party
:`model type`,target speaker extraction model
:cue,pre-registered cues
:limitation,unreliable pre-registered cues
:integration,natural language description in TSE
:model,LLM-TSE
:cue,semantic cues from text
:result,competitive performance with text-based cues
:result,effectiveness of text as task selector
:result,new state-of-the-art with combined cues
:study,incorporation of LLMs in TSE
:research,cocktail party problem
:issue,factuality of generated knowledge
:framework,CONNER
:aspect,generated knowledge quality
:task,knowledge-grounded dialogue
:finding,factuality vs. task performance
:issue,factual mistakes
:strategy,knowledge selection
:resource,evaluation code and LLM-generated knowledge with human annotations
:`research focus`,privacy in large language models
:issue,memorized training data extraction
:value,individual privacy
:study,comprehensive study on LLMs' capability to infer personal attributes
:capability,personal attribute inference
:dataset,real Reddit profiles
:metric,inference accuracy
:threat,privacy-invasive chatbots
:interaction,chatbot-user interaction
:mitigation,text anonymization
:threat,LLM inference
:`data source`,general web crawls
:dataset,cross-domain dataset
:benefit,data diversity
:dataset,high-quality dataset
:benchmark,comprehensive benchmark
:`model performance`,cross-domain dataset training
:`model performance`,high-quality dataset training
:model,trained models
:concept,residual stream
:challenge,bandwidth limitation
:component,attention heads
:function,memory management
:component,MLP layers
:layer,layer 2
:action,remove output
:layer,layer 0
:technique,direct logit attribution (DLA)
:phenomenon,misleading effects
:approach,circuit analysis
:goal,prevent interpretability illusions
:process,ChatGPT replication
:resource,cookbook
:task,customizing LLMs
:element,LLM bases
:element,parameter-efficient methods
:element,instruction data types
:factor,chain-of-thought data
:factor,human-value alignment
:model,Chinese version of ChatGPT
:model,Chinese LLMs
:URL,https://github.com/phoebussi/alpaca-cot
:`application domain`,tabular data learning
:`model family`,transferable tabular model
:limitation,lack of direct instruction support and foundational knowledge acquisition
:`model family`,tabular foundation model
:capability,profound understanding and universal capabilities for tabular data
:task,instruction-following tasks
:`performance aspect`,efficiency and competitive performance with scarce data
:aspect,limitations and potential opportunities
:state,outdated
:task,maintaining up-to-date status
:concept,aligning LLMs with changing world knowledge
:research,recent advances
:resource,paper list
:`label unit`,phoneme
:`label unit`,word
:approach,lattice-free
:`model application`,phoneme-based neural transducers
:approach,n-best-list
:method,context history approximation
:`model family`,word-level language model
:`model family`,phoneme-level language model
:factor,context size of the LM
:factor,hypothesis space quality
:method,ELECTRA
:model,auxiliary model
:process,training assistance
:process,post-training
:method,FAST-ELECTRA
:technique,temperature scaling
:strategy,learning curriculum
:benchmark,state-of-the-art ELECTRA-style pre-training
:quality,pre-training stability
:`learning approach`,multimodal learning
:`data modality`,multiple data modalities
:example,image-caption pairs
:example,audio-text pairs
:algorithm,multimodal learning algorithm
:`data structure`,one-to-one pairs
:`real-world setting`,complex multimodal interactions
:framework,multimodal graph learning (MMGL)
:goal,capturing multimodal information
:`research question`,infusing multiple neighbor information into LMs
:`research question`,infusing graph structure information into LMs
:`research question`,finetuning LMs with neighbor context
:problem,phonological reconstruction
:field,historical linguistics
:approach,computational historical linguistics
:field,computational biology
:model,MSA Transformer
:data,multiple sequence alignments
:model,Cognate Transformer
:task,cognate reflex prediction
:task,masked word prediction
:task,NLP downstream tasks
:model,KwaiYiimath
:model,KwaiYiibase1
:technique,reinforced learning from human feedback
:dataset,KMath
:task,evaluating problem-solving correctness
:benchmark,"GSM8K, CMath, KMath"
:language,English and Chinese
:challenge,inherent limitations
:`external assistance`,external world resources
:retriever,general-purpose retriever
:retriever,task-specific retriever
:approach,LLM-Embedder
:strategy,optimization strategies
:entity,retrievers
:resource,checkpoint and source code
:URL,https://github.com/flagopen/flagembedding
:issue,restricted access and privacy concerns
:technique,iterative self-critique and self-refinement
:method,model improvement
:metric,PERFICS
:task,optimal model selection
:process,domain-agnostic self-refinement
:activity,informed decision-making in model selection
:goal,democratizing access to high-performing language models
:goal,reducing costs
:technique,human feedback integration
:challenge,"efficient, effective, and unbiased feedback collection"
:survey,learning from human feedback approaches
:repository,ACL and arXiv
:trend,pre-LLM human feedback integration
:technique,present human feedback techniques
:concept,values and preferences frameworks
:process,feedback collection
:vision,better future of feedback learning
:challenge,conceptual and practical challenges in feedback learning
:task,NLP-related tasks
:`application area`,AI for IT operations
:performance,LLMs in AIOPS tasks
:task,AIOPS tasks
:benchmark,Opseval
:data,"7,200 questions"
:technique,few-shot in-context learning
:metric,GPT-4-score
:community,experts
:technique,automatic metrics
:`research area`,large language model evaluation
:benchmark,LLMBAR
:data,curated output pairs
:model,LLM evaluator
:finding,distinct evaluator performance
:goal,improvement
:comparison,LLM vs human evaluators
:goal,better instruction-following models
:issue,vulnerabilities
:attack,backdoor attack
:attack,composite backdoor attack
:attack,existing backdoor attacks
:attack,single-component trigger implantation
:condition,all trigger keys appear
:experiment,CBA against LLaMA-7B on Emotion dataset
:outcome,effective backdoor attack
:scenario,specific user groups targeting
:research,security research on LLM trustworthiness
:`data attribute`,alttexts
:issue,image-text alignment
:technique,caption rewriting using large language model
:dataset,curated datasets
:dataset,CC3M
:dataset,CC12M
:issue,noise and randomness in web-captured captions
:aspect,data quality
:technique,visual concepts integration
:aspect,data variety
:technique,mixed training scheme
:model,VECLIP
:evaluation,scales of raw data
:model,FERRET
:technique,hybrid region representation
:concept,region in image
:technique,spatial-aware visual sampler
:`input type`,diverse region inputs
:dataset,GRIT
:`data characteristic`,rich hierarchical spatial knowledge and hard negative data
:task,referring and grounding tasks
:benchmark,region-based and localization-demanded multimodal chatting
:capability,describing image details and alleviating object hallucination
:resource,code and data for FERRET
:URL,https://github.com/apple/ml-ferret
:setting,diverse deployment scenarios
:model,PALM 2
:`model architecture`,nested Transformer
:model,MatFormer
:component,FFN block
:capability,extraction of smaller models
:model,MatLM
:model,MatVit
:capability,metric-space structure preservation
:goal,data security
:challenge,preserving original data distribution
:watermark,distribution-preserving watermark
:framework,watermarking framework
:watermark,DiPMark
:property,original token distribution
:requirement,language model API or weights access
:property,moderate token changes
:strategy,reweight strategy
:function,hash function
:cipher,i.i.d. ciphers
:approach,DiPMark's approach
:qualities,"stealthiness, efficiency, and resilience"
:task,watermarking with quality preservation
:task,listwise ranking
:method,permutation self-consistency
:process,marginalizing list orders
:technique,prompt shuffling
:process,ranking aggregation
:outcome,convergence to true ranking
:model,LLaMA v2 (70B)
:benchmark,passage reranking
:URL,https://github.com/castorini/perm-sc
:model,RETRO
:attribute,7.5 billion parameters
:model,RETRO 48B
:model,GPT 43B
:data,100 billion tokens
:technique,RETRO augmentation method
:data,1.2 trillion tokens
:model,InstructRETRO
:task,zero-shot question answering
:content,creative content
:goal,usage policy compliance
:adversary,adversary
:attack,watermark removal
:goal,unregulated content usage
:attack,watermark forge
:goal,misattribution
:framework,WMAGI
:attack,watermark removal and forgery
:model,pre-trained diffusion model
:`model family`,diffusion model-based attacks
:method,existing prompt learning
:feature,domain-awareness
:method,domain-controlled prompt learning
:model,large-scale specialized domain foundation model (LSDM)
:knowledge,specialized domain knowledge
:strategy,noisy-adding strategy
:URL,https://anonymous.4open.science/r/dcpl-8588
:`data type`,tabular data
:`model family`,specialized table embedding model
:`model family`,language and table embedding model
:framework,Observatory
:representation,table embeddings
:concept,relational data model invariants and data distribution statistics
:concept,primitive properties for table embeddings
:analysis,Observatory analysis
:representation,learned representations over tables
:property,sample fidelity
:task,open-domain interleaved image-text generation
:framework,OpenLeaf
:technique,visual prompting
:concept,global context
:`model family`,large multi-modal model
:task,model assessment
:content,image-text content
:`evaluation technique`,large multi-modal model evaluation
:method,human assessment
:task,interleaved image-text generation
:`model type`,embedding-based and rule-based models
:task,temporal relational forecasting
:approach,temporal knowledge forecasting
:setting,generative
:challenge,complex temporal graph data structure
:challenge,enormous data sizes of TKGs
:cost,heavy computation costs
:framework,GenTKG
:challenge,generative forecasting on TKGs
:strategy,temporal logical rule-based retrieval
:technique,lightweight parameter-efficient instruction tuning
:benchmark,conventional methods of temporal relational forecasting
:quality,transferability
:task,fine-grained entity typing
:process,entity type identification
:method,conventional fine-grained entity typing
:approach,zero-shot fine-grained entity typing
:attribute,rich supporting information
:model,OneFET
:technique,ontology enrichment and coarse-to-fine typing algorithm
:technique,ontology enrichment
:attribute,instance and topic information
:technique,coarse-to-fine typing algorithm
:technique,entailment model training
:outcome,high-quality fine-grained entity typing
:concept,semantic identifier
:process,learning semantic identifiers
:technique,text encoding
:issue,information loss and distribution mismatch
:framework,LMIndexer
:component,semantic indexer
:capability,generating neural sequential discrete representations
:challenge,sequential discrete ID
:issue,semantic supervision deficiency
:objective,self-supervised document reconstruction
:`cognitive process`,analogy identification
:field,human cognition and language proficiency
:`research area`,word analogies
:concept,a is to b as c is to d
:`research interest`,text analogies
:`text type`,longer text
:community,NLP research community
:`language aspect`,syntactic and semantic structures
:ability,analogy identification ability
:ability,encoding syntactic and semantic structures
:method,time series encoding
:model,LLAMA-2
:procedure,time series tokenization
:ability,represent multimodal distributions
:feature,simplicity and repetition bias
:concept,activations in language models
:concept,sparse linear combinations
:technique,sparse coding
:task,reconstructing feature directions
:metrics,sparse coding success metrics
:concept,sparsity level
:concept,data distributions
:concept,model layers
:process,data collection and curation
:challenge,high costs and time investment
:technique,synthetic data generation
:data,LLM-generated synthetic data
:factor,subjectivity of classification
:performance,model trained on synthetic data
:task,vision-and-language navigation
:representation,language
:system,off-the-shelf vision systems
:task,converting views to language descriptions
:task,action selection for navigation
:approach,language-based navigation (LangNav)
:benchmark,R2R vision-and-language navigation
:task,generating synthetic trajectories
:environment,simulated environment (ALFRED)
:task,sim-to-real transfer
:baseline,visual feature-based baselines
:task,navigation tasks
:problem,reward specification
:requirement,large number of in-domain expert demonstrations
:`model family`,video-and-language model
:method,RoboClip
:approach,online imitation learning
:demonstration,out-of-domain demonstrations
:method,competing imitation learning methods
:task,robot manipulation tasks
:outcome,performance gains in NLP tasks
:resource,memory requirements
:component,position embeddings
:`model component`,multi-head attention mechanism
:`model component`,MHE attention
:outcome,predictive performance retention
:attribute,parameter requirements
:concept,abstract grammatical representations
:technique,structural priming
:setting,Dutch-English bilingual
:model,Dutch-English language model
:effect,crosslingual structural priming effects
:data,less than 1M tokens
:concept,low-resource transfer
:task,visual target navigation
:agent,autonomous robot
:`operation mode`,single-robot operation
:issue,reduced efficiency and robustness
:policy,multi-robot collaboration policy
:characteristic,resource-intensive
:framework,Co-NavGPT
:task,multi-robot cooperative visual target navigation
:capability,scene comprehension
:task,exploration frontier assignment
:dataset,Habitat-Matterport 3D (HM3D)
:domain,multi-robot collaboration
:resource,"supplementary video, prompts, and code"
:link,https://sites.google.com/view/co-navgpt
:process,construction inspection
:goal,construction project success
:issue,inefficiencies and inadequate information management
:issue,regulatory oversights and safety hazards
:framework,AutoRepo
:task,automated construction inspection report generation
:technology,unmanned vehicles
:task,construction inspection and information collection
:context,real-world construction site
:benefit,improved inspection process
:field,construction management
:value,efficiency and safety
:task,zero-shot object navigation
:capability,navigate towards open-vocabulary objects
:task,following individual instructions
:task,zero-shot interactive personalized object navigation
:limitation,neglecting natural language interaction and complexities of identifying user-specific objects
:framework,open-world interactive personalized navigation
:challenge,balance between task completion and efficiency
:method,all methods
:`user feedback`,diverse forms
:performance,agents' performance
:concept,language patterns and structures
:task,complex language tasks
:field,natural science
:task,scientific tasks
:method,using large language models for scientific datasets
:task,scientific inference
:task,knowledge synthesis
:system,conventional machine learning system
:task,explanation of predictions
:framework,AI for scientific discovery
:goal,accelerating scientific discovery
:task,information-seeking and reasoning tasks
:field,mental health care
:capability,generating empathetic responses
:dataset,EmpatheticDialogues
:model,Vicuna FastChat-T5
:model,Falcon-7B-Instruct
:metric,empathy-related metrics
:response,LLM responses
:response,traditional system responses
:response,human-generated responses
:advancement,empathetic conversational systems
:task,out-of-distribution detection
:attribute,reliability and trustworthiness
:approach,multi-modal out-of-distribution detection
:`information type`,textual information
:issue,LLM hallucinations
:method,selective generation from LLMs
:method,consistency-based uncertainty calibration
:metric,confidence score
:process,visual object extraction
:vehicle,next-generation autonomous vehicle
:capability,dynamic interaction and adaptation
:environment,highwayenv
:task,autonomous driving and tactical decision-making
:capability,driving behavior personalization
:outcome,improved driving decisions
:experience,personalized driving experience
:operation,autonomous vehicle operations
:ecosystem,"user-centric, transparent, and adaptive autonomous driving"
:integration,LLMs into autonomous vehicles
:challenge,deployment demands
:technique,quantization-aware training
:issue,activation outliers
:method,QLLM
:technique,adaptive channel reassembly
:technique,channel disassembly and assembly
:strategy,adaptive strategy for sub-channels
:technique,efficient tuning method
:hardware,A100-80G GPU
:`performance metric`,average accuracy across five zero-shot tasks
:agent,large language model based agent
:framework,GameGPT
:domain,game development
:challenge,LLM deployment in production
:issue,redundancy
:method,mitigation methods
:method,dual collaboration
:method,layered approach
:method,decoupling approach
:outcome,precise code generation
:task,clickbait spoiling
:goal,satisfying curiosity
:dataset,clickbait spoiling corpus
:language,Indonesian
:`model family`,cross-lingual zero-shot question answering-based model
:category,low-resource language
:model,XLM-RoBERTa (Large)
:task,phrase and passage spoiling
:model,mDeBERTa (Base)
:task,multipart spoiling
:task,text entry
:feature,intelligent text entry features
:`model family`,deep learning-based language model
:process,data collection and model fine-tuning
:technique,text prediction
:profession,designers
:agent,Promptor
:method,prompt creation comparison
:outcome,Promptor-designed prompts
:metric,similarity and coherence
:domain,Islamic domain
:country,Indonesia
:demographic,Islamic believer population
:dataset,Question Answering Sirah Nabawiyah (QASINA)
:literature,Sirah Nabawiyah literatures
:dataset,QASINA
:behavior,excessive interpretations
:task,question answering in religious domain
:field,physics-informed machine learning (PIML)
:task,geophysical data inversion
:`algorithm type`,full waveform inversion (FWI)
:method,finite-difference solution
:`algorithm type`,neural network (NN)
:task,model computation
:model,PIML architecture
:advantage,avoiding local minima
:advantage,minimizing extensive training data requirement
:factor,data similarity
:strategy,pretraining and fine-tuning
:capability,self-verification and self-critiquing
:system,planning system with LLM
:task,plan generation and verification
:system,systems with external verifiers
:feedback,binary feedback
:feedback,detailed feedback
:task,authorship verification
:application,forensic analysis
:technique,stylometric analysis
:limitation,data requirements
:technique,authorship verification techniques
:limitation,lack of explainability
:technique,PromptAV
:characteristic,limited training data
:component,attention module
:issue,key-value cache size
:approach,incremental context compression
:baseline,sparse attention
:benefit,system throughput improvement
:URL,https://github.com/drsy/kv_compression
:aspect,LLM internals
:method,sparse autoencoder interpretation
:`model state`,activations
:feature,unique features of learned reward model
:scenario,token-reward mapping learning
:application,sparse autoencoders for interpreting learned rewards
:concept,reward integrity
:goal,alignment of objectives and behaviors
:capability,zero-shot image-to-text generation and understanding
:resource,non-English multi-modal resources
:`model series`,Ziya-Visual
:model,Ziya-Visual-Base
:model,Ziya-Visual-Chat
:`model component`,Querying Transformer
:technique,multi-stage training
:technique,low-rank adaptation module
:capability,multi-modal understanding
:task,zero-shot image-text retrieval
:capability,image-text understanding and generation in Chinese
:resource,"code, demo, and models"
:URL,https://huggingface.co/idea-ccnl/ziya-blip2-14b-visual-v1
:discipline,political science
:task,classifying policy documents
:process,automated text classification
:purpose,social science research
:strategy,alternative text classification strategy
:project,Comparative Agendas Project
:`classification scheme`,21 major policy issue topics
:scenario,use-case scenarios
:result,accuracy range
:observation,insufficiency of minimal human intervention
:observation,increasing accuracy with human effort
:observation,high accuracy in humanly demanding use-case
:strategy,automated coding with manual review
:attribute,performance and intelligence
:field,cognitive research
:concept,knowledge structure of LLMs
:method,educational diagnostic assessment
:activity,evaluation of LLMs
:dataset,MOOCradar
:concept,knowledge structures
:concept,knowledge and cognitive patterns of LLMs
:goal,development and utilization of LLMs
:approach,plan-and-write
:task,long-form narrative text generation
:method,prompting large language models for planning
:outcome,suboptimal results
:framework,evaluation-guided iterative plan extraction for long-form narrative text generation (EIPE-Text)
:process,stages of EIPE-Text
:stage,plan extraction
:process,iterative plan improvement
:mechanism,question answer (QA) based evaluation
:process,plan evaluation and refinement
:goal,better planner construction
:evaluation,GPT-4-based and human evaluations
:outcome,coherent and relevant long-form narratives
:phenomenon,LLM revolution
:`use case`,content moderation filters
:goal,unified embedding model
:approach,dedicated embedding models
:task,universal embedding
:evaluation,English MTEB
:task,embedding tasks
:benchmark,multilingual classification
:benchmark,code search
:model,unified embedder
:goal,cross-task and cross-language application
:data,image-text pairs
:cost,training and data collection/curation costs
:paradigm,vendor-client
:goal,minimize inference cost
:metric,in-domain accuracy
:method,"VL2V-ADIP (Vision-Language to Vision-Align, Distill, Predict)"
:metric,domain generalization benchmarks
:hypothesis,reliance on co-occurrence statistics
:bias,co-occurrence bias
:behavior,preferring frequently co-occurred words
:task,recalling rare co-occurrence facts
:action,scaling up model sizes or finetuning
:technique,debiased finetuning
:task,memorizing rare facts
:task,recalling unseen rare facts
:`research direction`,mitigation research
:goal,reliable language models
:url,https://github.com/cheongwoong/impact_of_cooccurrence
:tool,LATTE
:tools,state of the art static binary taint analyzers
:process,taint propagation and vulnerability inspection
:task,vulnerability detection
:outcome,new bugs and CVEs
:value,low engineering cost
:community,security researchers and practitioners
:field,vulnerability analysis for binary programs
:goal,infer missing connections
:approach,text-based knowledge graph completion
:approach,graph embedding methods
:factor,quality of entity textual descriptions
:task,generating effective text
:problem,hallucination in LLM-generated text
:technique,constraint-based prompt
:factor,contextual constraints
:method,Constrained-Prompt Knowledge Graph Completion (CP-KGC)
:outcome,effective inference under low resource conditions
:benchmark,WN18RR and FB15K237 datasets
:integration,LLMs in KGC tasks
:task,knowledge base construction
:event,International Semantic Web Conference 2023
:`challenge track`,Track 1
:constraint,1 billion parameters maximum without entity descriptions
:capability,multi-token prediction
:model,Vocabulary Expandable BERT
:technique,task-specific re-pre-training
:framework,our framework
:model,BERT-base
:model,ChatGPT-3
:technique,token-recode
:technique,re-pre-training
:practice,equal treatment of demonstration examples
:research,determining optimal weights for demonstration examples
:metric,masked self-prediction score
:task,weight quality assessment
:performance,in-context learning performance
:process,weight-searching
:strategy,application of weights to demonstrations
:result,performance improvement on text classification tasks
:task,text classification tasks
:technique,multi-stage text retrieval
:`time period`,pre-trained language model era
:study,existing studies on text retrieval
:`model family`,pre-LLM models
:role,dense retriever and pointwise reranker
:model,ReplLaMDA
:task,passage and document retrieval
:model,RankLaMDA
:dataset,MS MARCO
:technique,segmenting and pooling strategies
:pipeline,ReplLaMDA-RankLaMDA
:benchmark,BEIR
:`model checkpoints`,study's model checkpoints
:expert,domain expert
:knowledge,recent biological knowledge
:activity,therapeutic decision-making
:challenge,biomedical data analysis
:data,biomedical data
:entity,biomedical entities
:`knowledge graph`,large-scale knowledge graph
:source,structured and unstructured sources
:activity,knowledge deduction
:ontology,OncoNet Ontology (ONO)
:relation,gene-disease relation
:tool,information extractors
:phenomenon,concept drift
:activity,diagnosis and treatment
:`content type`,image-text meme
:`communication form`,multimodal communication
:`content type`,hateful meme
:issue,discrimination
:task,hateful meme detection
:approach,ISSUES
:technique,textual inversion
:benchmark,Hateful Memes Challenge
:dataset,Harmeme
:resource,code and pre-trained models
:issue,factual inconsistency
:component,feed-forward network
:function,factual knowledge expression
:method,knowledge enhancement
:model,K-Dial
:component,extended feed-forward network
:method,Reinforcement Learning for Factual Consistency (RLFC)
:metric,NLI-based metrics
:aspect,factual consistency and dialogue quality
:dataset,WoW
:dataset,CMU_DoG
:task,instruction-following evaluation
:`evaluation method`,prompt-based approaches
:task,correctness verification
:metric,instruction-following metrics
:dataset,RISUM
:task,grounded query-based summarization
:`evaluation method`,evaluation methods
:`evaluation method`,LLM-based reference-free evaluation methods
:`evaluation method`,reference-based metrics
:quality,high-quality summaries
:task,question generation over knowledge bases
:process,conversion of logical form to natural language question
:capability,few-shot generalization
:method,KQG-COT
:data,unlabeled data pool
:technique,reasoning chain prompting
:method,KQG-COT+
:benchmark,prompting baselines
:benchmark,few-shot SOTA results
:dataset,PathQuestions
:issue,absence of standard guidelines
:outcome,methodological inconsistencies
:method,literature search
:data,medical literature databases
:analysis,meta-analysis
:data,17 studies
:metric,integrated accuracy
:study,studies
:aspect,methodological details
:issue,insufficient reporting
:outcome,reliability of results
:recommendation,well-designed studies
:vulnerability,adversarial jailbreaks
:feature,safety guardrails
:algorithm,Prompt Automatic Iterative Refinement (PAIR)
:technique,semantic jailbreaks
:technique,social engineering attacks
:model,attacker large language model
:technique,jailbreaks
:metric,jailbreaking success rates
:role,brain of an agent
:agent,multi-modal agent
:task,complex challenges
:issue,model selection neglect
:method,traditional model selection
:framework,M3
:goal,improving model selection and robustness
:dataset,MS-GQA
:challenge,model selection in multi-modal agents
:feature,dynamic model selection
:resource,code and benchmark
:model,target model
:technique,DistillSpec
:technique,on-policy data generation
:technique,divergence function tailoring
:technique,lossy speculative decoding
:task,multimodal model editing
:value,scrutiny and careful consideration
:benchmark,MMEdit
:concept,model editing baselines
:task,editing multimodal LLMs
:effect,editing effect
:modality,"image, video, audio"
:`data type`,structured graph data
:`data type`,unstructured text data
:knowledge,graph knowledge
:dataset,GraphExtQA
:task,graph-language model evaluation
:model,CrossGNN
:task,answer generation
:technique,cross-attention
:`model family`,graph-language model
:capability,graph understanding and utilization
:experiment,language-only model vs graph-language model
:task,evaluating long-form responses
:issue,proprietary LLM limitations
:model,Prometheus
:dataset,Feedback Collection
:benchmark,human preference benchmarks
:resource,Prometheus code and dataset
:process,instruction-based data curation
:field,materials science
:model,Honeybee
:milestone,first billion-parameter model for materials science
:module,instructor module
:module,verifier module
:task,data verification
:dataset,MATSrCI-Instruct dataset
:benchmark,MATSrCI-NLP benchmark
:task,materials science tasks
:process,instruction-data refinement
:url,https://github.com/banglab-udem-mila/nlp4matsci-honeybee
:task,preference classification
:`model architecture`,Graph Neural Network
:model,Graph Attention Network
:task,comparative preference classification (CPC)
:method,experiment design and conduct
:characteristic,text length
:algorithm,gradient descent
:setting,realistic natural language
:weakness,prior works on ICL and GD analogy
:method,transformer weight simulation
:analysis,empirical analysis
:`performance metric`,various performance metrics
:comparison,ICL vs. GD
:behavior,output distribution adaptation
:hypothesis,equivalence between ICL and GD
:status,open hypothesis
:activity,further studies
:`cognitive process`,inductive reasoning
:technique,iterative hypothesis refinement
:process,three-step process
:role,hypothesis proposer
:approach,hybrid approach
:benchmark,inductive reasoning benchmarks
:task,rule induction and application
:entity,human inductive reasoning
:analysis,empirical and human analyses
:discrepancy,LM and human inductive reasoning
:`potential and limitation`,using LMs in inductive reasoning tasks
:limitation,limited context windows
:technique,virtual context management
:capability,extended context usage
:concept,hierarchical memory systems
:system,MemGPT
:concept,interrupts
:domain,multi-session chat
:resource,MemGPT code and data
:URL,https://memgpt.ai
:task,visual semantic content recognition
:task,visual data-type identification
:application,data curation and autonomous vision
:dataset,animal images dataset
:evaluation,zero-shot evaluation
:performance,marginal gains in visual data-type identification
:model,OpenAI Flamingo
:performance,drop in visual data-type identification
:`blind spot`,visual data-type understanding
:method,pre-training distribution analysis and fine-tuning
:performance,enhanced visual data-type identification
:URL,https://github.com/bethgelab/datatypeidentification
:concept,close-loop task planning
:paradigm,prompting large language models for task planning
:qualities,performance and user-friendliness
:issues,inefficiencies
:model,Tree-Planner
:technique,three-phase task planning
:outcome,state-of-the-art performance and efficiency
:metric,token consumption
:metric,error corrections
:capability,multimodal perception and reasoning
:entity,embodied agent
:capability,formulating plans and executing commands
:model,Octopus
:capability,deciphering vision and textual task objectives
:process,generating training data
:environment,Octoverse
:`training scheme`,Reinforcement Learning with Environmental Feedback (RLEF)
:capability,functionality and decision-making
:community,embodied AI community
:resource,"open-source model architecture, simulator, and dataset"
:technique,LoRA fine-tuning
:framework,LoFT-Q
:goal,generalization in downstream tasks
:technique,existing quantization methods
:resource,LoFT-Q code
:task,visual navigation
:approach,fine-tuning LLMs for visual navigation
:component,history collector model
:data,previous observations
:output,probability distribution of actions
:data,Habitat-Matterport 3D dataset (HM3D)
:benchmark,state-of-the-art behavior cloning methods
:metric,collision rates
:study,financial reasoning capabilities assessment
:capability,financial reasoning
:evaluation,CFA exam questions evaluation
:exam,CFA exam
:field,finance
:issue,switching between generating and verifying code
:misalignment,programmers' prompts and generated code
:`model family`,large language model-driven code assistant
:process,prompt authoring
:workflow,programming with LLMs
:method,iterative design process
:strategy,programmers' strategies
:system,CoLADDER
:profession,programmers
:`user study`,with experienced programmers
:ability,navigate and edit code
:concept,abstraction levels
:task,joint language modeling
:`language form`,speech and text
:method,speech tokenization
:data,discrete speech units
:method,mixed speech-text data construction
:data,mixed speech-text data
:model,joint language model
:task,spoken language understanding (SLU)
:concept,shared representations
:baseline,speech-only baseline
:capability,zero-shot cross-modal transferability
:task,high-level goal execution
:environment,MiniWoB++
:method,supervised learning
:resource,trace examples
:method,few/many-shot prompting
:challenge,autonomous learning without expert traces
:capability,performing new tasks
:agent,zero-shot agent
:resource,expert traces
:task,executable action planning
:method,self-reflection and structured thought management
:benchmark,MiniWoB++ easy tasks
:benchmark,MiniWoB++ complex tasks
:criticism,task-specific circuits
:evidence,generalizability of insights
:concept,task-general algorithms
:circuit,indirect object identification (IOI) circuit
:task,colored objects identification
:experiment,intervention experiment
:`subcircuit behavior`,invariant to task inputs
:technique,text embedding
:data,query-corpus paired data
:method,Search-Adaptor
:dataset,real-world English and multilingual retrieval datasets
:api,Google Embedding APIs
:benchmark,BEIR datasets
:property,compositional nature
:`model family`,audio-language model
:ability,compositional reasoning
:benchmark,COMPA
:benchmark,COMPA-Order
:benchmark,COMPA-Attribute
:performance,random chance
:model,COMPA-CLAP
:technique,composition-aware hard negatives
:technique,modular contrastive loss
:approach,prompt-based approach
:issue,hidden stereotypes
:issue,encoded biases
:value,transparency and fairness
:concept,training and inference flexibility
:method,collecting large amounts of annotated image-text pairs
:`model type`,text-conditioned generative model
:characteristic,expensive and inefficient
:approach,leveraging pre-trained vision-language models
:goal,avoid data collection
:limitation,per text-prompt optimization or inference-time hyper-parameters tuning
:concept,CLIP DeltaSpace
:alignment,semantic alignment of visual and textual feature differences
:framework,DeltaEdit
:advantage,text-free training and zero-shot inference
:model,GAN and diffusion model
:resource,DeltaEdit code
:URL,https://github.com/yueming6568/deltaedit
:challenge,inefficiency in real-world deployment
:technique,task-agnostic distillation
:method,output distribution transfer
:method,hidden state transfer
:method,multi-head attention transfer
:model,MiniLMv2
:effectiveness,distillation methods
:application,latency-critical applications
:model,student models
:`model component`,visual perception interface
:component,visual branch
:model,DINO
:technique,MLP layer for alignment
:strategy,COMM
:task,visual benchmarks
:resource,code for COMM
:challenge,complex software systems
:task,managing code errors
:challenge,accelerating development cycles
:task,implementing business logic
:technique,traditional software quality assurance
:task,handling intricate business logic
:concept,intelligent code analysis agent (ICAA)
:metric,bug detection accuracy
:value,66% false-positive rate
:metric,recall rate
:value,60.8% recall rate
:challenge,token consumption cost
:field,software quality assurance
:methodology,case-based reasoning
:technique,computational technique
:community,CBR researchers
:development,technical developments in AI
:event,AI breakthroughs
:concept,persistent memory
:algorithm,new algorithm for few-shot molecular property prediction
:task,new property prediction
:benchmark,FS-Mol
:benchmark,BACE
:`algorithm family`,meta-learning algorithms
:benchmark,best methods
:framework,interactive navigation framework
:component,action-aware costmap
:model,Grounding DINO
:system,end-to-end system
:data,textual instructions
:component,bounding boxes
:object,traversable objects
:task,interactive navigation
:scenario,medical scenario
:task,code summarization
:benchmark,clean APR benchmark
:dataset,competitive programming problems
:workflow,dialog-based repair
:task,reevaluation of black-box LLM achievements
:task,task-oriented dialogue systems
:framework,InstructTODs
:component,proxy belief state
:task,dynamic query generation
:benchmark,fully fine-tuned TODs
:task,TODs subtasks
:resource,code and implementations
:URL,https://github.com/willyhc22/instructtods/
:technique,next-token prediction
:result,impressive AI results
:process,fine-tuning through human demonstrations
:reliance,human oversight
:goal,AI innovation
:paradigm,exploratory AI
:goal,autonomous high-quality training data generation
:technique,unsupervised reinforcement learning pretraining
:task,novelty assessment
:component,actor
:task,novel content generation
:task,content evaluation and guidance
:attribute,cooperative capabilities measurement
:game,Diplomacy
:game,Welfare Diplomacy
:strategy,balance of military and welfare
:attribute,cooperative capabilities assessment and training
:tool,open-source diplomacy engine
:technique,zero-shot prompted language models
:agent,baseline agents
:goal,societal safety
:URL,https://github.com/mukobi/welfare-diplomacy
:content,human-like content
:concern,abuse of large language models
:detector,AI-generated text detector
:task,combating abuse
:challenge,sentence-level AI-generated text detection
:dataset,sentence-level AI-generated text detection dataset
:content,documents with mixed authorship
:method,SeqXGPT
:feature,log probability lists
:`model architecture`,convolution and self-attention networks
:challenge,AI-generated text detection
:concept,human intervention
:capability,human cognitive abilities
:pipeline,human-in-the-loop
:task,customized output generation
:interaction,human-machine interaction
:resource,external database
:task,German-English translation
:evaluation,translation performance
:task,on-device deployment
:era,LLMs era
:technique,dynamic sparse no training (DSNOT)
:challenge,costly fine-tuning of LLMs
:technique,dynamic sparse training
:metric,reconstruction error
:process,iterative weight pruning-and-growing
:`time complexity`,linear time
:experiment,extensive experiments on LLMs
:model,LLAMA-7B
:resource,DSNOT code
:URL,https://github.com/zyxxmu/dsnot
:technique,text watermarking
:issue,text credibility
:method,EasyMark
:goal,maintaining text meaning
:process,watermark detection
:characteristic,implementation simplicity
:requirement,LLM access
:metric,detection accuracy and BLEU scores
:theorem,impossibility theorem of perfect watermarking
:experiment,EasyMark detection
:task,decision-making and planning
:concept,world conditions
:capability,continuous environmental knowledge acquisition and adaptation
:approach,LLAMA-Rider
:goal,improving task-solving capabilities
:mechanism,multi-round feedback-revision
:technique,sub-task relabeling
:technique,exploration experience-based training
:capability,task accomplishment
:issue,high instability
:factor,random selection of examples
:concept,information gain
:capability,informative ability of data examples
:strategy,sampling with maximum information gain
:issue,template bias
:process,evaluation of information gain
:strategy,calibration before sampling
:metric,average relative improvement
:task,classification tasks
:model,EasyGen
:capability,multimodal understanding and generation
:model,BiDiffuser
:capability,efficient multimodal interactions
:task,image-to-text generation
:evaluation,quantitative and qualitative experiments
:setting,lab
:URL,https://github.com/zxy556677/easygen
:`metric type`,reference-free learned metric
:data,dialogue data with high-quality human annotations
:`study focus`,english dialogues
:benchmark,multilingual dialogue evaluation benchmark
:benchmark,XDial-Eval
:data,English dialogue datasets
:task,multilingual extension
:`model family`,BERT-based metric
:baseline,self-supervised and multilingual baselines
:baseline,best baseline
:resource,data and code for XDial-Eval
:url,https://github.com/e0397123/xdial-eval
:task,knowledge base question answering
:resource,knowledge bases
:challenge,inefficient knowledge retrieval
:challenge,retrieval errors
:challenge,complexity of previous KBQA methods
:framework,ChatKBQA
:method,generate-then-retrieve
:paradigm,combining LLMs with knowledge graphs
:task,knowledge-required question answering
:task,simpler programming tasks
:task,complex programming tasks
:issue,monolithic code blocks
:professional,experienced programmers
:practice,modularized code
:framework,CodeChain
:goal,modularized code generation
:activity,research on MLLMs
:`evaluation study`,existing MLLM evaluations
:`content type`,unimodal (vision) content
:`content type`,multimodal (vision-language) content
:task,multimodal content comprehension
:concept,multimodal context understanding
:`assessment framework`,MM-BIGBench
:metric,best performance metric
:metric,mean relative gain metric
:metric,stability metric
:research,previous research on MLLMs
:concept,adaptability between models and instructions
:metric,adaptability metric
:evaluation,MM-BIGBench evaluation
:outcome,novel insights
:resource,MM-BIGBench code
:risk,hallucination problem
:approach,knowledge retrieval and fine-tuning
:limitation,high training costs
:method,knowledge-constrained tree search (KCTS)
:goal,reducing hallucinations in natural language generation
:method,Monte-Carlo Tree Search (MCTS)
:method,token-level hallucination detection
:method,RIPA (Reward Inflection Point Approximation)
:task,token-level guidance
:`application domain`,intelligent mobile networks
:`model family`,specialized AI model
:task,complicated AI tasks
:framework,Systematic Artificial Intelligence (SAI)
:task,AI tasks
:component,multi-input component
:module,model library module
:information,model information
:process,receiving user network requirements
:task,subtasks
:capability,language capabilities
:task,network optimization
:`application domain`,communication network
:method,direct pre-training
:issue,overconfident predictions
:method,domain-specific continued pre-training
:method,direct preference optimization
:dataset,Chinese Medicine (ChiMed) dataset
:model,Qilin-Med
:test,CMExam
:test,Huatuo-26M
:ability,general language understanding
:benchmark,GloRE
:method,self-consistency probing
:method,fine-tuned method
:resource,datasets and evaluation programs
:`business model`,embedding-as-a-service (EaaS)
:process,direct text transmission
:framework,Split-n-Denoise (SND)
:process,noise introduction and denoising
:stage,inference stage of LLMs
:concept,privacy-utility tradeoff
:solution,privacy-preserving solution
:method,generative querying
:method,discriminative querying
:procedure,language model decoding
:concept,consensus game
:role,generator and discriminator
:algorithm,equilibrium-ranking
:field,game theory
:challenge,truthfulness and consistency in language models
:language,Setswana
:model,PuoBERTa
:corpus,high-quality corpus for PuoBERTa
:dataset,Setswana news categorisation dataset
:capability,language generation and generalization
:issue,unreliable content in practical reasoning
:task,event relation extraction
:strategy,endowing LLMs with logical reasoning
:dataset,LLM-LR
:task,multi-hop reasoning evaluation and pre-training
:strategy,teaching LLMs with logic
:insight,for solving practical tasks with LLMs
:capability,handling a broader spectrum of tasks
:data,existing tuning data
:issue,inadequate domain coverage
:approach,Explore-Instruct
:goal,domain-specific instruction coverage
:concept,domain use cases
:technique,search algorithm
:analysis,data-centric analysis
:model,Explore-Instruct model
:baseline,multiple baselines
:resource,"code, model weights, and data"
:`document type`,recommendation letters
:application,writing assistance
:bias,underlying biases in LLMs
:harm,societal harms
:issue,fairness in LLM-generated documents
:task,comprehensive study
:bias,hallucination bias
:phenomenon,bias exacerbation
:model,ALPACA
:warning,against using LLMs without scrutinization
:value,public well-being and societal trust
:task,manual fact-checking
:issue,misinformation proliferation
:framework,Fact-GPT
:task,claim matching in fact-checking
:resource,labeled dataset
:process,fine-tuning LLMs
:resource,social media content dataset
:resource,datasets and models
:task,click-through rate prediction
:application,internet applications
:`model type`,traditional CTR models
:technique,one-hot encoding
:problem,semantic information loss
:issue,collaborative information capture failure
:issue,inference inefficiency
:framework,ClickPrompt
:technique,soft prompts generation
:task,prompt-augmented masked language modeling (PA-MLM)
:knowledge,collaborative and semantic knowledge
:interface,prompt interface
:`model type`,CTR model
:capability,code-generation
:method,natural language programming
:activity,collaborative programming
:challenge,prompt engineering in collaborative settings
:study,formative study on collaborative NL programming
:prototype,CoPrompt
:activity,collaborative prompt engineering
:`user study`,CoPrompt user study
:data,vast data
:concept,self-perceived bias
:`social group`,indigenous people
:task,scenario simulation
:concept,technology perception
:concept,societal bias
:field,social computing
:field,critical computing
:task,legal judgment prediction
:field,legal AI
:concept,precedents
:framework,precedent-enhanced LJP framework
:concept,in-context precedents comprehension
:data,real-world dataset
:collaboration,LLM and domain-model collaboration
:`model family`,GPT family
:trend,reducing inference costs
:goal,efficient local computation
:task,memory-bound generative setting
:problem,general quantization
:task,batched inference or prompt processing
:strategy,QUIK
:goal,practical speedups with good accuracy
:performance,end-to-end throughput improvements
:`model family`,OPT family
:`model family`,LaMDA-2 family
:`model family`,FALCON family
:technique,quantization plus 2:4 sparsity
:goal,accurate inference
:URL,https://github.com/ist-daslab/quik
:capability,following instructions and task performance
:task,table-understanding tasks
:`data format`,one-dimensional natural-language texts
:`data format`,two-dimensional relational tables
:paradigm,table-tuning
:goal,enhancing table-understanding capabilities
:model,Table-GPT
:goal,classifying relationships
:scope,document-level relation extraction
:scope,sentence-level relation extraction
:method,traditional relation extraction
:resource,human-annotated training data
:approach,weakly-supervised relation extraction
:challenge,imbalance and probing failure
:method,PromptRE
:concept,prior knowledge
:goal,relation classification
:dataset,ReDocReD
:task,compositional image retrieval
:goal,retrieve target image
:approach,supervised compositional image retrieval
:method,annotating triplets
:task,zero-shot compositional image retrieval
:task,training task-specific models
:pipeline,Compositional Image Retrieval through Vision-by-Language (CIREVL)
:value,human-understandability
:technique,inductive loop invariants synthesis
:task,loop invariants synthesis
:issue,high number of verifier calls
:component,ranker
:task,distinguishing inductive invariants
:ranker,contrastive ranker
:outcome,improved invariant ranking
:goal,reducing verifier calls
:challenge,coherence and informativeness in responses
:task,single-hop evidence aggregation
:task,multi-hop reasoning over dialogue context
:method,Dialogue Chain-of-Thought (CoT) reasoning
:framework,knowledge distillation framework
:technique,alignment filters
:tool,DoCoT
:capability,reliable CoT rationales
:agent,dialogue agent
:outcome,improved response quality
:task,neural information retrieval
:method,synthetic annotation
:technique,query generation
:technique,rule-based string manipulation
:dataset,MS-MARCO
:scenario,unsupervised domain adaptation
:representation,identity representation
:technology,pre-trained language model encoder
:aspect,contextual item description
:scenario,"few-shot, zero-shot, and unified modeling"
:signal,collaborative filtering
:challenge,collaborative contextualization
:integration,collaborative signals with contextual representations
:challenge,representation gap
:representation,ID-based and contextual representations
:model,CollabContext
:alignment,representations within contextual space
:performance,recommendation performance in cold-start scenarios
:data,three real-world datasets
:model,speech augmented language model
:component,frozen text large language model
:component,audio encoder
:component,modality adapter module
:component,LoRA layers
:task,speech translation
:capability,zero-shot in-context learning
:technique,speech supervised in-context training
:goal,bridging training gap
:capability,in-context learning ability
:toolkit,NeMo
:dataset,ReClor-Plus
:dataset,LogiQA-Plus
:dataset,LogiQAv2-Plus
:experiment,logical reasoning dataset performance
:technique,task variation
:technique,logic-driven data augmentation
:resource,source code and data
:competition,document-based visual question answering
:task,automatic detection of parent-child relationships
:task,identifying answering elements
:institution,Polito
:approach,text-only approach
:technique,masked language modeling
:strategy,ad hoc sampling
:metric,high performance
:task,high-level planning
:technique,LLM-guided planning
:assumption,key assumptions
:approach,LLM-guided Teacher-Student Learning (LGTS)
:algorithm,teacher-student learning
:domain,gridworld based doorkey
:experiment,graphical sub-goals structure
:domain,search-and-rescue inspired
:experiment,teacher-student learning effectiveness
:interface,general interface for language-related applications
:objective,unified interface for vision-language tasks
:challenge,single model for diverse vision-language tasks
:model,MiniGPT-v2
:method,unique identifiers for tasks
:benchmark,visual question-answering and visual grounding benchmarks
:`model family`,vision-language generalist models
:resource,MiniGPT-v2 model and codes
:url,https://minigpt-v2.github.io/
:task,zero-shot document ranking
:approach,pointwise prompting
:approach,pairwise prompting
:approach,listwise prompting
:approach,prompting approaches
:value,effectiveness
:approach,setwise prompting
:dataset,TREC DL
:benchmark,BEIR zero-shot document ranking
:task,text generation tasks
:issue,high inference latency
:technique,model efficiency improvement
:method,Hessian sensitivity-aware mixed sparsity pruning
:concept,adaptive sparsity allocation
:field,3D representation learning
:trend,transferring 2D alignment strategies to 3D
:challenge,information degradation
:challenge,insufficient synergy
:challenge,underutilization
:approach,JM3D
:component,Structured Multimodal Organizer (SMO)
:component,Joint Multi-modal Alignment (JMA)
:model,JM3D-LLM
:dataset,ModelNet40
:dataset,ScanObjectNN
:resource,JM3D code and models
:URL,https://github.com/mr-neko/jm3d
:method,random-shuffled training
:approach,structured cognitive learning
:dataset,highly structured synthetic dataset
:framework,educational frameworks
:framework,Bloom's Taxonomy
:metric,+3.06 performance enhancement
:challenge,AI2 Reasoning Challenge (hard set)
:metric,+1.28 performance enhancement
:approach,cognitive rigorous training
:principle,human learning principles
:capability,comprehending and responding to complex instructions and tasks
:issue,problematic text generation
:procedure,Reward-Augmented Decoding (RAD)
:attribute,desired text properties
:model,unidirectional reward model
:benefit,reduced computational overhead
:comparison,generation procedure modification methods
:task,non-toxic text generation
:task,sentiment-controlled text generation
:task,following natural language instructions
:task,domain-specific question answering
:factor,training data collection time
:factor,complex user utterance
:factor,wrong retrieval
:issue,unexpected and unsafe answers
:system,CarExpert
:task,in-car conversational question-answering
:benchmark,state-of-the-art large language models
:evaluation,comprehensive empirical evaluation
:language,ancient Chinese
:benchmark,ACLUE
:capability,comprehending ancient Chinese
:task,language comprehension tasks
:evaluation,performance disparity
:language,ancient Chinese vs modern Chinese
:task,language model detoxification
:goal,safer deployment
:`method category`,finetuning-based detoxification
:`method category`,decoding-based detoxification
:characteristic,additional components
:approach,self-detoxification
:observation,negative steering prompt
:outcome,toxic content generation
:concept,information stream
:method,toxification direction identification
:experiment,self-detoxification performance evaluation
:outcome,comparable performance
:`knowledge type`,common-sense human knowledge
:concept,penetrative AI
:interaction,interaction with the physical world
:capability,processing sensory signals
:task,interpreting IoT sensor data
:application,new applications
:integration,human knowledge in cyber-physical systems
:community,data visualization practitioners
:skill,formal training in visualization design
:goal,addressing knowledge gap in visualization design
:forum,VisGuide
:task,data visualization question answering
:study,user study on ChatGPT as a visualization assistant
:source,human experts and ChatGPT
:method,experience surveys and post-interviews
:capability,wide range of design options
:capability,depth and critical thinking
:evaluation,robustness evaluation
:requirement,comprehensive encapsulation
:methodology,ASSERT
:method,semantically aligned augmentation
:method,target bootstrapping
:method,adversarial knowledge injection
:evaluation,test suite generation
:setting,semantic equivalence
:setting,related scenarios
:setting,adversarial
:analysis,safety domain partitioning
:evaluation,safety evaluation
:performance,classification accuracy
:setting,zero-shot adversarial
:concern,physical safety
:challenge,memory demand
:method,gradient-based training
:method,zeroth-order optimization
:benefit,reduced memory consumption
:challenge,dimension-dependent complexity
:algorithm,DPZero
:analysis,theoretical analysis
:feature,dimension-independent rates
:application,real-world LLM deployments
:submission,best performing submission
:event,Arabic AI Tasks Evaluation Challenge (AraIEval) at ArabicNLP 2023
:task,Task 1: Identifying persuasion techniques
:technique,persuasion technique detection
:model,XLM-RoBERTa
:approach,fine-tuning multilingual language model
:metric,Micro F1 score
:task,Subtask A
:activity,model testing
:task,creating test cases
:step,identifying what to test
:tool,Weaver
:task,requirements elicitation
:`user study`,Weaver user study
:context,real-world settings
:scenario,code understanding
:scenario,transcript summarization
:application,embodied instruction following
:task,embodied tasks with multimodal prompts
:capability,robot multimodal understanding
:framework,effective framework for robot manipulation with multimodal prompts
:task,robot manipulation
:`training pipeline`,two-stage training pipeline
:technique,inverse dynamics pretraining
:technique,multi-task finetuning
:component,multimodal prompt encoder
:benchmark,VIMA-Bench
:field,protein engineering
:goal,engineering new molecules
:problem,de novo design
:`model family`,generative architecture
:`model type`,diffusion process
:task,generating novel proteins
:protocol,state-of-the-art design protocol
:metric,experimental success rate
:product,de novo designed proteins
:challenge,in silico metrics prioritization
:challenge,designing dynamic proteins
:increase,number of models
:need,framework for model understanding
:concept,biochemical knowledge
:attribute,model performance and interpretability
:issue,software failures
:cause,misconfigurations
:technique,configuration validation
:method,manually written rules or test cases
:challenge,configuration validation using ML and NLP
:requirement,large-scale configuration data and system-specific features
:framework,CIRI
:technique,prompt engineering with few-shot learning
:issue,hallucination and nondeterminism of LLMs
:evaluation,validation effectiveness of CIRI
:finding,potential of LLMs for configuration validation
:challenge,ineffectiveness in detecting certain types of misconfigurations
:task,query rewriting
:application,conversational search
:approach,human-rewritten queries
:method,query rewriting model training
:limitation,insufficient information in human rewrites
:performance,retrieval performance
:role,query rewriter
:property,well-formed rewrites
:instruction,query rewrite instruction
:role,rewrite editor
:process,rewrite-then-edit
:goal,reducing rewriting latency
:dataset,QReCC
:technique,informative query rewrites
:concept,real-world knowledge
:benchmark,KGQuiz
:ability,knowledge generalization
:data,triplet-based knowledge
:domain,knowledge domains
:evaluation,KGQuiz benchmark evaluation
:performance,LLMs' performance on KGQuiz
:tool,testbed
:goal,understanding LLMs' knowledge abilities
:model,IndoBERT
:domain,financial domain
:corpus,Indonesian financial corpus
:corpus,Indonesian self-supervised financial corpus
:dataset,Indonesian financial sentiment analysis dataset
:dataset,Indonesian financial topic classification dataset
:task,topic classification
:`model family`,BERT for financial NLP
:process,domain-specific post-training
:`research area`,example selection
:approach,textual similarity-based retrieval
:outcome,sub-optimal performance
:approach,LAIL
:task,example estimation
:component,retriever
:task,relational reasoning
:skill,complex abilities
:task,regression
:resource,large quantities of training data
:task,next-token-prediction
:phenomenon,inverse scaling law
:modification,trainable parameters per head
:goal,veracity probing
:scenario,multi-hop fact verification
:dataset,previous datasets
:issue,simplification and lack of explainability
:dataset,EX-FEVER
:claim,EX-FEVER claims
:source,Wikipedia documents
:instance,EX-FEVER instance
:component,veracity label and explanation
:system,novel baseline system
:`model family`,existing fact verification models
:approach,multi-modal large language model utilization
:task,road network generation
:technique,unique training methodology
:paper,LISA
:technique,reasoning segmentation method
:technique,binary segmentation masks
:outcome,precise navigational guidance
:system,autonomous navigation systems
:technique,mixture of experts
:principle,divide-and-conquer
:problem,performance degeneracy
:problem,homogeneous representation
:outcome,high similarities in representations
:solution,OMoE
:strategy,alternating training
:outcome,orthogonal updates
:algorithm,orthogonal expert optimizer
:attribute,representation diversity
:benchmark,NLP benchmarks and tasks
:`text type`,legal text
:population,Indian population
:task,legal text translation
:`research area`,translation to and between Indian languages
:corpus,legal parallel corpus
:language,Indian languages
:`model family`,machine translation system
:profession,law practitioners
:metric,automatic MT evaluation metrics
:field,multimodal language generation
:modalities,language and vision
:task,complex linguistic understanding tasks
:framework,Visual-Language models as Importance Sampling Weights (VLIS)
:capabilities,visual conditioning and language understanding
:metric,pointwise mutual information
:technique,importance sampling weight
:task,diverse vision-language tasks
:task,commonsense understanding
:dataset,"WHOOPS, OK-VQA, ScienceQA"
:task,complex text generation
:dataset,"Concadia, Image Paragraph Captioning, ROCStories"
:system,dialogue system
:task,intent detection
:technique,adaptive pre-training
:challenge,data collection cost
:information,conversational responses of agents
:framework,RSVP
:task,response retrieval
:study,case studies
:task,vulnerability tasks
:task,vulnerability prediction
:`model family`,language models for software vulnerability
:assessment,empirical assessment
:skill,domain-specific expertise
:model,CodeBERT
:resource,chatgpt4vul GitHub repository
:data,studied dataset
:function,knowledge base
:metric,Model Knowledge Reliability Score (MoNITOR)
:attribute,factual reliability
:process,probability distribution comparison
:experiment,evaluation of 12 LLMs
:dataset,Factual Knowledge Test Corpus (FKTC)
:goal,research advancement
:`model scaling`,language model scaling
:outcome,advancements in NLP tasks
:architecture,sparse mixture of experts (MoE)
:approach,merging experts into one (MEO)
:component,token-level attention block
:benchmark,GLUE benchmark
:url,https://github.com/shwai-he/meo
:concept,text-attributed graph
:domain,web domains
:methodology,word embedding models
:task,acquiring text representations
:attribute,text attributes of graph data
:context,graph learning tasks
:paradigm,ENG
:task,empowering text-attributed graphs
:task,node generation
:component,edge predictor
:dataset,ogbn-arxiv
:`evaluation metric`,1-shot setting performance
:technique,content-based recommendation
:`data type`,item content information
:issue,resource-intensive training
:technique,dataset condensation
:outcome,small informative dataset
:method,dual-level condensation method
:level,content-level condensation
:level,user-level condensation
:paradigm,condensation paradigm
:study,empirical findings
:performance,original performance
:dataset,MIND
:problem,open problem
:literature,existing literature
:method,iterative demonstration selection
:technique,majority voting
:benchmark,ICL demonstration selection methods
:condition,dementia
:symptom,language disorders
:`model family`,moderately-sized language model
:pattern,linguistic disorder patterns
:pattern,linguistic patterns
:tool,digital linguistic markers
:metric,communication quality and language disorder intensity
:tool,communication marker
:symptom,dementia speech
:approach,existing linguistic approaches
:metric,clinical markers of behaviour
:tool,linguistic disorder markers
:symptom,gradual language impairment
:task,zero-shot cross-lingual generation
:problem,generation in wrong language
:model,mT5
:model,mBART
:model,NLLB-200
:approach,full finetuning
:approach,parameter-efficient tuning with adapters
:approach,training on several source languages
:task,zero-shot species recognition
:concept,scientific names
:data,CLIP's training set
:technique,descriptive prompts
:outcome,marginal gains
:concept,common English names
:technique,common name prompting
:resource,benchmarking datasets
:limitation,left-to-right generation
:concept,bidirectional context
:model,Fill-in Language Model (FiLM)
:concept,flexible generation
:task,text infilling
:method,left-to-right language models with rearranged text segments
:process,training from scratch or fine-tuning
:`model family`,strong left-to-right language model
:quality,text generation quality
:concept,computational demands
:system,Chameleon
:architecture,disaggregated architecture
:metric,speedup and energy efficiency
:metric,latency and throughput
:hardware,FPGA
:component,retrieval accelerators
:task,LM inference
:hardware,CPU server
:approach,BOSS
:library,learned skill library
:requirement,expert supervision
:technique,skill bootstrapping
:outcome,complex and useful behaviors
:experiment,BOSS in household environments
:performance,zero-shot execution of unseen tasks
:approach,naive bootstrapping
:approach,unsupervised skill acquisition methods
:website,clvrai.com/boss
:paradigm,decomposed question-answering
:technique,syntactic augmentation
:technique,syntactic prompting
:strategy,two-stage majority voting
:analysis,comprehensive error analysis
:resource,high-performance GPU clusters
:`training period`,long training periods
:timeframe,months
:challenge,uninterrupted and long-duration training
:event,hardware and software failures
:`training efficiency`,overall training efficiency
:activity,task checkpoint operations
:system,TransOM
:challenge,fault tolerance in training
:subsystem,TransOM Operator and Launcher (TOL)
:task,training task lifecycle management
:subsystem,TransOM Eagle Eye (TEE)
:task,training task monitoring and anomaly reporting
:subsystem,TransOM Checkpoint Engine (TCE)
:feature,asynchronous checkpoint saving and loading
:model,GPT-3 175B
:metric,28% reduction
:performance,checkpoint saving and loading performance
:metric,factor of 20 improvement
:`performance metric`,pass-at-one
:`performance metric`,pass-at-n
:strategy,solution fine-tuning
:strategy,solution-cluster re-ranking
:strategy,multi-task sequential fine-tuning
:factor,quality and style of step-by-step solutions
:technique,solution re-ranking
:technique,multi-task fine-tuning
:recipe,fine-tuning recipe
:performance,accuracy improvement
:model,PaLM 2-L
:`enterprise type`,small and medium-sized enterprises
:framework,FATE-LLM
:domain,public domain documents
:format,hard copy
:technology,Optical Character Recognition (OCR)
:attribute,"accuracy, cost-efficiency, sample-efficiency"
:technology,existing OCR engines
:requirement,liberating texts at scale
:software,EffOCR
:attribute,customization
:project,digitizing historical U.S. newspaper scans
:language,Japanese
:`research area`,language model augmentation
:tool,augmentation tools
:example,search engines and calculators
:shortcoming,missing or incorrect knowledge
:shortcoming,incorrect logical inferences
:strategy,few-shot tool-usage
:baseline,no-tool
:strategy,tool-assisted
:strategy,refine with tools
:task,knowledge-retrieval tasks
:metric,token usage
:challenge,few-shot tool integration
:model,ChatGPT (GPT-3.5)
:task,automatic scoring
:capability,natural language prediction
:data,online language materials
:model,domain-specific fine-tuned ChatGPT (GPT-3.5)
:data,student responses and expert scoring
:data,science questions and responses
:metric,scoring accuracy
:goal,public use and community engagement
:technique,Reinforcement Learning from AI Feedback (RLAIF)
:bias,verbosity bias
:metric,verbosity bias measurement
:platform,web applications
:process,alignment training
:goal,content alignment with user intent and ethics
:approach,harmful prompt detection and training
:type,superficial harmful prompts
:technique,compositional instruction attacks (CIA)
:goal,obfuscating harmful instructions
:technique,transformation methods
:study,previous studies
:technique,feedback integration
:capability,reasoning accuracy
:model,Process-Supervised Reward Model (PRM)
:technique,step-by-step feedback
:technique,Proximal Policy Optimization (PPO)
:objective,examine PRM efficacy in inference phase
:algorithm,heuristic greedy search algorithm
:technique,step-level feedback
:model,tailored PRM
:benchmark,mathematical benchmarks
:method,automatic step-level reward dataset generation
:approach,reward-model-based approach to inference
:phenomenon,impact wave in technology
:domain,medical domain
:application,general-purpose applications
:technique,LoRA-based instruction-tuning
:task,Japanese medical question-answering
:evaluation,multifaceted evaluation
:adaptation,English-centric models to Japanese applications
:model,Japanese-centric models
:initiative,medical institutions operating models
:capability,independent operation of models
:setting,unfamiliar environments
:technique,mapping and planning
:ability,human navigation
:concept,semantic knowledge
:artifact,narrative
:method,Language Frontier Guide (LFG)
:concept,semantic guesswork
:setting,real-world and simulated environments
:technique,uninformed exploration
:process,information dissemination
:challenge,disconnect between offline metrics and online performance
:concept,recommendation simulator
:simulator,Agent4Rec
:agent,generative agent
:dataset,MovieLens
:module,profile module
:`type of memory`,factual and emotional memories
:module,action module
:behavior,taste-driven and emotion-driven actions
:algorithm,collaborative filtering-based recommendation algorithm
:`research question`,simulation of human behavior in recommender systems by LLM-empowered generative agents
:evaluation,multi-faceted evaluation
:experiment,emulating the filter bubble effect
:experiment,discovering causal relationships in recommendation tasks
:URL,https://github.com/lehengthu/agent4rec
:capability,simulating human behavior
:capability,simulating specific individuals
:model,ChatGPT API
:model,Character-LLM
:method,editing profiles as experiences
:capability,personal simulacra
:environment,test playground
:goal,building future simulacra
:task,out-of-domain intent discovery
:task,generalized intent discovery
:method,fine-tuning discriminative models
:task,intent discovery
:comparison,zero-shot vs fine-tuned models
:challenge,"clustering, domain-specific understanding, cross-domain in-context learning"
:research,empirical guidance
:domain,automated theorem proving
:benchmark,current ATP benchmarks
:task,symbolic inference
:benchmark,Trigo
:task,trigonometric expression reduction
:capability,number term manipulation
:data,trigonometric expressions
:source,the web
:process,simplification
:`language system`,Lean formal language system
:generator,automatic generator based on Lean-Gym
:data,open-source formal theorem-proving language data
:capability,formal and mathematical reasoning
:event,AI race
:community,open-source research community
:task,text-to-SQL parsing
:evaluation,LLM text-to-SQL parsing capability
:resource,substantial memory
:technique,low-memory optimization
:optimizer,AdamW
:optimizer,Adam
:feature,adaptive learning rate
:feature,momentum
:technique,low-memory optimization with adaptive learning rate
:method,non-negative matrix factorization
:technique,grouped update normalization
:goal,stabilize convergence
:barrier,hardware barrier
:framework,MoCoNVQ
:concept,physics-based motion control
:`model architecture`,Vector Quantized Variational Autoencoder
:technique,model-based reinforcement learning
:data,large unstructured motion dataset
:representation,motion representation
:application,universal tracking control
:application,interactive character control
:application,physics-based motion generation
:task,complex and abstract tasks
:benchmark,Bongard-OpenWorld
:task,few-shot reasoning in machine vision
:problem,Bongard Problems
:concept,open-world free-form concepts
:attribute,abstract visual attributes
:`knowledge type`,commonsense factual knowledge
:`data type`,real-world images
:approach,neuro-symbolic reasoning
:`performance gap`,human-machine gap in Bongard-OpenWorld
:capability,few-shot visual reasoning
:`application domain`,robotic vision
:task,visual perception tasks
:task,object detection
:task,segmentation
:task,identification
:challenge,integration of specialized models
:solution,simplified framework
:framework,RoboLLM
:challenge,ArmBench Challenge
:URL,https://github.com/longkukuhi/armbench
:field,legal studies
:task,court decision analysis
:task,court outcome prediction
:profession,judges and lawyers
:dataset,"10,813 commercial court cases"
:task,predictive analysis
:model,JAIS-13B
:`training paradigm`,one-shot
:`training paradigm`,tailored fine-tuning
:task,text summarization and translation
:`model variant`,14 model variants
:task,model performance evaluation
:metric,GPT evaluation
:model,LLaMA models
:performance,limited performance
:model,GPT-3.5-based models
:model,Arabic-centric JAIS model
:quality,inconsistent and unreliable
:field,computational linguistics and Arabic legal analytics
:process,fast-prototyping task solvers
:factor,prompt configurations
:phenomenon,label shift
:distribution,label marginal
:approach,generative calibration
:method,state-of-the-art calibration methods
:metric,macro-F1 score
:event,IRSE at FIRE 2023
:task,code comment classification
:data,code snippet and comment pair
:evaluation,algorithmic perspective
:`model family`,classical machine learning systems
:evaluation,data-driven perspective
:data,LLM-generated data
:`model type`,multilingual model
:technique,debiasing technique
:language,multilingual context
:model,Multilingual BERT (mBERT)
:process,cross-lingual transfer of debiasing
:dataset,Crows-Pairs
:task,bias analysis
:technique,SentenceDebias
:technique,debiasing with additional pretraining
:language,lower-resource languages
:context,different language contexts
:`network type`,biological neural network
:feature,stochasticity
:`network type`,artificial neural network
:function,uncertainty quantification
:application,cutting-edge applications
:device,spin-orbit-torque device
:component,SOT synapse
:mechanism,spike-timing-dependent-plasticity
:`network type`,stochastic spiking-neural-network
:task,diagnosing breast tumor type
:comparison,stochastic SNN vs deterministic SNN
:issue,hallucination of text
:issue,resource-intensive retraining
:concept,unidirectional editing
:concept,bidirectional language model editing
:goal,rigorous model editing evaluation
:metric,reversibility
:concept,bidirectional recall of editing knowledge
:benchmark,Bidirectional Assessment for Knowledge Editing (BAKE)
:observation,deficiencies in reverse direction recall
:method,Bidirectionally Inversible Relationship Modeling (BIRD)
:concept,bidirectional relationships
:task,question answering and judgement
:application,tabular tasks
:element,prompt representation
:collection,self-supervised structural tasks
:task,structural understanding tasks
:format,8 formats
:work,prior work
:operation,noise operations
:study,impact on LLM performance
:data,real-world messy data
:`model family`,multilingual large-scale pretrained language model
:attribute,languages
:goal,cross-lingual consistency
:metric,Ranking-based Consistency (RankC)
:analysis,determining factors for cross-lingual consistency
:resource,annotated dialogs
:task,training task-oriented dialog systems
:task,dialog response generation
:entity,agent
:entity,user input
:entity,dialog context
:model,dialog augmentation model
:entity,user turn
:technique,prompt design for language model
:technique,output re-ranking
:resource,high quality dialogs
:dataset,MultiWOZ
:dataset,SGD
:metric,dialog success rate
:concept,generative pipelines
:data,massive textual data
:value,accessibility and usability
:access,unrestricted access to LLMs
:risk,privacy risks
:effort,safety and privacy efforts
:analysis,comprehensive analysis of privacy attacks
:concept,adversary capabilities
:overview,detailed overview of defense strategies
:concept,defense strategies
:research,future exploration
:task,semantic change detection
:problem,predicting semantic change
:method,swapping-based semantic change detection (SSCD)
:concept,contextualised word embeddings
:concept,word meaning
:concept,word meaning stability
:outcome,accurate semantic change prediction
:URL,https://github.com/a1da4/svp-swap
:`research area`,weakly-supervised scene graph generation
:`research area`,fully-supervised scene graph generation
:`data source`,image captions
:task,obtaining unlocalized triplets
:task,grounding unlocalized triplets
:`data source`,image regions
:issue,semantic over-simplification
:process,triplet extraction from captions
:issue,low-density scene graph
:process,triplet alignment with classes
:approach,large language model for weakly-supervised scene graph generation
:strategy,in-context few-shot learning
:dataset,Visual Genome
:dataset,GQA
:metric,recall@k
:metric,mean recall@k
:concept,commonsense norms
:activity,reading books
:scenario,embodied scenarios
:entity,machines
:benchmark,NormLens
:data,human judgments and explanations
:approach,distilling social commonsense knowledge
:resource,NormLens data and code
:URL,https://seungjuhan.me/normlens
:era,digital economy
:activity,economic behavior analysis
:field,macroeconomics
:technique,data-driven modeling
:`modeling approach`,advanced AI agent-based modeling
:challenge,agent heterogeneity
:field,macroeconomic simulation
:challenge,macroeconomic trend influence
:challenge,multifaceted economic factors
:characteristic,human-like decision-making
:approach,LLM-empowered macroeconomic simulation
:experiment,simulation experiments on macroeconomic activities
:`historical trend`,interdisciplinary expertise
:concept,interdisciplinary relationships
:domain,multiscale materials failure
:model,MechGPT
:task,knowledge retrieval and generation
:technique,ontological knowledge graphs
:outcome,explanatory insights
:`model version`,13 to 70 billion parameters
:technique,retrieval augmented strategies
:model,MPT-7B-Instruct
:model,ChatGPT Text-DaVinci-003
:dataset,CNN Daily Mail
:dataset,XSum
:metric,BERT score
:field,generative AI applications
:concept,opportunities and challenges
:content,harmful and toxic responses
:method,traditional alignment methods
:goal,desired performance
:strategy,novel alignment strategy
:process,mistake analysis
:goal,understanding internal reasons
:content,toxic responses
:resource,instruction tuning corpus
:ability,self-criticism
:method,conventional alignment techniques
:interaction,human-model interaction
:capability,following human-issued instructions
:task,editing-oriented tasks
:process,interactive text refinement
:need,efficient general editing models
:model,G-SPEED
:algorithm,unsupervised text editing data clustering
:problem,data scarcity
:`model architecture`,sparse editing model
:limitation,limited learning capabilities of small language models
:resource,G-SPEED code and model checkpoints
:metric,learned metric
:task,machine translation evaluation
:metric,COMET
:`evaluation method`,sentence-level score
:metric,BLEURT
:`evaluation method`,granular error analysis
:metric,XCOMET
:`evaluation method`,integrated evaluation
:`evaluation method`,error span categorization
:analysis,robustness analysis
:`error type`,localized critical errors and hallucinations
:method,zero-shot dialogue state tracking
:data,unlabelled data
:method,few-shot dialogue state tracking
:technique,joint training
:task,generating slot types
:concept,cycle consistency
:process,quality sample generation and selection
:method,automatic label creation
:`model family`,dialogue state tracking models
:metric,average joint goal accuracy
:task,abstractive text summarization
:metric,consistency metrics
:quality,summary consistency
:`metric suite`,unsupervised metrics for summary consistency
:dataset,WikiBio_GPT3_Hallucination
:model,linear ensemble models
:method,ensemble methods
:method,LLM-based ensemble method
:issue,computational inefficiency
:algorithm,REMAX
:algorithm,REINFORCE
:model,LaMDA2 (7B)
:resource,GitHub repository for REMAX
:task,image processing
:`model type`,task-specific model
:task,image processing tasks
:paradigm,pretraining and in-context learning
:model,universal model for general image processing
:framework,PromptGIP
:technique,visual prompting question answering
:process,task-specific finetuning
:challenge,acquiring and annotating task-oriented dialogues
:strategy,updating strategies
:method,ParsingDST
:framework,novel framework for ParsingDST
:metric,joint goal accuracy
:metric,slot accuracy
:factor,linguistic proximity and pretraining data amount
:method,source-language validation
:approach,extensive hyperparameter tuning
:approach,narrow hyperparameter grid search
:outcome,lower performance
:protocol,unsupervised evaluation protocol for ZS-XLT
:task,performance maximization and hyperparameter tuning
:method,accumulative averaging of snapshots
:experiment,broad ZS-XLT experiments
:task,semantic and token classification tasks
:outcome,suboptimal ZS-XLT performance
:method,accumulative run-by-run averaging
:`data format`,narrow bit-width data format
:cost,computational and storage costs
:`data format`,microscaling data format
:feature,per-block scaling factor
:goal,"hardware efficiency, model accuracy, and user friction"
:standard,FP32
:`data format`,sub-8-bit
:`communication form`,figurative expression
:activity,visualisation
:outcome,creative thoughts and nuanced emotions
:`communication form`,non-literal expression
:approach,humanly annotated datasets
:task,visualising non-literal expressions
:model,VIPE
:data,large-scale lyrics with visual descriptions
:data,synthetic visual descriptions
:capability,high-quality image generation
:skill,understanding figurative expressions
:`language feature`,probabilistic phrases
:community,English speakers
:communication,successful communication
:concept,shared understanding of probability
:activity,coordination of actions
:task,estimating probability and ambiguity
:context,coordination game
:context,investment advice
:context,medical advice
:group,human participants
:concept,context invariance
:concept,ambiguity estimates
:issue,outdated information and hallucinations
:challenge,information retrieval determination
:challenge,effective information combination
:concept,future target text consideration
:method,latent variable aggregation
:model,ReGAVAE
:`model architecture`,Variational Auto-Encoder
:text,source and target text
:concept,latent space
:concept,probabilistic retrieval generation paradigm
:outcome,text generation quality and hallucination removal
:task,zero-shot abstractive summarization
:capability,handling long-input contexts
:task,multi-document question answering
:behavior,uneven input context utilization
:pattern,U-shaped performance
:bias,favoring initial and final segments
:investigation,comprehensive investigation
:bias,introductory content bias
:challenge,LLM performance across diverse summarization benchmarks
:issue,factual inaccuracy
:strategy,citing sources
:benefit,quick verifiability of statements
:task,developing LMs with the ability to cite
:task,textual response generation for videos
:skill,vision understanding and knowledge reasoning
:framework,BILL-VTG
:concept,relevant video events concentration
:tool,structured scene graph generation
:tool,descriptive image caption generation
:role,reasoning agent
:algorithm,Instruction-Oriented Video Events Recognition (INSOVER)
:task,specifying events for agent
:model,Flamingo-80B
:task,video-based text generation
:task,open information extraction
:goal,structured knowledge extraction
:method,reasoning environment construction for LLMs
:method,syntactic distribution discrepancy estimation
:evidence,correlation evidence for positive demonstrations
:mechanism,reasoning environment establishment
:approach,6-shot
:benchmark,CaRB
:benchmark,TACRED
:benchmark,ACE05
:`material class`,protein material
:examples,"elastins, silks, keratins, collagens"
:field,mechanobiology
:challenge,discovering proteins with specified mechanical properties
:task,protein design
:model,generative model for protein design
:model,pre-trained protein language model
:process,mechanical unfolding response
:`validation method`,full-atom molecular simulations
:outcome,designed proteins
:properties,mechanical properties
:goal,discovery of protein materials with superior mechanical properties
:stage,early stage
:theory,existing theory
:scenario,simple scenarios
:scenario,complex scenarios
:problem,synthetic in-context learning problems
:structure,compositional structure
:algorithm,optimal in-context learning algorithm
:function,representation function
:performance,near-optimal in-context learning performance
:layer,lower layers of Transformer
:data,transformed dataset
:layer,upper layers of Transformer
:capability,linear in-context learning
:experiment,pasting experiment
:mechanism,trained Transformer mechanisms
:theory,in-context learning theory
:algorithm,video language planning
:task,visual planning
:procedure,tree search
:function,policies and value functions
:`model type`,text-to-video model
:function,dynamics models
:output,long video plan
:resource,computation budget
:output,generated video plan
:action,real robot actions
:metric,long-horizon task success rates
:task,summarizing academic papers
:method,factored verification
:benchmark,Halueval
:metric,average hallucinations
:technique,factored critiques
:metric,average hallucinations after self-correction
:advice,caution
:task,synthesizing academic papers
:practice,training on internet data
:task,measurement and mitigation
:method,controlled experimentation
:method,canary strings
:method,embedding similarities
:analysis,longitudinal analysis of data contamination
:model,GPT models
:dataset,Project Euler
:trend,LLM pass rate vs. GitHub popularity and release date
:resource,"dataset, raw results, and evaluation framework"
:discussion,best practices and future steps for releasing benchmarks
:task,benchmark release
:model,LLEMMA
:model,Code Llama
:dataset,Proof-Pile-2
:content,mathematical content
:benchmark,math benchmark
:capability,tool use and formal theorem proving
:organization,authors
:artifact,LLEMMA models and Proof-Pile-2
:capability,automatic generation of scientific protocols
:goal,automation of science
:task,multi-step problems and long-term planning
:task,evaluation of scientific protocols
:issue,complexity and expertise requirement
:framework,automatic evaluation framework
:task,planning experimental protocols
:dataset,BioProt
:representation,pseudocode
:task,scientific protocol generation
:task,protocol generation
:protocol,generated protocol
:location,biological laboratory
:goal,evaluation and improvement of planning abilities
:framework,current language agent frameworks
:goal,proof-of-concept language agents
:platform,OpenAgents
:context,everyday life
:agent,Data Agent
:agent,Plugins Agent
:agent,Web Agent
:`user interface`,web user interface
:`user group`,general users
:community,developers and researchers
:goal,innovative language agents
:research,future research and development of real-world language agents
:concept,growth mindset supportive language (GMSL)
:outcome,students' learning outcomes
:profession,teachers
:concept,growth mindset
:task,automated personalized coaching
:tool,effective coaching tool
:dataset,parallel dataset for GMSL
:framework,GMSL prompt framework
:framework,evaluation framework for GMSL
:theory,psychological theory
:evaluation,large-scale evaluation of GMSL
:outcome,fostering a growth mindset
:reframing,model-generated reframings
:profession,GMSL-trained teachers
:task,automated GMSL feedback
:task,token prediction
:task,long-form generation
:task,prompting-style tasks
:`pretraining pipeline`,existing pretraining pipeline
:method,concatenating random documents
:approach,in-context pretraining
:method,document sorting
:algorithm,approximate algorithms
:`input type`,lengthy and intricate text prompts
:`input type`,"short, single-object descriptions"
:approach,novel approach leveraging large language models
:component,critical components from text prompts
:model,layout-to-image generation model
:process,two-phase operation
:phase,initial global scene generation
:component,object layouts and background context
:scheme,iterative refinement scheme
:limitation,representation of object characteristics
:evaluation,evaluation on complex prompts
:metric,recall
:framework,interactive robot framework
:capability,long-horizon task planning
:method,traditional methods
:requirement,predefined module design
:approach,large language model based approaches
:capability,open-ended planning
:requirement,heavy prompt engineering
:framework,simple framework for interactive task planning
:problem,interactive task planning
:system,our system
:capability,high-level planning and low-level function execution
:capability,generating novel high-level instructions
:capability,task adaptation
:action,replanning
:website,ITP site
:video,YouTube video
:challenge,scaling high-quality tutoring
:platform,tutoring platforms
:role,novice tutors
:opportunity,seizing learning opportunities
:role,math tutors
:benchmark,ReMath
:role,experienced math teachers
:task,step-by-step remediation tasks
:`model type`,instruct-tuned model
:`model type`,dialog model
:task,remediating student mistakes
:information,error type and strategy
:potential,LLMs in education
:issue,misuse
:study,watermark impact on output quality
:aspect,output quality
:concept,trade-off
:aspect,watermark strength
:watermark,unbiased watermark
:aspect,output probability distribution
:technique,unbiased watermarking
:discussion,responsible AI development
:task,emotion summary estimation
:output,emotion summary
:`data structure`,dictionary with probabilities
:task,emotion analysis
:`data source`,Amazon product reviews
:technique,emotion descriptors mapping
:`analysis technique`,PCA type space
:task,eliciting text descriptions of actions
:technique,tail prompt
:challenge,complexity of elicitation
:goal,selection of action via emotional response comparison
:experiment,large-scale experiment
:concept,cross-cultural differences
:instrument,Ten-Item Personality Inventory (TIPI)
:concept,Big Five personality traits
:countries,US and South Korea
:phenomenon,upward bias and lower variation
:concept,structural validity
:field,cross-cultural psychology
:attribute,efficiency and effectiveness
:technique,RL from human feedback (RLHF)
:field,large language model research
:condition,limited resources
:task,exploration and strategic tasks
:method,external program utilization
:capability,passive tree search
:issue,inefficiency
:concept,autonomous tree-search ability of LLM
:capability,autonomous tree search
:method,autonomous tree search (ATS)
:technique,fixed system prompt
:method,ATS-BFS
:method,chain of thought
:method,ATS prompt method
:technique,ATS prompt method data fine-tuning
:`model type`,time-series foundation model
:`model type`,supervised forecasting model
:`model architecture`,patched-decoder style attention model
:data,large time-series corpus
:concept,student modeling
:domain,educational technologies
:environment,open-ended learning environment
:concept,in-context student modeling
:framework,LLM-SS
:task,student attempt synthesis
:benchmark,StudentSyn
:model,LaMDA2-70B
:concept,problem generation
:value,curiosity
:field,science
:value,innovation
:domain,python programming puzzles
:goal,modeling reference distribution
:method,diversity optimization
:space,hand-coded representation spaces
:space,learned embedding spaces
:method,ACES (Autotelic Code Exploration via Semantic descriptors)
:goal,interesting diversity optimization
:dimension,programming skill dimensions
:entity,puzzle
:entity,programming puzzles
:method,diversity-maximizing algorithms
:concept,puzzle solving model training
:concept,semantic mapping
:approach,decoder-only causal language models
:representation,plain text tokens
:approach,semantic chain-of-thought
:concept,semantic information of code
:method,SeCoT
:concept,program execution behavior
:technique,static or dynamic code analysis
:concept,semantic features
:task,multi-agent cooperative text game
:`model type`,Multi-Agent Reinforcement Learning (MARL)
:phenomenon,emergent collaborative behaviors
:issue,long-horizon context management
:strategy,explicit belief state representations
:capability,task performance and ToM inference accuracy
:technique,paraphrasing
:action,content removal
:technique,supervised paraphrasing
:resource,labelled data
:attribute,offensiveness
:dataset,context-aware polite paraphrase dataset
:data,dialogue-style rude utterances and polite paraphrases
:evaluation,principled evaluation
:attribute,generation quality and toxicity
:resource,reduced training data
:task,Q&A analysis
:profession,financial analyst
:issue,scalability and human error
:technique,optical character recognition (OCR)
:data,unstructured transcript text
:data,metadata
:task,Q&A system evaluation
:approach,self-supervised learning of speech
:era,new era of spoken language processing
:representation,SSL representations
:space,phonetic space
:organization,syllabic organization
:representation,sentence-level representation of speech
:technique,self-distillation
:technique,aggregator token
:model,resulting model from self-distillation
:structure,syllabic structures
:truth,ground truth syllables
:benchmark,spoken speech ABX
:model,self-distilled HuBERT
:task,unsupervised syllable discovery
:unit,novel data-driven units for spoken language modeling
:methodology,conversational inquiry
:technique,vector embeddings
:community,humanities researchers
:resource,customized corpora
:`data type`,documents
:interface,digital catalogues search
:task,data extraction and organization
:ability,semantic retrieval and reasoning
:resource,textual archives
:resource,research-specific sources
:feature,writing style
:`content type`,reliable news
:`attack type`,style-based attack
:capability,mimicking trustworthy news style
:detector,text-based detector
:`content type`,LLM-camouflaged fake news
:detector,Sheepdog
:feature,style-agnostic detection
:technique,LLM-empowered news reframing
:technique,style-agnostic training
:technique,content-focused veracity attribution
:technique,sparse mixtures of experts
:goal,resource efficiency
:framework,general framework for MOEs
:technique,approximating two-layer neural networks
:technique,product-key memories
:`evaluation condition`,parameter-equal
:model,Transformer-XL
:dataset,Wikitext-103
:dataset,Enwiki8
:concept,architecture and capability
:system,complex systems
:field,adversarial attacks on large language models
:field,Trustworthy Machine Learning
:field,Security
:attack,jailbreak attack
:research,survey of adversarial attacks on LLMs
:concept,learning structures
:concept,textual-only attacks
:concept,multi-modal attacks
:event,62nd annual meeting of the Association for Computational Linguistics (ACL'24)
:resource,slides for presentations
:`literature genre`,fairytales
:dataset,FairytaleQA
:technique,counterfactual data augmentation
:approach,novel approach for text genre support
:feature,massive vocabulary of language models
:issue,gender perturbations
:dataset,counterfactual training dataset
:technique,in-context example prompting
:outcome,strong performance
:challenge,high annotation costs
:process,prompt sampling
:method,influence-driven selective annotation
:goal,minimize annotation costs
:process,pivotal subset selection
:structure,directed graph
:data,unlabeled data
:metric,influence
:algorithm,greedy algorithm for unlabeled data selection
:method,selective annotation
:outcome,better performance and efficiency
:URL,https://skzhang1.github.io/ideal/
:`model family`,vision foundational model
:task,open-world segmentation
:method,Image Prompt Segmentation (IPSeg)
:technique,image prompting
:model,DINOv2
:component,feature interaction module
:model,Segment Anything model
:process,exhaustive training sessions
:dataset,Pascal VOC
:task,open-world understanding
:domain,various fields
:gap,evaluation gap in focused science domains
:domain,focused science domains
:task,subject understanding
:benchmark,NuclearQA
:task,nuclear domain questions
:metric,new evaluation metric
:experiment,experiments on state-of-the-art models
:`knowledge gap`,scientific knowledge gap of existing LLMs
:need,new quantization methods
:method,TEQ
:technique,trainable equivalent transformation
:technique,low-precision quantization
:attribute,lightweight training
:attribute,no computational overhead
:strategy,combining methods
:resource,code for TEQ
:URL,https://github.com/intel/neural-compressor
:task,sentence representation
:resource,labeled sentences
:dataset,natural language inference datasets
:challenge,acquiring high-quality labeled data
:method,unsupervised sentence representation learning
:framework,SemCSR
:challenge,building positive and negative pairs
:corpus,NLI-style corpus
:issue,high-quality data requirement
:task,dialogue tasks research
:method,manual data collection
:characteristic,cost
:method,web crawling
:method,automatic data generation
:task,open-domain multimodal dialogue tasks
:drawback,lack of open-source large model for multimodal input
:drawback,difficulty in quality control
:framework,Multimodal Data Construction Framework (MDCF)
:issue,high resource expenditure in data collection
:feature,automatic explanation
:dataset,Explanatory Multimodal Open-domain Dialogue Dataset (ExMoDD)
:correlation,positive correlation between model's understanding and response quality
:url,https://github.com/poplpr/exmodd
:capability,learning new concepts during inference
:`model family`,visual model
:process,meta-training or fine-tuning
:algorithm,meta-learning algorithm
:component,pre-trained feature extractor
:benchmark,meta-learning benchmarks
:algorithm,P>M>F
:method,conventional dialogue summarization
:factor,user's specific interests
:`model family`,instruction-finetuned language model
:capability,dialogue summarization
:problem,scarcity of instructive dialogue summarization data
:method,three-step approach for synthesizing summarization triples
:model,InstructDS
:data,summarization datasets with instructive triples
:data,dialogue summarization and reading comprehension datasets
:quality,generalizability and faithfulness
:evaluation,human subjective evaluations
:task,automatic speech recognition performance
:approach,conventional language model training
:entity,words in corpora
:approach,correction focused language model training
:entity,ASR fallible words
:metric,word-level ASR fallibility score
:role,fallibility score predictor and text generator
:scenario,insufficient text
:approach,language model training with LLM-generated text
:method,shallow fusion
:model,backward language model
:model,forward language model
:method,rescoring ASR hypotheses
:combination,BLM and FLM
:method,iterative shallow fusion
:model,partial sentence-aware backward language model
:data,reversed text data
:system,attention-based encoder-decoder ASR system
:process,early pruning of hypotheses
:method,post-processing with PBLM
:combination,SF and ISF
:task,text generation evaluation
:goal,improving accuracy
:trend,larger decoder-based language models
:method,tuning with encoder-based models
:method,tuning with large language models
:method,automatic evaluation methods
:`model type`,decoder-based models
:`model type`,encoder-based models
:concept,surface word sequences
:concept,fine-grained semantic differences
:approach,MapGPT
:field,spatial data analysis
:capability,contextually aware responses
:methodology,building LLMs on spatial and textual data
:technique,tokenization and vector representations
:challenge,generating spatial vector representations
:capability,computational capabilities within MapGPT
:task,geospatial computations
:`research paper`,this research paper
:risk,societal risks
:content,unethical content
:value,intrinsic values
:perspective,moral philosophy
:theory,Moral Foundation Theory
:value,ethical values
:algorithm,DeNeVil
:task,eliciting ethical value violations
:dataset,MoralPrompt
:task,benchmarking intrinsic values
:finding,misalignment of models
:method,VilMo
:benchmark,existing competitors
:method,traditional biases investigation
:strategy,mitigation
:outcome,structured human knowledge
:method,state-of-the-art language model-based methods
:issue,named entity requirement and human annotation
:framework,in-context few-shot relation extraction
:dataset,DocRED
:issue,missing annotations
:benchmark,DocRED development set labels
:problem,cold-start problem
:approach,cross-domain recommendation
:representation,user interest representation
:representation,personal knowledge graph
:paradigm,MEKB-Rec
:`knowledge base`,MEKB
:evaluation,public CDR datasets
:metric,HR@10
:metric,NDCG@10
:application,Weixin recommendation scenarios
:population,hundreds of millions of users
:workload,parallel deep neural network execution
:context,edge devices and data centers
:context,edge devices
:device,mobile phones
:context,data centers
:approach,efficient sparsification
:requirement,computational and memory requirements
:need,scheduling sparse multi-DNN workloads
:problem,sparse multi-DNN workload scheduling
:scheduler,DySta
:information,static and dynamic sparsity
:level,software and hardware levels
:benchmark,sparse multi-DNN workload benchmark
:scenario,different deployment scenarios
:evaluation,comprehensive evaluation on sparse multi-DNN benchmark
:`performance metric`,latency and turnaround time
:artifact,DySta code and artifacts
:attribute,collaborative
:challenge,sentiment identification
:issue,shortage of labeled data
:`model family`,bigger large language model
:task,sentiment analysis for software engineering
:attribute,limited training data and imbalanced distributions
:attribute,ample training data or balanced dataset
:concept,scientific models of language
:terminology,LLM characterization
:concept,target system representation
:risk,theoretical and empirical risks
:concept,lack of transparency in scientific models
:concept,scientific model components
:concept,explanations for language
:capability,language processing
:capability,creative content generation
:task,Divergent Association Task (DAT)
:technique,stochastic sampling
:`trade-off`,creativity vs stability
:capability,divergent semantic associations
:language,Vietnamese
:model,PhoBERT
:task,Vietnamese NLP tasks
:model,ViBERT
:model,VElectra
:model,ViSoBERT
:novelty,first monolingual pre-trained language model for Vietnamese social media texts
:task,Vietnamese social media NLP tasks
:policy,research-only availability
:feature,self-explanations
:method,feature attribution explanation
:field,model interpretability
:technique,occlusion
:technique,LIME saliency maps
:technique,traditional explanation methods
:advantage,cost-efficiency
:task,complex reasoning on knowledge graphs
:framework,KG-GPT
:task,tasks employing knowledge graphs
:process,"sentence segmentation, graph retrieval, inference"
:benchmark,KG-based fact verification and KGQA
:performance,competitive and robust performance
:goal,unifying structured and unstructured data processing
:trend,rapid deployment
:risk,malicious usage
:goal,protecting model weights
:model,GPT-Neo
:`model state`,FP32 mode
:`model state`,INT8 quantized
:action,inference without fine-tuning
:era,large language model applications
:task,entity matching
:process,data integration pipelines
:application,e-commerce applications
:method,pre-trained language model-based matching
:issue,task-specific training data requirement and robustness
:`model family`,open source large language model
:technique,rule generation
:model,Ditto
:`model family`,code completion model
:task,single-file code completion
:benchmark,CrossCodeEval
:task,cross-file code completion
:attribute,multilingual and diverse repositories
:approach,static-analysis-based
:task,identifying cross-file context
:model,CodeGen
:capability,leveraging extensive context
:capability,code retrieval
:trend,digital transformation
:solution,data-driven solutions
:task,industrial tasks
:approach,data-centric R&D
:cost,significant costs
:task,data-centric R&D tasks
:scenario,quantitative investment research
:platform,Qlib
:result,promising results
:vision,automatic evolving of industrial data-centric R&D cycle
:interest,interactive methods for guiding model outputs
:technique,prompt refinement
:goal,influencing model output
:challenge,prompting large language model
:category,"data- and model-specific, linguistic, and socio-linguistic challenges"
:method,comprehensive examination of model outputs
:algorithm,beam search tree
:`model output`,comprehensive output information
:method,interactive visual method
:value,additional insights
:approach,weak supervision
:technique,labeling functions
:model,learned label model
:dataset,soft-labeled dataset
:dataset,Indonesian NLP dataset
:source,conservation news text
:`dataset type`,multi-class classification
:`dataset type`,sentiment classification
:experiment,baseline experiments
:`performance metric`,test performance
:purpose,further research and exploration
:framework,Booksmed
:process,human cognitive processes
:framework,GRADE
:benchmark,ExpertMedQA
:models,state-of-the-art medical models
:model,Med-PALM 2
:model,ALMANAC
:task,clinical inquiries
:institution,ILLC at the University of Amsterdam
:challenge,BabyLM challenge
:model,ChapGTP
:technique,automatic task formation
:`evaluation suite`,"BLiMP, (Super)GLUE, MSGS"
:context,low-resource settings
:track,strict-small track
:task,zero-shot commonsense question-answering
:ability,reasoning about general situations
:task,commonsense question-answering
:resource,commonsense knowledge bases
:data,QA pairs
:issue,noise in QA synthesis
:framework,QADynamics
:task,QA diagnostics and refinement
:issue,machine-detectable artifacts
:evaluation,expert evaluations
:resource,QADynamics code and model checkpoints
:trend,open and reproducible research
:attribute,quality metadata
:issue,lack of quality metadata
:task,data curation
:method,LLM-based in-context learning
:task,metadata annotation
:performance,lower performance in discipline-specific annotation
:limitation,limited contextual information
:method,in-context learning for automated subject metadata annotation
:technology,language technologies
:behavior,model behavior
:metric,accuracy points
:factor,prompt formatting sensitivity
:practice,reporting performance on a single prompt format
:method,prompting-based evaluation
:performance,format performance
:algorithm,FormatSpread
:task,systematic analysis of prompt formats
:analysis,suite of analyses
:phenomenon,sensitivity to prompt formatting
:task,text-to-text privatization
:data,perturbed pre-text
:technique,interpretation techniques
:task,disentangling distortion
:analysis,representational similarity analysis
:phenomenon,reduced similarity of internal representations
:task,probing tasks
:phenomenon,dissimilarity in internal representations
:concept,linguistic competence
:domain,emotion understanding
:`information type`,multi-modal information
:model,DialogueLLM
:data,"13,638 multi-modal emotional dialogues"
:`information type`,visual information
:dataset,ERC datasets
:model,DialogueLLM-7B
:technique,automatic image captioning
:field,satellite remote sensing
:challenge,lack of large-scale image-caption datasets
:`data type`,images
:model,CLAUDE
:model,BLIP
:`data type`,aerial images
:model,GIT
:model,CM3
:model,CM3LEON
:approach,automatic remote sensing image captioning (ARSIC)
:task,collecting captions for remote sensing images
:`data type`,remote-sensing images
:data,private data
:issue,privacy and security challenges
:task,LLM adaptation
:technique,soft prompt tuning
:attack,"membership inference, backdoor, and model stealing"
:finding,no silver bullet for privacy and security in LLM adaptation
:practice,penetration testing
:field,cybersecurity
:task,privilege escalation
:benchmark,automated Linux privilege-escalation benchmark
:tool,LLM-guided privilege-escalation tool
:strategy,high-level guidance
:challenge,maintaining focus during testing
:challenge,coping with errors
:entity,stochastic parrots
:entity,human hackers
:`material class`,polymer
:application,sustainability applications
:task,computational polymer discovery
:capability,automated analysis of reaction pathways
:capability,stability assessment through retro-synthesis
:task,polymerization reactions
:dataset,polymerization dataset for vinyl polymers
:model,forward model
:metric,top-4 accuracy
:model,backward model
:`model performance`,model performance analysis
:issue,unreliable output
:technique,hypothesis ensembling
:technique,multiple prompts
:technique,hypothesis generation
:technique,temperature-based sampling
:technique,Minimum Bayes Risk (MBR) decoding
:field,vision and language generative model
:timeframe,recent years
:service,public-available video generation
:quality,high-visual quality videos
:metric,FVD
:metric,IS
:`model family`,large conditional generative model
:metric,simple metrics
:framework,new evaluation framework
:task,video generation performance evaluation
:`prompt list`,new prompt list for text-to-video generation
:source,real-world prompt list
:`model family`,state-of-the-art video generative model
:benchmark,carefully designed benchmarks
:quality,video qualities
:method,opinion alignment method
:score,final score
:method,proposed evaluation method
:research,operations on parameters
:concept,implicit knowledge
:technique,sensitivity-based techniques
:concept,knowledge-specific parameters
:process,knowledge injection
:concept,parametric knowledge transfer
:url,https://github.com/maszhongming/paraknowtransfer
:challenge,deployment
:issue,environmental impact
:architecture,1-bit Transformer
:component,BitLinear
:component,nn.Linear layer
:model,BitNet
:outcome,competitive performance with reduced memory and energy
:method,8-bit quantization
:baseline,FP16 Transformer
:challenge,storage
:method,vector-based random matrix adaptation
:technique,shared low-rank matrices and scaling vectors
:benchmark,GLUE and E2E
:model,LLaMA2 7B
:fields,science and engineering
:task,interpreting languages
:field,computational materials science
:challenge,software adoption and reproducibility
:software,LAMMPS
:task,molecular dynamics simulations
:output,input files for LAMMPS
:output,descriptions of computational tasks
:benefit,reduction of routine tasks
:benefit,accelerated training
:benefit,enhanced reproducibility
:task,code comment quality classification
:technique,integration of generated pairs
:dataset,code and comments dataset
:`programming language`,C
:data,generated code comment pairs
:attribute,utility
:model,classification model with original dataset
:model,classification model with augmented dataset
:dataset,augmented code and comments dataset
:issue,factual inaccuracies
:issue,diminished versatility and unhelpful responses
:framework,self-reflective retrieval-augmented generation
:qualities,quality and factuality
:technique,adaptive retrieval and self-reflection
:technique,reflection tokens
:models,state-of-the-art LLMs and retrieval-augmented models
:model,LLaMA2-Chat
:qualities,factuality and citation accuracy
:judgment,subjective judgments
:algorithm,alignment algorithm
:group,specific groups
:framework,Group Preference Optimization (GPO)
:task,language model alignment
:task,preference prediction
:evaluation,rigorous evaluations
:task,human opinion adaptation tasks
:strategy,existing alignment strategies
:resource,computing resources
:system,speech recognition system
:domain,single data domain
:approach,multi-stage approach
:stage,first stage
:technique,language model re-scoring
:stage,second stage
:technique,LLM prompting for ASR error correction
:domain,multiple test domains
:activity,data generation and analysis
:domain,various industries and disciplines
:issue,biases and errors in data
:source,software and algorithms
:framework,advanced search and optimization framework
:goal,correcting errors and biases
:domain,corporate and data science
:theory,Solomonoff induction
:concept,Kolmogorov conditional complexity
:task,creating candidate programs
:activity,asking questions
:model,legal assistant chatbot
:information,user's circumstances
:problem,task-oriented asking
:`model type`,zero-shot chat model
:framework,natural language task-oriented asking
:procedure,fact-level masking
:dataset,self-supervised TOA datasets
:dataset,TOA dataset from HotpotQA
:`model family`,zero-shot language model
:opportunity,training and evaluating better TOA models
:task,target tasks
:task,writing prompts
:process,task specification
:framework,Generative Active Task Elicitation (GATE)
:tool,LM-driven elicitation
:value,human preferences and values
:metric,BLEU
:aspect,lexical similarity
:metric,ROUGE
:metric,automated metrics
:quality,personalized content quality
:task,personalized evaluation
:`evaluation method`,AUpEL
:aspect,semantic aspects of generated text
:quality,consistency and efficiency
:role,high-level planner for robots
:knowledge,low-level trajectory knowledge
:task,predicting end-effector poses
:`model type`,vision models
:technique,task-agnostic prompting
:task,real-world language-based tasks
:example,specific manipulation tasks
:capability,failure detection and re-planning
:resource,"videos, code, and prompts"
:URL,https://www.robot-learning.uk/language-models-trajectory-generators
:concept,general intelligence
:theory,psychometric theory
:method,factor analysis
:dataset,Open LLM Leaderboard
:quantity,"1,232 models"
:dataset,GLUE Leaderboard
:quantity,88 models
:concept,g factor
:technique,tokenization
:issue,tokenization limitations
:technique,byte/character-level tokenization
:issue,increased computational cost
:technique,fixed size convolutions
:issue,ignoring word boundaries
:scheme,learn your tokens
:concept,word boundary
:process,parallel decoding
:tokenizer,end-to-end tokenizer
:task,rare word prediction
:value,trade-off in efficiency and robustness
:capability,novel interpretation assignment
:limitation,knowledge cutoff
:limitation,costly finetuning
:capability,novel interpretation learning in-context
:`evaluation suite`,Magnifico
:study,LLMs' novel interpretation acquisition
:framework,text-to-SQL semantic parsing
:feature,diverse tokens and prompt settings
:capability,comprehending novel interpretations
:task,interpreting unfamiliar words
:characteristic,semantic predispositions
:bias,recency bias
:method,question-answering benchmarks
:limitation,factual knowledge coverage
:domain,generic domains
:data,pretraining data
:framework,knowledge graph-based evaluation
:factor,instruction finetuning
:factor,domain
:factor,question complexity
:factor,adversarial context
:issue,unfaithfulness
:`model family`,foundation language model
:task,zero-shot faithfulness evaluation
:metric,FFLM
:task,object classification
:technique,class semantic description
:method,manual attributes
:method,automatic word vectors
:model,Word2Vec
:technique,word vector
:approach,novel word vector fusion
:dataset,2D and 3D datasets
:issue,knowledge transfer between tasks
:condition,low-data regimes
:framework,Prototype-based HyperAdapter
:component,instance-dense retriever
:component,prototypical hypernetwork
:condition,small data size
:method,empirical experiments
:task,open-ended commonsense reasoning
:challenge,extremely large search space
:challenge,implicit multi-hop reasoning
:task,reasoning path retrieval
:dataset,commonsense benchmark datasets
:scenario,high-stakes decision-making
:technique,selective prediction
:framework,adaptation with self-evaluation
:method,self-evaluation
:dataset,question-answering datasets
:benchmark,state-of-the-art selective prediction methods
:benchmark,CoQA
:technology,augmented reality
:`input type`,auditory input
:feature,real-time user guidance
:`input type`,visual input
:feature,contextual user guidance
:study,our study
:method,LLM information assimilation
:challenge,task performance quantification in AR
:`data type`,egocentric video
:`data type`,context analysis
:integration,LLM integration
:feature,enhanced state estimation
:technology,augmented reality systems
:attribute,adaptive
:resource,"code, dataset, and demo"
:`database type`,vector database
:`data type`,high-dimensional data
:problem,approximate nearest neighbor search
:literature,algorithmic articles
:field,vector database algorithms
:approach,hash-based
:approach,tree-based
:approach,graph-based
:approach,quantization-based
:challenge,vector database challenges
:capability,output control and alignment
:issue,low-quality training data
:method,reflection-tuning
:model,oracle large language model
:data,recycled training data
:outcome,improved performance in benchmarks
:technique,chain-of-thought tuning
:experiment,COTT effectiveness verification
:task,hierarchical classification
:`model family`,aligned language model
:characteristic,overconfidence
:concept,logit-based uncertainty calibration
:setting,multiple-choice
:uncertainty,distinct uncertainties
:concept,calibration
:reason,conflation of uncertainties
:method,post-hoc calibration methods
:method,sample-efficient calibration method
:findings,insights into alignment process design
:domain,law
:solution,baseline solutions based on LLMs
:system,information retrieval (IR) system
:combination,LLM+IR
:status,redundant
:pipeline,evaluation pipeline
:task,domain-specific evaluations
:url,https://github.com/srhthu/lm-compeval-legal
:technology,foundational model
:industry,telecommunications industry
:concept,AI native telco
:challenge,engineering considerations
:process,software life cycle
:approach,AI native-first approach
:challenge,"ethical, regulatory, and operational challenges"
:context,mission-critical telecom contexts
:community,graph machine learning researchers
:concept,graph foundation model
:attribute,key characteristics and technologies
:category,reliance on GNNs and LLMs
:field,graph foundation models
:attribute,capabilities
:issue,response faithfulness
:issue,management of (un)answerable queries
:behavior,hallucinatory behavior
:task,(un)answerable queries
:concept,query answerability
:technique,decoding techniques
:goal,factual generation
:scenario,query (un)answerability
:topic,linguistic information in Transformer layers
:work,existing work on language models
:representation,word-level representations
:`model type`,encoder-only language model
:`training objective`,masked-token
:method,semantic structural probing
:representation,sentence-level representations
:performance,performance and layer dynamics
:task,natural-language inference
:finding,model-size invariance
:field,AI-empowered music processing
:task,music tasks
:task,timbre synthesis
:task,music comprehension
:task,music classification
:challenge,grasping diverse music tasks
:stakeholder,developers and amateurs
:data,music data representation
:`model applicability`,model applicability across platforms
:system,MusicAgent
:tool,music-related tools
:workflow,autonomous workflow
:goal,creative focus
:experience,music experience
:source,Web API
:complexity,quadratic complexity of self-attention
:application,long sequences
:`attention mechanism`,Fast Multipole Attention
:complexity,reduced complexity
:field,n-body physics
:model,Fast Multipole Transformer
:performance,consistency across domains
:technique,Reinforcement Learning (RL)
:issue,exploiting shortcuts
:performance,stability and generalization
:approach,novel RL policy learning
:method,automatic data classification into groups
:performance,challenging groups
:strategy,exploration space adjustment
:issue,over-optimization on simpler data
:convergence,embodied agents and large language model
:capability,strong reasoning
:capability,long-horizon reasoning
:task,tabletop manipulation
:benchmark,LOHORAVENS
:problem,modality bridging
:task,long-horizon manipulation tasks
:method,caption generation
:method,learnable interface
:experiment,benchmark experiments
:task,Multiple Choice Symbol Binding (MCSB)
:task,Multiple Choice Question Answering (MCQA)
:setting,"zero-shot, one-shot, few-shot"
:dataset,ViMMRC
:examination,Vietnamese National High School Graduation Examination (VNHSGE)
:dataset,novel high-quality dataset
:format,LaTeX
:evaluation,character prediction
:model,BLOOMZ-7.1B-MT
:`model family`,CLIP embedding space
:task,visual sentiment analysis
:architecture,CLIP-E
:dataset,WebEmo
:experiment,WebEmo testing
:experiment,cross-dataset evaluation
:result,CLIP-E performance
:model,CLIP zero-shot
:dataset,FI dataset
:question,benchmark design and evaluation
:question,tailored deep learning models vs. large vision-language models
:scenario,real-time scenarios
:characteristic,autoregressive nature
:issue,inference latency
:method,token-level parallelism
:characteristic,memory-bound
:method,SPEED
:method,speculative execution
:concept,early-layer hidden states
:concept,parameter sharing
:`model component`,Transformer decoders
:challenge,limited annotated data availability
:field,human activity recognition
:`data type`,virtual IMU data
:`model family`,motion synthesis model
:`model type`,foundational models for HAR
:concept,hierarchical structures in HAR
:application,health sensing and activity summarization
:application,practical applications
:issue,fact-conflicting hallucinations
:benchmark,FactCHD
:dataset,large-scale dataset
:pattern,factuality patterns
:tool,Truth-Triangulator
:resource,benchmark dataset and source code
:url,https://github.com/zjunlp/factchd
:`model scaling`,parameter count scaling
:scale,10^9 to 10^12 levels
:challenge,adaptation and serving of large models
:technique,intrusive parameter-efficient fine-tuning
:aspect,model's internal architecture
:technique,non-intrusive parameter-efficient fine-tuning
:technique,AdaLink
:aspect,parameter-count scaling and training regime
:scaling,quadratic scaling
:architecture,Monarch Mixer (M2)
:scaling,sub-quadratic scaling
:component,Monarch matrices
:property,expressiveness and hardware efficiency
:model,BERT-style language model
:model,ViT-style image classification
:challenge,quadratic bottleneck
:property,causality with sub-quadratic scaling
:benchmark,pretraining perplexity on the Pile
:`model capability`,compositional generalization
:approach,novel architectures and learning paradigms
:method,dataset cartography
:goal,improving compositional generalization
:data,compositional generalization data
:dataset,CFQ
:dataset,COGS
:URL,https://github.com/cyberiada/cartography-for-compositionality
:approach,targeted evaluation
:concept,pseudorandomness
:concept,pseudointelligence
:maxim,perceived intelligence
:entity,evaluator
:framework,complexity-theoretic framework of model evaluation
:interaction,model and evaluator
:attribute,answer quality
:`document set`,retrieval document set
:attribute,generated answer attributes
:attribute,attribution of answers
:method,automatic attribution judging
:insight,retrieval augmentation impact
:pattern,attribution patterns
:`error type`,attribution errors
:solution,privacy-preserving inference
:activity,practical deployment and implementation
:framework,PrivInfer
:module,perturbation module
:module,restoration module
:output,perturbed results
:mechanism,RanText
:technique,random adjacency
:benchmark,current leading scheme in privacy protection
:attack,adaptive attack
:metric,pointwise V-usable information
:metric,in-context PVI
:concept,exemplar selection and shot number
:concept,stability
:task,identifying challenging instances
:`data granularity`,sentence-level
:data,parallel training data
:system,context-aware translation system
:data,document-level monolingual data
:`model combination`,sentence-level translation model with document-level language model
:technique,novel weighting techniques
:metric,document-targeted scores
:technique,back-translation
:approach,molecular language modeling
:task,generating chemical structures
:`model family`,molecular language model
:preference,chemist preferences
:attribute,simplicity and efficiency
:concept,pretrained language models
:ability,remarkable performance in NLP tasks
:`model family`,GPT-3 family large language model
:survey,comprehensive survey on GPT-3 family LLMs
:ability,data labelling and data augmentation
:attribute,robustness and effectiveness as evaluators
:capability,high-level reasoning abilities
:mind,human mind
:`thinking type`,mixed linear and nonlinear thinking
:technique,Inferential Exclusion Prompting (IEP)
:capability,non-linear thinking
:process,forward planning and backward eliminating
:capability,simulation of complex human thinking processes
:integration,IEP and CoT integration
:capability,LLMs' task performance
:benchmark,Mental-Ability Reasoning Benchmark (MARB)
:capability,human logic evaluation
:task,subtasks with rationale references
:capability,logic and verbal reasoning abilities
:task,programming code processing
:task,code analysis
:literature,existing body of literature
:task,systematic evaluation and assessment
:context,obfuscated code
:method,real-world case studies
:tool,code analysis automation
:understanding,potential and constraints of LLMs in code analysis
:outcome,enhanced applications in code analysis
:task,query-specific article generation
:method,search result presentation
:issue,information hallucination
:system,article generation system
:component,query-specific clustering
:value,citations
:framework,evaluation framework
:`system quality`,F-1 overall system quality
:framework,Remark-LLM
:output,LLM-generated texts
:resource,extensive datasets
:issue,malicious exploitation
:component,learning-based message encoding module
:component,reparameterization module
:output,watermarked textual tokens
:component,decoding module
:process,signature extraction
:algorithm,optimized beam search algorithm
:quality,coherence and consistency
:quality,resilience against attacks
:benchmark,prior art
:field,lexical semantics
:concept,concept relations
:`knowledge graph`,ConceptNet
:issue,"fixed relation types, incompleteness, and noise"
:strategy,relation embeddings from language models
:concept,indirectly related words
:concept,structured domain knowledge
:approach,combining knowledge graphs and relation embeddings
:task,intermediate word identification
:representation,proposed representations
:task,solving analogy questions
:belief,iterative self-critique capabilities of LLMs
:task,graph coloring
:problem,NP-complete reasoning problem
:technique,iterative prompting
:factor,criticism content and correctness
:phenomenon,increase in effectiveness
:technique,top-k completions
:claim,self-critiquing capabilities of LLMs
:need,human user requirements
:collection,user-GPT conversations
:divergence,NLP research vs real-world application needs
:collection,real user queries to GPT
:benchmark,NLP benchmark tasks
:gap,user needs vs research tasks
:task,design
:interaction,user interactions with LLMs
:task,overlooked NLP tasks
:challenge,practical challenges
:insight,roadmap for LLM alignment with user needs
:language,declarative software specification language
:technique,automated repair techniques
:`research area`,automated repair of declarative software specifications
:technique,template-based repair
:technique,feedback-driven iterative repair
:technique,bounded exhaustive approach
:task,automatic repair of declarative specifications
:language,Alloy
:technique,backend constraint solvers
:technique,automatic Alloy repair methods
:technique,existing repair techniques
:error,ChatGPT-generated repair errors
:issue,improper operator usage
:issue,type errors
:issue,higher-order logic misuse
:issue,relational arity mismatches
:phenomenon,hallucinations in ChatGPT-generated repairs
:phenomenon,inconsistency in ChatGPT results
:community,software practitioners
:task,declarative specification repairs
:task,natural language reasoning
:`research trend`,self-improvement using feedback
:approach,single generic feedback source
:`error type`,diverse error types in reasoning chains
:framework,multi-aspect feedback
:`feedback module`,multiple feedback modules
:`error type`,errors in reasoning chain
:task,logical entailment
:scenario,LLM application scenarios
:vulnerability,backdoor
:`model family`,prompt-based large language model
:attack,PoisonPrompt
:method,prompt methods
:finding,potential security threats
:`model family`,pretrained Transformer models
:mechanism,attention mechanism
:concept,sequence dependencies
:mechanism,full attention mechanism
:concept,modeling capacity
:model,MasFormer
:concept,mixed attention spans
:mechanism,full and sparse attention
:concept,continual training and sequence length impact
:task,web search
:challenge,validating reliability of generated results
:problem,LLM hallucination
:goal,PageRank for LLM era
:framework,generative retrieval framework
:knowledge,LLM knowledge
:module,generator
:goal,trustworthy online sources
:module,validator
:goal,source reliability
:module,optimizer
:goal,refining unreliable sources
:method,extensive experiments and evaluations
:quality,"superior relevance, responsibility, and trustfulness"
:process,text to vector mapping
:approach,white-box model design
:task,interpretability analysis
:challenge,interpretability in white-box models
:`model characteristic`,lack of inherent interpretability
:task,constructing metrics
:`metric family`,family of metrics
:task,mechanism investigation
:model,tree topological probe
:model,BERT-Large
:`model family`,BERT-like pretrained language model
:concept,working mechanism
:strategy,fine-tuning performance enhancement
:activity,conversation
:task,inference tasks
:concept,semantic information gap
:challenge,information disparity
:`cognitive process`,inductive inference
:data,negative samples
:`cognitive process`,inference generation
:property,black-box adaptation
:model,speech language model
:technique,warmup training
:task,speech classification tasks
:capability,classifying unseen classes
:source,text origin
:data,doctor responses
:model,AI model
:data,AI responses
:capability,understanding language
:task,classifying healthcare consultation responses
:field,medical text classification
:goal,advancing the field
:approach,more effective text classification
:task,classifying doctor-generated and AI-generated text
:method,debias-while-prompt tuning
:method,CO$^2$PT
:`model family`,debiased language model
:attack,red teaming attacks
:method,attack prompt construction
:issue,construction cost and quality
:approach,integrated manual and automatic methods
:goal,high-quality attack prompts
:framework,attack framework
:framework,defense framework
:framework,attack and defense frameworks
:dataset,SAP
:goal,safety evaluation and enhancement
:url,https://github.com/aatrox103/sap
:technique,multimodal technique
:`model family`,vision large language model
:task,vision tasks
:concept,cross-modal interaction
:concept,multilingualism
:framework,systematic framework for cross-modal capability quantification
:dataset,datasets for cross-modal evaluation
:concept,consistent cross-modal performance
:modality,vision modality
:concept,challenging vision tasks
:method,vision description prompting
:`application scenario`,precision agriculture
:technique,sensors and data analysis
:`data type`,textual data
:`application scenario`,crop health threat detection
:event,plant health event detection
:`data type`,unstructured textual data
:model,Choubert
:event,unseen natural hazards
:problem,lack of labelled data
:task,token-level annotation
:`research area`,multilingual vision-and-language (V&L)
:capability,multilingual and multimodal
:challenge,scarcity of multilingual captions
:approach,ICU (Image Caption Understanding)
:model,V&L model
:task,image captioning in English
:task,crosslingual language understanding
:capability,conquering language barriers
:benchmark,IGLUE
:`data format`,structured product descriptions
:`data type`,attribute/value pairs
:platform,e-commerce platforms
:`data format`,unstructured product descriptions
:task,attribute/value extraction
:`algorithm family`,multiobjective evolutionary algorithm
:`problem type`,multiobjective optimization problem
:requirement,handcrafted search operators
:approach,learning-based operators
:component,search operators
:component,MOEA operators
:algorithm,MOEA/D
:algorithm,MOEA/D-LO
:study,experimental studies
:operator,LLM-based operator
:quality,robust generalization performance
:potential,using pre-trained LLMs in MOEA design
:activity,accessing information
:attribute,truthfulness and factuality
:experiment,comparison of LLMs and search engines for fact-checking
:participant,crowdworkers
:task,validate claims
:task,contrastive explanation
:issue,over-reliance on LLMs
:system,search engines
:combination,search engine results and LLM explanations
:activity,reading retrieved passages
:dataset,CelebA
:concept,social inequality
:issue,high cost
:concept,bias origin
:framework,Fast Model Debiasing (FMD)
:task,model debiasing
:technique,counterfactual concept
:technique,machine unlearning
:dataset,Colored MNIST
:dataset,Adult Income
:outcome,reduced bias
:requirement,minimal data and parameter updates
:concept,text-attributed heterogeneous graph
:element,entities with texts and relationships
:task,textual information learning
:framework,pretraining framework for language models
:concept,topological and heterogeneous information
:concept,context graph
:element,neighborhoods of a target node
:task,topology-aware pretraining task
:element,nodes in context graph
:model,heterogeneous graph neural network
:strategy,text augmentation strategy
:issue,imbalance of text-rich and textless nodes
:element,datasets from various domains
:factor,time
:task,Question Answering (QA)
:concept,time specifiers relationships
:dataset,QA datasets
:concept,time expressions
:framework,Time-Context Aware Question Answering (TCQA)
:problem,language models' time-context understanding
:task,Time-Context Dependent Span Extraction (TCSE)
:framework,Time-Context Dependent Data Generation
:metric,time awareness evaluation
:model,TCQA-trained model
:dataset,TimeQA
:resource,dataset and code
:URL,https://github.com/sonjbin/tcqa
:issue,undesirable biases
:method,causal mediation analysis
:task,identifying causal relations
:method,automated circuit discovery
:method,DiffMask+
:result,overlap in identified components
:issue,defining and measuring bias
:goal,dataset development
:approach,uncertainty-aware deep learning
:concept,confidence
:approach,evidential deep learning
:concept,evidential signal
:concept,Dirichlet strength
:concept,KL regularisation
:concept,misclassification bias
:approach,generative evidential neural networks
:approach,prior networks
:concept,out-of-distribution samples
:framework,FinLMEval
:`model type`,fine-tuned expert model
:issue,failures
:resource,extensive system scales
:hardware,GPU/TPU
:method,checkpointing
:goal,fault-tolerance
:issue,storage I/O overheads
:framework,in-memory fault-tolerance framework
:issue,data transfer and I/O
:goal,system reliability
:goal,fault-tolerance efficiency
:sector,industries
:goal,data privacy and security
:`research area`,tabular data synthesis
:task,tabular data generation
:problem,curse of dimensionality
:model,Tabula
:strategy,token sequence compression
:task,new tabular data synthesis tasks
:metric,46.2% training time reduction
:goal,synthetic data utility
:model,Backpack
:concept,token-defined meaning
:model,Backpack language model
:concept,character composition
:model,Chinese Backpack language model
:evaluation,SimLex-style lexical semantic evaluation
:concept,multi-character meanings
:technique,consistent sense weights
:concept,interpretability-through control
:task,entity legal form classification
:`model variant`,BERT variants
:baseline,traditional text classification approaches
:data,Legal Entity Identifier (LEI) data
:standard,Entity Legal Form (ELF) code standard (ISO 20275)
:metric,macro F1 score
:review,third-party expert reviews
:proposal,Transformer-based models for entity legal form classification
:field,data standardization and integration
:stakeholder,"financial institutions, corporations, governments"
:goal,performance and safety balance
:tension,helpfulness vs harmlessness
:process,LLM training
:algorithm,Safe Reinforcement Learning from Human Feedback (Safe RLHF)
:goal,human value alignment
:`model component`,reward and cost models
:`safety concern`,LLM safety
:method,Lagrangian method
:goal,mitigating harmful responses and enhancing performance
:model,ALPACA-7B
:capability,few-shot or zero-shot learning
:technique,black-box prompt search
:property,combinatorial optimization complexity
:concept,search space design and optimization
:method,CLaPS
:task,few-shot text classification
:component,verbalizer
:process,class prediction
:verbalizer,manual label verbalizer
:method,manual label selection
:method,Label-Aware Automatic Verbalizer (LAAV)
:technique,conjunction induction
:benchmark,existing verbalizers
:quality,relevance
:capability,cross-lingual generalization
:question,limits of implicit cross-lingual generalization
:concept,explicit knowledge transfer
:aspect,syntactic
:analysis,conceptual space alignability
:`model type`,encoder-only and decoder-only large language model
:method,meta-learning-based method
:capability,zero-shot and few-shot generalization
:phenomenon,cross-lingual in-context learning
:task,syntactic analysis
:`performance gap`,between languages
:task,molecule understanding
:ability,2D graph perception
:model,MolCA
:gap,text and graph-based molecular content comprehension
:component,cross-modal projector
:component,uni-modal adapter
:task,efficient adaptation to downstream tasks
:ability,open-ended text generation with 2D graph information
:task,"molecule captioning, IUPAC name prediction, molecule-text retrieval"
:resource,codes and checkpoints
:URL,https://github.com/acharkq/molca
:concept,poor generalization
:domain,safety-critical domains
:goal,robust text classifiers
:problem,prediction problems with spurious correlations
:method,importance re-weighting
:method,diff-in-diff methodology
:task,matching examples
:concept,conditional probability of text
:outcome,improved OOD accuracy
:task,learning caregiver-invariant predictors
:domain,clinical diagnoses
:data,semi-synthetic data
:technique,weighted-averaging of parameters
:task,merging models
:phenomenon,inaccuracy of weighted-averaging
:concept,gradient mismatches
:scheme,uncertainty-based scheme
:goal,reducing gradient mismatch
:`model architecture`,Vision Transformer
:scheme,averaging
:concept,implicit assumptions
:scheme,task arithmetic
:scheme,fisher-weighted averaging
:application,LLM-integrated application
:vulnerability,prompt injection attack
:literature,existing works
:concept,systematic understanding
:framework,general framework for prompt injection attacks
:framework,defense framework against prompt injection attacks
:url,https://github.com/liu00222/open-prompt-injection
:`model type`,parameter-shared pre-trained language model
:benefit,reduced storage and memory costs
:challenge,computational burdens during inference
:technique,neural ordinary differential equations
:`model type`,fully or partially shared model
:`model type`,autoregressive pre-trained language model
:technique,parameter sharing and neural ODEs
:`model type`,autoencoding pre-trained language model
:context,resource-constrained settings
:system,gesture recognition system
:task,gesture identification
:gap,gesture to function connection
:framework,GestureGPT
:module,gesture description
:data,hand landmark coordinates
:system,dual-agent dialogue system
:agent,gesture agent
:agent,context agent
:dataset,public gesture datasets
:setting,real-world
:`performance metric`,zero-shot top-5 grounding accuracy
:paradigm,gesture understanding
:`model family`,open large language model
:capability,central controller capabilities
:method,AgentTuning
:capability,agent abilities
:dataset,AgentInstruct
:strategy,hybrid instruction-tuning
:model,AgentLM-70B
:resource,AgentInstruct and AgentLM models
:url,https://github.com/thudm/agenttuning
:problem,factually incorrect responses
:approach,knowledge retrieval augmentation
:problem,suboptimal text generation performance
:approach,verification with a separate verifier
:model,verifier
:task,error detection
:technique,ensemble of outputs
:concept,verification reliability
:evaluation,question answering benchmarks
:outcome,factually correct outputs
:URL,https://github.com/jinheonbaek/kalmv
:task,hateful or toxic language detection
:information,"explanation, context, and victim community information"
:technique,prompt variation
:model,Text-DaVinci
:dataset,HateXplain
:dataset,Implicit Hate
:dataset,ToxicSpans
:information,target information
:information,rationales/explanations
:typology,error cases
:concept,jailbreak prompts
:task,safeguard development
:component,positional encoding
:`model family`,bidirectional masked language model
:property,locality
:property,symmetry
:property,locality and symmetry
:URL,https://github.com/tigerchen52/locality_symmetry
:`cognitive process`,analogy-making
:capability,human reasoning
:corpus,StoryAnalogy
:task,analogy identification and generation
:data,24k story pairs with human annotations
:theory,extended structure-mapping theory
:metric,multiple-choice question accuracy
:model,FLAN T5-XXL
:task,analogy generation
:capability,analogy generation in LLMs
:goal,robust language technologies
:field,multilingual natural language processing
:system,multilingual task-oriented dialogue system
:issue,performance disparities
:measure,absolute equivalence
:measure,relative equivalence
:experiment,controlled experiments
:factor,nature of TOD task
:factor,underlying pretrained language model
:factor,target language
:factor,amount of TOD annotated data
:bias,adaptation bias
:bias,intrinsic bias
:language,Turkish
:insight,performance disparities insights
:tip,TOD data collection and system development
:task,task-oriented dialogue system development
:framework,behavioral and computational experiments
:tool,fictional prompts
:study,analysis of stories
:method,narratology and inferential statistics
:story,crowdworker-authored stories
:story,AI-generated stories
:paradigm,experimental paradigm
:comparison,human vs. LLM-generated storytelling
:prompt,pygmalionesque prompts
:myth,Pygmalion
:narrative,GPT-3.5 narratives
:characteristic,progressive gender roles and sexuality
:narrative,GPT-4 narratives
:narrative,AI narratives
:characteristic,innovative plot twists
:framework,fiction as a window
:concept,collective imaginary and social dimensions
:task,specifying a reward function
:characteristic,infeasibility
:task,learning a reward model from human feedback
:characteristic,expensive
:`model family`,zero-shot reward model
:approach,VLM-RMs
:platform,MuJoCo humanoid
:technique,minimal prompt engineering
:technique,embedding space projection
:effect,scaling effect
:limitation,capability limitations of VLMS
:function,semantic planning
:task,low-level manipulation tasks
:algorithm,Eureka
:outcome,reward functions
:outcome,pen spinning tricks
:model,simulated shadow hand
:factor,model factors
:study,comprehensive study on MLMs
:relationship,model factors and social biases
:result,study results
:factor,model objectives
:`neural network type`,Recurrent Neural Network (RNN)
:concept,Turing completeness
:`model family`,RNN-based language model
:task,weighting over strings
:concept,probabilistic Turing completeness
:machine,deterministic probabilistic Turing machine
:mode,real-time computation
:concept,expressivity
:approach,procedural generation
:goal,automated content creation
:knowledge,"rules, algorithms, and parameters"
:framework,3D-GPT
:role,problem solver
:component,core agents
:objective,scene description enhancement and procedural generation integration
:software,Blender
:field,3D modeling
:goal,advancements in scene generation and animation
:process,creative processes
:output,visual and textual outputs
:`interaction paradigm`,current interaction paradigms
:process,idea exploration
:framework,structured generation of design space
:limitation,limited idea exploration
:system,Luminate
:`user study`,user study with professional writers
:interaction,interaction with LLMs for creative tasks
:task,mathematical problem-solving
:method,subgoal-based methods
:framework,SEGO
:goal,improved mathematical problem-solving
:challenge,identifying suitable subgoals
:concept,problem-specific subgoals
:concept,optimized subgoals
:process,policy model training
:benchmark,GSM8K and MATH
:resource,data and code for SEGO
:data,large diverse text dataset
:data,targeted examples
:hypothesis,knowledge and skills origin
:technique,decoupling knowledge and skills
:framework,RL-based framework
:method,Emulated Fine-Tuning (EFT)
:trait,behavioral traits adjustment
:concept,LM up-scaling
:process,resource-intensive fine-tuning
:service,cloud API
:challenge,optimizing computational cost and performance
:field,large language model usage
:approach,AutoMix
:task,query routing
:mechanism,few-shot self-verification
:mechanism,meta verifier
:model,LLaMA2-13/70B
:metric,incremental benefit per cost
:resource,AutoMix code and data
:URL,https://github.com/automix-llm/automix
:task,image caption evaluation
:challenge,persistent challenge
:concept,dimensions of similarity
:measure,existing evaluation measures
:goal,holistic score
:method,CLAIR
:concept,correlation with human judgments
:dataset,Flickr8k-Expert
:concept,noisily interpretable results
:resource,CLAIR code
:URL,https://davidmchan.github.io/clair/
:task,visual tasks
:component,Transformer block
:`data type`,visual tokens
:approach,leveraging LLMs for computer vision
:approach,multi-modal vision-language setup
:approach,using LLM Transformer blocks for visual encoding
:task,visual and multi-modal tasks
:hypothesis,information filtering hypothesis
:phenomenon,effectiveness of LLMs in visual tasks
:observation,feature activation focus
:resource,code for visual encoding with LLMs
:URL,https://github.com/ziqipang/lm4visualencoding
:concept,healthcare data interoperability
:standard,FHIR resources
:data,clinical text snippets
:experiment,LLM conversion to FHIR resources
:process,NLP and human calibration
:trend,exponential growth
:requirement,domain-optimized frameworks
:model,ConFiRM
:task,financial information retrieval
:dataset,finance domain-specific QA dataset
:solution,data-efficient query intent extraction
:task,query intent classification
:task,knowledge base labeling
:`data type`,geographic data
:task,geospatial decision-making
:`data type`,geo-coordinates
:concept,geospatial awareness
:experiment,multidimensional scaling (MDS)
:capability,geospatial reasoning
:task,city location determination
:capability,geospatial knowledge synthesis
:task,binary code comment quality classification
:api,OpenAI API
:dataset,1239 code-comment pairs
:label,useful or not useful
:dataset,9048 code-comment pairs
:model,support vector machine
:technique,generative techniques
:factor,fine-tuning data selection
:principle,data quality and distribution
:dimension,learnability
:intuition,pretraining phase capabilities
:method,Loss Based SFT Data Selection (LOBASS)
:concept,model capabilities
:experiment,LOBASS method evaluation
:outcome,LOBASS method efficiency
:outcome,LOBASS method adaptability
:`model family`,preference model
:framework,compositional preference model
:concept,interpretable features
:concept,scalar scores
:technique,logistic regression classifier
:concept,preference data properties
:outcome,preferred samples
:risk,biased and harmful content
:goal,open and safe AI development
:solution,open-source ecosystem
:project,open-source ecosystem for LLMs
:model,H2OGPT
:framework,H2O LLM Studio
:task,"fine-tuning, evaluation, and deployment of LLMs"
:belief,"accessibility, efficiency, and trust in AI"
:url,https://gpt.h2o.ai/
:phenomenon,code-switching speech
:`linguistic concept`,language mixing
:issue,grammatical structure complexity
:technique,n-best hypotheses generation
:`model family`,automatic speech recognition model
:mapping,hypotheses-to-transcription
:method,generative error correction
:output,accurate transcription
:metric,mixed error rate
:capability,future prediction
:capability,probabilistic prediction
:event,forecasting tournament
:`time period`,July to October 2023
:`participant count`,843 participants
:`performance metric`,forecast accuracy
:strategy,no-information forecasting
:hypothesis,midpoint probability prediction predisposition
:environment,real-world forecasting tournaments
:capability,generalized reasoning and prediction
:task,arithmetic problems
:method,computational graph for multiplication
:algorithm,graph-based multiplication algorithm
:task,multiplication challenge
:concept,human-like numerical operations
:operator,10k operator
:concept,maximum power to base 10
:metric,100% accuracy
:task,large number multiplication tasks
:quantity,"1,000,000"
:insight,simple human insights
:technique,linear position interpolation
:`model component`,Rotary Position Embeddings (RoPE)
:`model component`,Attention with Linear Biases (ALiBi)
:dream,automated mathematicians
:interest,building automated mathematician systems
:goal,building mathematical AI systems
:system,mathematical AI system
:discussion,open discussions and questions
:approach,multi-disciplinary perspective
:community,cognitive scientists
:community,mathematicians
:resource,massive labeled data
:approach,semi-supervised learning
:issue,noisy labels
:characteristic,multiple training iterations
:framework,UPET
:issue,labeled data scarcity
:technique,Monte Carlo dropout
:model,Bayesian neural network
:technique,parameter-efficient learning
:technique,easy-hard contrastive tuning
:metrics,performance and efficiency
:resource,codes and data for UPET
:task,graph structure understanding
:method,pre-trained graph embedding generation
:task,fine-tuning with downstream task labels
:`research focus`,generalization capabilities of graph models
:`model type`,graph-oriented large language model
:framework,GraphGPT
:component,text-graph grounding
:concept,graph structures
:paradigm,dual-stage instruction tuning
:signal,self-supervised graph structural signals
:event,academic conferences
:need,conference information
:system,intelligent question-answering system
:dataset,ConferenceQA
:method,manual and automated data organization
:format,semi-structured JSON
:`data type`,question-answer pairs
:task,information-seeking question answering
:study,Conference QA study
:issue,hallucination and outdated knowledge
:method,structure-aware retrieval method
:url,https://github.com/zjukg/conferenceqa
:`user preference`,diverse high-quality outputs
:task,text-generation
:algorithm,quality-diversity search
:technique,AI feedback
:algorithm,Quality-Diversity through AI Feedback (QDAIF)
:algorithm,non-QD controls
:potential,AI feedback in open-ended search
:capability,innovation in human society
:algorithm,data-parallel stochastic gradient descent
:task,distributed optimization
:issue,communication bottleneck
:condition,noiseless communication
:task,practical tasks
:task,wireless distributed optimization
:property,low-rank structure of gradients
:algorithm,stochastic gradient descent
:scheme,state-of-the-art compression
:task,GPT language modeling
:task,entity typing
:knowledge,legal knowledge
:field,legal NLP
:`model family`,law-oriented language model
:performance,inconsistent performance
:domain,sub-domains of law
:cue,syntactic cues
:`model architecture`,BERT-based
:method,natural language instruction
:performance,LLM task performance
:quality,instruction quality
:process,writing instructions
:characteristic,laborious and subjective
:method,Auto-Instruct
:goal,improving instruction quality
:output,candidate instructions
:model,scoring model
:data,575 NLP tasks
:benchmark,human-written and baseline LLM-generated instructions
:activity,information access and consumption
:development,LLM development
:framework,XLingEval
:criteria,evaluation criteria
:finding,disparity in LLM responses
:need,cross-lingual capabilities
:benchmark,XLingHealth
:need,cross-lingual capacities
:value,equity
:challenge,enhancing robustness against adversarial attacks
:method,existing pruning methods
:`time period`,current era
:attribute,pre-trained knowledge
:strategy,post-training pruning
:goal,conserving pre-trained knowledge
:concept,cumulative error
:experiment,pruning experiments
:dataset,SST2
:dataset,IMDb
:dataset,AG News
:approach,proposed pruning approach
:industry,database industry
:challenge,abbreviated column names
:task,data tasks
:task,NameGuess
:problem,expanding column names
:dataset,training dataset for NameGuess
:method,data fabrication
:benchmark,human-annotated evaluation benchmark for NameGuess
:data,real-world table examples
:model,fine-tuned language model for NameGuess
:technique,conditioning on table content and column headers
:analysis,comprehensive analysis for NameGuess
:opportunity,future opportunities in NameGuess
:resource,NameGuess code
:URL,https://github.com/amazon-science/nameguess
:task,discriminative natural language understanding tasks
:data,human-written text
:concept,cognitive biases
:phenomenon,primacy effect
:behavior,label selection bias
:factor,label order in prompt
:goal,reliable ChatGPT-based solutions
:url,https://github.com/wangywust/primacyeffectgpt
:task,new task implementation
:concept,mechanism understanding
:process,in-context learning inference
:method,kernel methods
:relationship,gradient descent and self-attention mechanism
:setting,softmax attention
:method,contrastive learning without negative samples
:component,self-attention layer
:perspective,contrastive learning perspective
:task,future model design
:field,computational physics
:concept,equivariance
:task,simulating physical systems
:risk,erroneous extrapolation
:technique,imposing symmetry
:issue,poor acceptance rate in SLMC
:mechanism,attention
:technique,symmetry equivariant attention
:technique,Self-Learning Monte-Carlo (SLMC)
:architecture,new architecture for spin-fermion model
:model,spin-fermion model on a two-dimensional lattice
:issue,poor acceptance rate in linear models
:phenomenon,scaling law of acceptance rate
:concept,cryptocurrency
:value,decentralization
:task,market trend prediction
:technique,instruction-based fine-tuning
:result,zero-shot performance gain
:`model scale`,smaller-scale models
:issue,reduced generalization
:investigation,instruction-based model response
:`instruction characteristic`,short and simple instructions
:performance,model accuracy under long and complex instructions
:agent,LLM-based autonomous agent
:tool,functional APIs
:process,solution plan execution
:challenge,action space navigation
:condition,expansive action space
:issue,inefficient navigation
:algorithm,ToolChain*
:structure,decision tree
:algorithm,A* search algorithm
:technique,cross-language transfer learning
:research,linguistic representation investigation
:concept,language group
:`model type`,community-centered model
:field,multilingual language model research
:concern,training data transparency
:concept,generalization vs memorization
:dataset,GitHub Recent Bugs (GHRB)
:data,76 real-world Java bugs
:task,character understanding
:source,script utterances
:framework,multi-level contrastive learning
:experiment,character understanding sub-tasks
:model,SpanBERT
:model,Longformer
:model,BigBird
:result,improved performance
:analysis,in-depth
:resource,open-source code
:url,https://github.com/david-li0406/script-based-character-understanding
:`model type`,Query Likelihood Model
:capability,zero-shot ranking effectiveness
:system,state-of-the-art ranking system
:scenario,zero-shot and few-shot scenarios
:resource,codebase
:URL,https://github.com/ielab/llm-qlm
:capability,world interaction
:task,comprehending surroundings
:model,Steve-Eye
:limitation,visual richness oversight
:dataset,850k open-world instruction pairs
:function,multimodal perception
:function,foundational knowledge base
:function,skill prediction and planning
:benchmark,open-world evaluation benchmarks
:task,sentence production and comprehension
:entity,human brain language representations
:`training method`,grounded supervision
:task,language learning
:task,word learning
:task,language acquisition
:task,benchmark evaluation
:`supervision type`,visual supervision
:`information type`,text information
:`modeling approach`,multi-modal modeling
:goal,human-like word representations
:task,multi-modal open-domain question answering
:modality,diverse modalities
:examples,"images, tables, passages"
:framework,MOQAGPT
:capability,zero-shot multi-modal question answering
:challenge,multi-modality ranking
:capability,modality accommodation and model transition
:activity,multi-modal information retrieval and extraction
:dataset,MMCOQA
:dataset,MultimodalQA
:methodology,supervised methods
:resource,MOQAGPT codebase
:URL,https://github.com/lezhang7/moqagpt
:ability,hearing
:entity,AI agents
:hearing,general auditory information
:`sound type`,"speech, audio events, music"
:model,SALMONN
:capability,processing and understanding general audio inputs
:task,speech and audio tasks
:approach,few-shot activation tuning
:milestone,AI with generic hearing abilities
:resource,interactive demo of SALMONN
:URL,https://github.com/bytedance/salmonn
:resource,training code and model checkpoints of SALMONN
:event,acceptance
:attack,membership inference attack
:signal,text similarity
:signal,model's resistance to document modifications
:`model family`,summarization model
:risk,exposing data membership
:safeguard,training summarization models
:`trade-off`,privacy and utility
:task,automatic response forecasting
:entity,news media
:concept,social dynamics
:`user type`,lurkers
:challenge,limited user data
:statistic,97% of tweets by 25% of users
:framework,SocialSense
:technique,graph-based propagation
:graph,belief-centered graph
:`user type`,distant users
:`user type`,unseen users and lurkers
:quality,robustness and practical applicability
:capability,knowledge transfer and adaptation
:task,finetuning with labeled datasets
:task,knowledge transfer with unlabeled data
:`model family`,smaller self-adaptive language model
:capability,self-adaptation with unlabeled data
:strategy,stochastic multiple answer generation
:strategy,self-adaption
:performance,improvement on QA datasets
:quality,robustness and stability
:url,https://github.com/starsuzi/t-sas
:`model family`,domain-specific pretrained language model
:`model family`,general-domain pretrained language model
:domain,scientific
:domain,clinical
:`model family`,financial pretrained language model
:data,diverse financial data
:model,Financial Language Model (FiLM)
:data,diverse financial datasets
:data,unseen corpus groups
:goal,reducing memory overhead and accelerating inference
:demand,zero-shot quantization
:problem,overfitting in generative adversarial learning
:framework,zero-shot sharpness-aware quantization (ZSAQ)
:task,zero-shot quantization of PLMs
:algorithm,SAM-SGA optimization
:goal,quantization accuracy and model generalization
:result,convergence rate for minimax optimization
:problem,minimax optimization
:framework,nonconvex-PL minimax optimization frameworks
:method,ZSAQ framework
:goal,model generalization
:ability,emergent abilities in NLP
:ability,instruction-following ability
:approach,tailored learning approach
:role,reasoning teacher
:paradigm,interactive multi-round learning
:process,exposing deficiencies
:technique,self-reflection learning
:task,mathematical and commonsense reasoning tasks
:issue,domain specificity
:phenomenon,knowledge repetition
:issue,knowledge illusion
:issue,knowledge toxicity
:solution,diversify training data
:solution,fine-tune models
:value,transparency and interpretability
:training,ethics and fairness training
:trend,future technological trends
:value,"fairness, transparency, and ethics"
:tool,PromptAttack
:attack,prompt-based adversarial attack
:technique,attack prompt
:component,original input
:component,attack objective
:component,attack guidance
:filter,fidelity filter
:technique,ensembling adversarial examples
:element,emoji
:benchmark,AdvGLUE
:benchmark,AdvGLUE++
:model,Instruct-GPT
:approach,probabilistic ranking
:approach,contextual ranking
:model,TUNA
:technique,sequential application of probabilistic and contextual ranking
:benchmark,LMEntry
:benchmark,Vicuna QA
:`model family`,reinforcement learning baselines
:resource,code and data for TUNA
:concept,embodied language comprehension
:theory,cognitive theories
:dataset,POSQA
:task,size comparison question answering
:condition,zero-shot setting
:technique,advanced prompting
:concept,real-world comprehension
:issue,deception and confusion by prompt form
:standard,human behaviour alignment
:setting,zero- and few-shot learning
:service,third-party LLM prompting services
:cost,operating expense
:phenomenon,repetitive prompting
:framework,LLM response caching framework
:goal,reducing operating expenses
:component,decision criteria and tradeoff measurement
:model,k-nn classifier
:model,multi-layer perceptron
:task,intent recognition
:result,operating expense savings
:result,performance tradeoff
:era,big data and large models
:function,automatic annotating functions
:application,AI-driven applications
:`annotation type`,open-vocabulary annotation
:capability,human-level cognition
:system,OpenAnnotate3D
:`system type`,open-vocabulary auto-labeling system
:annotation,"2D masks, 3D masks, and 3D bounding box annotations"
:field,open-vocabulary multi-modal 3D auto-labeling
:evaluation,comprehensive evaluations
:data,public and in-house real-world datasets
:result,accurate open-vocabulary auto-annotating results
:`model type`,self-supervised pre-trained model
:`data type`,unlabeled speech data
:model,WavLM
:issue,large memory and computational requirements
:context,resource-restricted devices
:framework,GenDistiller
:process,autoregressive generation of hidden layers
:mechanism,two-dimensional attention mechanism
:concept,causality of hidden layers
:baseline,direct prediction of hidden layers
:`research area`,open-domain chatbots
:limitation,focus on short single-session dialogue
:need,contextual understanding in multi-session conversations
:element,time intervals between sessions
:context,multi-session conversations
:element,relationships between speakers
:dataset,Conversation Chronicles
:task,long-term conversation setup
:model,ReBot
:task,long-term context understanding
:task,tasks requiring consistency
:benchmark,self-consistency evaluation
:issue,under-specification
:experiment,behavioral experiments
:`model suite`,OpenAI model suite
:task,ambiguous integer sequence completion
:metric,average consistency
:range,67% to 82%
:capability,model capability
:phenomenon,emergent capability
:issue,self-judgment calibration
:test,nonparametric test
:issue,alternative answer probability
:`information type`,typological information
:database,WALS
:database,Grambank
:resource,linguistic grammars
:issue,discrete categorical nature of databases
:phenomenon,disagreements in typological databases
:activity,systematic exploration of disagreements
:view,continuous view of typological features
:field,linguistics
:application,language modeling in low-resource scenarios
:issue,brittleness in MT systems
:issue,overgeneration
:technique,finetuning on translation instructions
:issue,overspecialization
:technique,adapter-based finetuning with LoRA
:technique,traditional finetuning
:capability,few-shot performance
:approach,incorporating few-shot examples during finetuning
:data,10 language pairs
:issue,noisy input
:dataset,MTNT
:issue,noise in source and target sentences
:task,noise removal
:dataset,C-MTNT
:evaluation,human and GPT-4 evaluations
:task,data cleaning
:task,perception and interpretation tasks
:task,predictive reasoning
:benchmark,predictive reasoning benchmark
:capability,predictive reasoning capabilities
:domain,"abstract pattern reasoning, human activity prediction, physical interaction prediction"
:`evaluation method`,evaluation methods powered by large language model
:performance,predictive reasoning performance
:framework,standardized evaluation framework
:`model family`,advanced multimodal large language model
:challenge,adapting pre-trained language models to tasks
:`model family`,task-tuned model
:analysis,design choices in LLM predictions
:evaluation,systematic holistic evaluation
:attribute,factor influence
:result,factors for safe use
:process,reasoning over spans of tokens
:`explanation type`,highlight-based explanations
:concept,token importance identification
:dataset,SpanEx
:method,community detection based unsupervised method
:capability,self-improvement
:algorithm,TriPOST
:technique,experience replay
:technique,interactive learning
:task,math and reasoning tasks
:goal,general representation
:characteristic,large amounts of textual data
:data,downstream task dataset
:technique,controlled randomness
:technique,targeted noise
:technique,adding noise
:task,joint named entity recognition and relation extraction
:effort,sequential recommendation enhancement
:`information type`,behavioral information
:model,Unified Pre-trained Language Model Enhanced Sequential Recommendation (UPSR)
:gap,behavioral-textual information gap
:concept,key indicators
:evaluation,extensive evaluations
:dataset,seven datasets
:outcome,best performance
:analysis,comprehensive model analyses
:promise,source code release
:task,autonomous fact-checking
:task,human fact-checking
:task,information verification
:concept,truth discernment
:feature,explanation and citation
:factor,query language and claim veracity
:research,investigation into LLMs for fact-checking
:task,research on LLM success and failure
:task,open-domain question-answering
:`reasoning type`,single-hop reasoning
:task,open-domain multi-hop reasoning
:method,automated chain-of-thought prompting
:method,manual chain-of-thought prompting
:issue,scalability and diversity
:framework,self-prompted chain-of-thought
:benchmark,multi-hop question-answering benchmarks
:capability,intermediate reasoning step recall
:deployment,generative AI tools
:operation,API calls to large language model
:process,neural caching
:policy,request processing decision
:criteria,active learning-based selection criteria
:criteria,margin sampling
:criteria,query by committee
:`information source`,dialogue state
:approach,knowledge-grounded response generation
:technique,probabilistic logical programming
:process,conversion to natural language
:model,neural conversational model
:dataset,KVRET
:dataset,GraphWOZ
:outcome,coherent chain of thoughts
:model,hierarchical graphical model
:metric,geometrical convergence rate
:outcome,likelihood of LLM-generated chain of thoughts
:outcome,correct sequence of thoughts
:task,chemical reaction prediction
:limitation,insufficient training data and textual information utilization
:`domain knowledge`,chemical knowledge
:framework,RELM
:capability,self-assessment of prediction reliability
:condition,out-of-distribution settings
:component,model of human preferences
:function,reward function
:entity,stakeholders and academic disciplines
:model,RLHF reward model
:information,descriptors and evaluations
:history,optimizing preferences
:context,sociotechnical context of reward models
:difference,"ontological differences between costs, rewards, and preferences"
:tension,methodological tensions in RLHF
:direction,research directions for reward models
:capability,multi-modality input sensing
:`semantic space`,visual-text space
:domain,marine domain
:model,MarineGPT
:dataset,Marine-5M
:capability,marine vision and language alignment
:community,academic and industrial communities
:resource,MarineGPT data and models
:task,mathematics learning
:task,adaptive feedback for math problem-solving
:issue,wrong reasoning generation
:issue,misinterpretation of questions
:issue,difficulty in understanding rationales
:paper,position paper on LLMs in math education
:challenge,employing LLMs in math education
:`research question`,LLMs' pedagogical ability in math education
:concept,geometric and information-theoretic compression
:concept,geometric compression
:concept,information-theoretic compression
:concept,coding length
:data,linguistic data
:concept,intrinsic dimension estimators
:goal,model alignment with human intent
:algorithm,RLHF algorithms
:assumption,human preferences follow reward
:concept,regret under optimal policy
:challenge,optimization challenges
:method,contemporary RLHF methods
:setting,contextual bandit settings
:domain,state-based robotics
:setting,limited observation dimensionality
:`algorithm family`,algorithms for optimizing behavior from human feedback
:concept,regret-based model of human preferences
:principle,maximum entropy
:algorithm,Contrastive Preference Learning
:concept,preferences
:attribute,off-policy and contrastive objective
:problem,high-dimensional and sequential RLHF problems
:feature,multi-turn dialogue interaction
:evaluation,human-based evaluation
:challenge,intensive manual labor
:evaluation,LLM-based evaluation
:capability,human-style multi-turn chatting
:method,chatseed-based dialogue generation
:task,multi-turn dialogue generation
:quality,impressive multi-turn dialogue quality
:quality,satisfactory multi-turn dialogue quality
:resource,evaluation dataset and code
:objective,language modelling (LM) objective
:property,precisely evaluable code
:method,reinforcement learning (RL)
:task,coding capabilities improvement
:resource,unit tests
:approach,novel approach
:task,RL training data acquisition
:method,actor-critic RL training scheme
:model,pre-trained code language model
:metric,9.9% improvement
:metric,4.3% improvement
:challenge,distributional discrepancy
:framework,Synthesis Step by Step (S3)
:baseline,ZeroGen
:baseline,GoldGen
:data,human-annotated data
:issue,harmful associations
:framework,Stereomap
:perception,LLMs' perceptions of demographic groups
:theory,Stereotype Content Model (SCM)
:concept,stereotype dimensions
:activity,investigating LLMs' reasoning
:issue,social disparities
:issue,biases and harmful associations
:characteristic,long-form spoken content
:task,segmentation of ASR transcripts
:technique,finite-state constraints
:issue,ASR errors
:baseline,automatic punctuation
:`language pair`,English-German
:technique,LLM segmentation
:`language pair`,English-Spanish
:`language pair`,English-Arabic
:model,Fusion-in-Decoder (FiD)
:task,open-domain tasks
:task,fact checking
:`model component`,reader
:performance,reader model performance
:method,eliminating non-essential information
:metric,run-time reduction
:process,generating insights
:limitation,scale of real-time conversations
:size,large human groups
:process,real-time conversations
:group,81 American voters
:activity,forecasting
:outcome,converged candidate selection
:candidate,selected candidate
:value,qualitative and quantitative benefits
:function,communicative function of gestures
:interaction,face-to-face interaction
:factor,individual differences and context
:approach,automatic gesture generation
:characteristic,data-driven reliance and specificity
:challenge,designer control
:task,gesture analysis and generation
:task,suggesting context-specific gestures
:characteristic,novelty and appropriateness of gestures
:task,laborious annotations
:goal,designer intent
:tool,chatbot-based teaching assistant
:issue,increasing classroom sizes
:challenge,efficacy and interaction nuances
:study,formative study
:setting,undergraduate computer science classroom
:platform,Prolific
:strategy,pedagogically informed guidance strategies
:interaction,learner-LLM interaction
:`response type`,direct LLM answers
:outcome,student performance
:`response type`,refining student solutions
:relationship,guidance and LLM role
:recommendation,design recommendations
:team,IIT(ISM) Dhanbad
:event,FIRE IRSE 2023 Shared Task 1
:task,automatic usefulness prediction of code-comment pairs
:data,original base data
:framework,machine learning-based model training
:data,neural contextual representations
:entity,code-comments pair
:analysis,performance analysis
:system,IIT(ISM) Dhanbad's system
:quality,photorealism
:metric,FID score
:`model component`,UNet
:attribute,visual attributes
:`model component`,text-encoder
:`model component`,CLIP text-encoder
:method,Diff-QuickFix
:advantage,speedup and editing performance
:`legal concept`,fair use
:activity,extraction of information
:activity,verbatim reproduction
:issue,copyright violations
:capability,verbatim memorization
:activity,redistribution of copyrighted text
:data,popular books and coding problems
:capability,redistribution of copyrighted materials
:resource,code repository
:URL,https://github.com/coastalcph/copyrightllms
:characteristic,inconsistency
:metric,automatic evaluation metric
:trend,generative model development
:method,preliminary and hybrid evaluation
:task,text summarisation
:task,grammatical error correction
:output,best models' outputs
:benchmark,popular benchmarks
:evaluation,human judgement
:approach,multi-view approach
:concept,latent space fusion
:model,MHG-GNN
:representation,molecular graph embeddings
:model,Molformer
:representation,chemical language embeddings
:concept,graph-based molecular relations
:model,Molformer-XL
:data,1.1 billion molecules
:`dataset collection`,MoleculeNet
:concept,scale-up
:`data type`,sequential tabular data
:characteristic,contextual and sequential information
:approach,transformer-based
:characteristic,dynamic and static field differences
:model,FATA-Trans
:characteristic,field- and time-aware
:component,field-type embedding
:component,time-aware position embedding
:study,visualization studies
:resource,FATA-Trans code
:URL,https://github.com/zdy93/fata-trans
:`model family`,transformer language model
:concept,plausibility processing
:task,detecting plausible noun-verb relationships
:process,narrative construction
:structure,plot structure
:domain,intelligence analysis
:task,intelligence report generation
:challenge,integrating dynamic event information
:challenge,closing information gaps
:system,retrieval augmented generation (RAG) system
:approach,retrieval augmented generation (RAG) approach
:`model component`,autoregressive decoder
:framework,Fabula
:analyst,intelligence analyst
:graph,Event Plot Graph (EPG)
:evaluation,evaluation studies
:attribute,semantic relevance
:attribute,coherency
:attribute,low data redundancy
:phenomenon,human label variation
:dataset,LiveNLI
:phenomenon,NLI label variation
:dataset,MNLI
:phenomenon,within-label variation
:task,explanation generation
:method,ad-hoc prompt selection
:method,auto prompt generation
:characteristic,inefficient
:framework,EVOKE
:technique,automatic prompt refinement
:role,LLM-reviewer
:technique,prompt scoring
:role,LLM-author
:technique,prompt editing
:technique,author-reviewer feedback loop
:technique,data selection approach
:data,hard samples
:goal,deeper task understanding
:benchmark,logical fallacy detection
:task,edge probing tests
:knowledge,grammatical knowledge
:conjecture,LLM linguistic knowledge encoding
:performance,high performance in edge probing tests
:criticism,edge probing test validity
:encoder,random encoder
:modification,information theoretic probes
:dataset,edge probing test datasets
:encoder,large language model encoder
:probe,non-information theoretic probes
:framework,RTSum
:unit,relation triples
:process,multi-level salience scoring
:tool,web demo for RTSum
:feature,fine-grained interpretations
:concept,customization options
:visualization,salience visualization
:unit,textual units
:resource,RTSum codes
:tool,GPTutor
:issue,training data limitations and prompt design issues
:`development tool`,LLM development tools
:concept,black box
:tool,Copilot
:action,customizing prompts
:capability,multilingual and multiprogramming language support
:user,GPTutor users
:action,fine-tuning prompts
:language,Sui-Move
:entity,individuals' lives
:concept,morality
:`research gap`,terminology and theories in moral NLP
:issue,lack of discussion on terminology and theories
:literature,existing literature on moral NLP
:concept,philosophical foundation
:finding,survey findings
:issue,lack of clear definitions
:action,future research recommendations
:hope,authors' hope
:outcome,informed discussion of morality
:technique,self-instruct
:task,training conversational agents
:technique,alpaca
:characteristic,proprietary and non-public
:technique,new in-context learning methods
:task,selecting high-quality synthetic examples
:algorithm,ensemble-instruct
:output,useful outputs
:url,https://github.com/ibm/ensemble-instruct
:phase,requirements engineering
:activity,software requirements process
:challenge,complexities and uncertainties
:chapter,potential of LLMs in RE
:potential,LLMs in RE
:research,using LLMs for RE
:methodology,SWOT analysis
:evaluation,preliminary evaluation of LLMs in RE
:issue,hate speech
:approach,suggesting rephrasing of potential hate speech
:task,rephrasing hate speech
:model,BART-Detox
:technique,few-shot demonstrations
:analysis,failure analysis
:`evaluation metric`,GEMBA-MQM
:task,translation quality estimation
:technique,three-shot prompting
:characteristic,language-agnostic prompts
:benchmark,system ranking accuracy
:context,academic works
:task,bilingual lexicon induction
:technique,cross-lingual word representations
:`paradigm shift`,towards large language models in NLP
:technique,few-shot in-context prompting
:`model family`,text-to-text multilingual large language model
:technique,few-shot prompting with in-context examples from nearest neighbours
:benchmark,state-of-the-art BLI scores
:research,in-depth analyses and ablation studies
:industry,commercial vector database management system
:quantity,over 20
:`time frame`,past five years
:task,embedding-based retrieval
:`time frame`,over ten years
:task,similarity search
:trend,shift from algorithms to systems
:application,data intensive applications
:requirement,vast stores of unstructured data
:obstacle,vagueness of semantic similarity
:task,vector data management
:obstacle,large size of vectors
:obstacle,high cost of similarity comparison
:obstacle,lack of natural partitioning for indexing
:obstacle,difficulty of efficiently answering hybrid queries
:technique,vector compression
:technique,partitioning based on randomization
:technique,learning partitioning
:technique,navigable partitioning
:technique,hybrid query operators
:technique,hardware accelerated execution
:system,native systems specialized for vectors
:spectrum,VDBMS design and runtime characteristics
:system,extended systems with vector capabilities
:system,vector database management system
:task,research challenges
:approach,transformer-based methods
:task,ambiguity resolution
:task,unimodal retrieval
:task,explainable answer generation
:model,learn to rank
:task,ranking
:experiment,extensive experiments on VWSD
:insight,future directions in VWSD
:task,crystal property prediction
:process,crystal design
:concept,complex interactions in crystals
:`data type`,crystal text descriptions
:dataset,TextEdge
:task,crystal property prediction from text
:method,LLM-Prop
:model,MatBERT
:concept,space group symmetry and Wyckoff sites
:interface,language model prompting
:technique,gradient-based finetuning
:technique,tree prompting
:feature,model decision-making inspection
:dataset,classification datasets
:model,MARVEL
:task,multi-modal retrieval
:technique,unified encoder model
:issue,modality gap
:model,T5-ANCE
:component,visual module plugin
:dataset,ClueWeb22-MM
:capability,image understanding
:capability,extracting image semantics
:resource,MARVEL code repository
:url,https://github.com/openmatch/marvel
:`model family`,code large language model
:context,real-life applications
:metric,general accuracy
:concept,shared semantics
:framework,IdentityChain
:study,study of code LLMs
:tool,model debugging tool
:code,IdentityChain code
:url,https://github.com/marcusm117/identitychain
:dataset,MedEval
:attribute,comprehensiveness
:data,expert annotations
:`model family`,generic language model
:investigation,benchmarking language models for healthcare
:task,physical skill learning
:challenge,high-dimensionality and nuanced feedback
:task,acquiring expert demonstration data
:characteristic,costly and time-consuming
:knowledge,task-related knowledge
:application,LLM for proposing reward functions
:limitation,numerical instability and environment feedback incorporation
:approach,extracting task knowledge from LLMs
:goal,efficient reward functions
:component,iterative self-alignment process
:process,iterative self-alignment
:goal,minimizing ranking inconsistency
:method,testing on simulated tasks
:outcome,effective support for design choices
:metric,LLM-based metrics
:strategy,task-specialization
:setting,industrial
:finding,trade-offs of task-specialization
:insight,actionable insights
:`model deployment`,instruction fine-tuning model deployment
:`data type`,resting-state fMRI scan
:`data type`,task-evoked fMRI scan
:`data type`,task fMRI scan
:technique,zero-shot prediction
:`data type`,group-average contrast
:model,OPIC
:`data type`,rsfMRI-derived connectome
:model,state-of-the-art model
:`data type`,in-domain tasks' data
:method,contrastive language image pretraining
:capability,object localization
:question,augmenting CLIP with task-specific vision models
:resource,model zoo
:data,uncurated and noisy image-text dataset
:improvement,visual representations
:metric,performance increase
:capability,promptable zero-shot classification
:research,sentiment analysis of African languages
:issue,data and technological limitations
:event,AfriSenti-SemEval Shared Task 12
:resource,annotated dataset
:strategy,one-model-per-language
:strategy,single-model-all-languages
:`model type`,standard multilingual model
:task,cross-lingual representation learning
:finding,more data produces better models
:`model type`,models for African languages
:benchmark,all tasks
:finding,no one-model-fits-all solution
:evaluation,per-language evaluation
:`model type`,larger multilingual model
:benchmark,biomedical language understanding benchmark
:application,AI applications with LLM back-ends
:issue,corpus leakage
:benchmark,CBLUE
:benchmark,PromptCBLUE
:capability,multi-task capabilities
:technique,fine-tuning techniques
:resource,massive corpora and advanced hardware
:scenario,sequential task learning
:approach,Orthogonal Low-Rank Adaptation (O-LORA)
:concept,orthogonal vector subspaces
:advantage,marginal parameter costs and no user data storage
:benchmark,continual learning benchmarks
:capability,generalization ability on unseen tasks
:`content type`,short-form funny videos
:platform,social networks
:task,video humor understanding
:entity,AI models
:dataset,previous video humor datasets
:cue,verbal cues
:dataset,ExFunTube
:platform,YouTube
:task,video filtering
:annotation,timestamps and text explanations
:technique,zero-shot video-to-text prompting
:`evaluation method`,automatic scores
:task,humor explanation
:`evaluation method`,rationale quality experiments
:method,semantic parsing-based method
:format,logical forms
:challenge,knowledge base schema understanding
:method,in-context schema understanding
:data,annotated question-query pairs
:benchmark,KQA Pro
:strategy,random retrieval strategy
:method,PromptMix
:step,generate and relabel text augmentations
:step,generate challenging text augmentations
:`model family`,smaller classifiers
:model,DistilBERT base
:model,BERT base
:benchmark,5-shot data augmentation methods
:resource,PromptMix code
:URL,https://github.com/servicenow/promptmix-emnlp-2023
:system,fact verification system
:task,veracity assessment
:approach,natural logic
:resource,substantial training resources
:approach,question answering for predicting natural logic operators
:dataset,FEVER
:system,pre-trained seq2seq natural logic system
:system,prompt-based classifier
:performance,robustness and portability
:technique,advanced prompt engineering
:interaction,multi-round interactions
:framework,optimal control framework for multi-round interactions
:goal,analytical improvements
:extension,prompt engineering via ensemble methods and multi-agent collaboration
:perspective,optimal control perspective
:challenge,theoretical challenges
:goal,development of effective and interpretable methods
:`model architecture`,multi-head self-attention-based Transformer
:task,learning tasks
:capability,context understanding
:`model architecture`,Transformer encoder
:capability,layer-wise contextual information preservation
:capability,mathematical equivalence preservation
:model,Transject
:capability,layer-wise distance preservation
:concept,Lipschitz continuity
:capability,injective mappings learning
:field,statistical physics
:characteristic,random and unorderly learning
:characteristic,low entropy and scalability
:phenomenon,attention in research
:task,emotion recognition
:technique,deep prompt tuning
:`model family`,specialized deep models
:quality,transferability and feasibility in emotion recognition
:data,six widely used datasets
:attribute,vast number of parameters
:capability,remarkable capabilities
:challenge,continuous knowledge assimilation
:framework,DynaMind
:technique,memory mechanisms and modular operators
:process,model inference
:outcome,accuracies of LLMs' outputs
:resource,DynaMind code and demo
:status,solved problem
:level,next level
:variant,fine-grained entity types
:variant,retrieval setup
:corpus,silver-annotated corpus
:goal,advancing NER research
:attribute,harmfulness
:activity,jailbreaking safety behavior
:technique,input text-based red-teaming
:activity,bypassing safety behavior
:technique,prompt-based attacks
:activity,diagnosing hidden harmful information and biases
:perspective,parametric red-teaming through unalignment
:technique,unalignment
:activity,breaking model guardrails
:model,Llama-2-Chat 7B
:model,Llama-2-Chat 13B
:`model family`,safety-aligned large language model
:data,user interaction history
:method,cross-domain methods
:data,other domain data
:method,general cross-domain methods
:concept,domain relationship
:representation,item IDs
:challenge,transferable signals
:behavior,user cross-domain behaviors
:challenge,learning item relationships
:task,world knowledge learning
:framework,LLM-Rec
:task,domain-agnostic recommendation
:challenge,data sparsity and cold start
:task,recommendation scenarios
:task,text sanitization
:goal,concealing identity
:approach,two-step approach to text sanitization
:dataset,text anonymization benchmark
:dataset,wikipedia biographies collection
:component,privacy-oriented entity recognizer
:model,named entity recognition model
:resource,gazetteer
:process,assessing privacy risk
:indicator,re-identification risk indicators
:analysis,contrastive analysis of privacy indicators
:task,adult content detection
:classifier,existing classifiers
:task,distinguishing erotic content
:attribute,nuance
:activity,handling harmful erotic content
:approach,structural analysis
:narrative,harmful erotic narratives
:system,hybrid neural and rule-based context-aware system
:technique,coreference resolution
:classifier,hybrid model
:task,distinguishing harmful content
:language,Polish
:model,RoBERTa and Longformer
:approach,hybrid model with coreference resolution
:attribute,visual explainability
:evaluation,gender-neutrality
:knowledge,useful gender knowledge
:dataset,DiFair
:task,gender bias evaluation
:metric,Gender Invariance Score
:`model behavior`,biased behavior
:finding,debiasing impact
:phenomenon,misleading claims on social media
:task,claim identification
:characteristic,time-consuming
:task,claim normalization
:process,decomposition of complex posts
:model,CACN
:dataset,CLAN
:issue,sensitive information leakage
:method,Model Perturbations (MoPE)
:task,training data identification
:technique,noise addition and log-likelihood measurement
:statistic,drop in log-likelihood
:`mathematical concept`,trace of the Hessian matrix
:method,loss-based attacks
:factor,training point order
:task,attack success
:concept,loss of a point
:task,determining extractability
:process,emotion experience
:aspect,physiological
:concept,cognitive appraisal
:value,personal values
:task,emotion detection
:field,psychology
:task,cognitive appraisal prediction
:dataset,COVIDET-Appraisals
:concept,appraisal dimensions
:concept,natural language rationale
:task,cognitive appraisal assessment
:`model family`,emotionally intelligent model
:capability,task completion without additional training
:challenge,few-shot example limitation
:quality,high-quality solutions
:limitation,limited context length
:data,larger-scale demonstrations
:framework,offline learning framework
:data,offline data
:`data type`,logs of human interactions
:policy,LLM-powered policy
:approach,text-based and code-based approaches
:framework,Offline Data-driven Discovery and Distillation (O3D)
:process,offline discovery and distillation
:capability,decision-making capabilities
:task,passage re-ranking
:method,first stage retrieval
:technique,few-shot demonstration selection
:condition,demonstration helpfulness
:strategy,demonstration selection based on difficulty
:finding,effectiveness of demonstrations for ranking and question generation
:goal,principled research
:integration,language data integration
:capability,environmental understanding
:value,driving safety
:value,driving efficiency
:work,comprehensive survey
:field,language models in AD and ITS
:survey,advances in language models
:artifact,models and datasets
:concept,potential applications and research directions
:concept,challenges and research gap
:task,catalyst discovery
:challenge,combinatorial search space
:approach,goal-driven combinatorial search
:approach,Monte Carlo Tree Search-based approach
:dataset,computational chemistry simulations curation
:field,computational chemistry
:dataset,catalysis researchers' questions
:field,catalysis
:process,scientific reasoning and discovery
:task,judging profoundness
:`statement type`,nonsensical statements
:model,TK-Instruct
:bias,overestimation of profoundness
:challenge,resource-intensive annotation
:question,feasibility of minimizing human-in-the-loop feedback
:method,metric-based methods
:outcome,minimizing required annotations
:outcome,reduction of indecisive outcomes
:strategy,prioritizing data instances
:expertise,domain expertise
:capability,language comprehension and in-context learning
:limitation,handling very long inputs/contexts
:knowledge,significant background knowledge
:limitation,maximum context length
:data,semi-structured legal and financial data
:task,efficient context retrieval
:system,domain-specialized QA system
:benchmark,contemporary models
:integration,LLMs into legal and financial NLP systems
:event,WMT 2023 Terminology Shared Task
:`language pair`,German-to-English
:`language pair`,English-to-Czech
:`language pair`,Chinese-to-English
:task,terminology-constrained post-editing
:model,generic encoder-decoder MT model
:data,terminology-based synthetic data
:process,four-step translation process
:metric,term incorporation rate
:activity,reading articles
:information,geospatial information
:model,GeoLM
:concept,geospatially grounded language model
:context,geospatial context
:mechanism,spatial coordinate embedding
:field,geospatial sciences
:resource,GeoLM code
:URL,https://github.com/knowledge-computing/GeoLM
:outcome,success
:issue,misuse of AI-generated texts
:task,text origin detection
:source,human vs. AI-generated text
:concept,distribution gap
:method,DetectGPT-SC
:technique,masking and prediction
:task,paper-reviewer matching
:trend,increasing paper submissions
:factor,expertise relevance factors
:aspect,"semantic closeness, shared topics, citations"
:model,UniPR
:technique,contextualized language model with instruction tuning
:dataset,four datasets
:field,"machine learning, computer vision, information retrieval, data mining"
:benchmark,state-of-the-art methods and models
:task,text style transfer
:goal,style control
:task,text fact transfer
:goal,factual content transfer
:framework,MODQGA
:technique,specificity-aware question answering
:hypothesis,reasoning tree embedding
:approach,MechanisticProbe
:task,k-th smallest element
:task,language-based reasoning tasks
:task,ProofWriter
:task,AI2 Reasoning Challenge
:finding,MechanisticProbe effectiveness
:domain,spreadsheets
:application,Excel OfficeScripts
:benchmark,InstructExcel
:feature,Automate in Excel
:dataset,10k samples of Excel operations
:technique,dynamic prompting
:resource,isomorphic problem bank
:challenge,traditional exams
:challenge,content sharing websites
:security,exam items
:task,creating isomorphic problems
:strategy,open access to problem banks
:hypothesis,open bank access impact
:context,mid-term physics exams
:`problem type`,open bank problems
:`problem type`,transfer problems
:analysis,item response theory
:`problem type`,open bank and transfer problems
:analysis,exploratory factor analysis
:concept,problem correlation
:goal,exam reform
:task,event coreference resolution
:process,grouping event mentions
:framework,"encoding first, then scoring"
:method,current methods in ECR
:knowledge,human-summarized ECR rules
:model,CorefPrompt
:process,simultaneous event modeling and coreference discrimination
:task,event-type compatibility
:task,argument compatibility
:benchmark,state-of-the-art benchmark
:framework,Questions Under Discussion (QUD)
:process,discourse progression
:task,QUD parsing
:question,QUD-generated question
:attribute,curiosity-driven and open-ended
:task,QUD parsing evaluation
:dataset,QUD-Eval
:metric,existing evaluation metrics
:QUD,human-authored QUDs
:task,QUD parsing and evaluation
:approach,sequence-to-sequence grammatical error correction
:issue,parallel data requirement and lack of token correctness awareness
:data,parallel data
:attribute,noisy and limited
:feature,explicit awareness of token correctness
:framework,unified decoding intervention
:component,external critic
:critic,pre-trained left-to-right language model critic
:critic,incremental target-side grammatical error detector critic
:benchmark,strong baselines and state-of-the-art methods
:concept,grounded knowledge
:task,natural-language navigation tasks
:concept,spatial structures
:strategy,landmarks for spatial maps
:factor,spatial and non-spatial factors
:concept,spatial structure
:task,benchmark tasks
:benchmark,sentence planning benchmark
:attribute,different granularities
:`model family`,finetuned smaller model
:attribute,fine-grained hard constraints
:activity,pandemic forecasting and analysis
:goal,public health management
:approach,traditional pandemic forecasting
:`data type`,epidemiological data
:framework,MGL4MEP
:technique,temporal graph neural networks
:`data source`,social media content
:concept,pandemic dynamics indicators
:fusion,temporal graph learning and multi-modal data
:goal,comprehensive pandemic understanding
:concept,cross-lingual sociopragmatic meaning
:benchmark,SPARROW
:gap,sociopragmatic meaning understanding
:data,multilingual datasets
:attribute,language diversity
:`model family`,medical instruction-tuned large language model
:limitation,limited scope of tasks and instructions
:`model series`,Llama-series models
:dataset,MedInstruct-52k
:model,AlpaCare
:attribute,medical proficiency and generalizability
:`test set`,MedInstruct-test
:profession,clinician
:`project page`,AlpaCare GitHub page
:concept,social norms
:activity,interpersonal communication
:dataset,NormDial
:task,social norm observance detection
:language,Chinese and English
:finding,nuances of social norms
:metric,factual output frequency
:tool,fact verifier
:goal,progress measurement
:model,FLAN-T5-11B
:factor,high-quality evidence
:goal,trustworthy generation models
:task,image reasoning tasks
:model,LLaMA-1.5
:concept,language prior
:issue,language hallucination
:`model component`,vision module
:benchmark,HallusionBench
:issue,visual illusion
:analysis,detailed analysis of HallusionBench
:resource,HallusionBench codebase
:application,radiology reports
:`model family`,state-of-the-art radiology-specific models
:benchmark,radiology task performance
:technique,example-based prompting
:professional,board-certified radiologist
:task,findings summarisation
:standard,manually-written impressions
:framework,large search model
:system,conventional search stack
:task,search tasks
:problem,autoregressive text generation
:technique,natural language prompting
:capability,language understanding and reasoning
:experiment,proof-of-concept
:system,real-world search systems
:task,unsupervised text style transfer
:goal,style-content separation
:concept,prefix
:task,encoding task-specific information
:attribute,richer information
:strategy,recursive language model usage
:system,question answering system
:task,community-based question answering
:topic,COVID-19
:`model family`,transformer
:model,community-based COVID-19 question answering models
:domain,high-stake applications
:`model family`,traditional machine learning model
:technique,label-flipping
:task,ambiguous text translation
:system,translation systems
:topic,grammatical ambiguities
:topic,semantic ambiguities
:dataset,TIDE
:topic,idioms with disambiguating context
:`model family`,machine translation-specific models
:topic,literal translation of idioms
:topic,context-aware translation
:potential,language models as backbone for translation
:system,context-aware translation systems
:task,emotion recognition in conversation
:goal,empathetic machines
:method,data-driven fine-tuning
:method,cross-task prompt tuning
:task,few-shot conversational emotion recognition
:concept,cross-task knowledge
:attribute,parameter efficiency
:experiment,CTPT evaluation
:data,contextual conversation datasets
:approach,coarse-to-fine chain-of-thought
:capability,multi-grained task solving
:representation,abstract meaning representation
:concept,utterance nuances and structures
:task,multi-grained natural language understanding
:task,e-commerce pre-sales dialogue
:goal,understanding user needs
:task,mimicking pre-sales dialogues
:combination,LLM and CRS collaboration
:method,CRS assisting LLM
:method,LLM assisting CRS
:data,real-world e-commerce pre-sales dialogues dataset
:analysis,collaborative approaches impact
:finding,CRS and LLM collaboration effectiveness
:technique,Chain of Thought
:technique,Program of Thought
:task,math reasoning tasks
:framework,XOT
:technique,method selection
:entity,external executors
:technique,method switching
:trend,improvements on single reasoning methods
:concept,collaborative integration of diverse reasoning thoughts
:task,training language models on long inputs
:attribute,fixed sequence length
:method,adapting models to longer inputs
:method,segmented sequence training
:method,interpolation-based positional embedding extension
:procedure,extending input context size
:technique,sub-sampling segments from long inputs
:`model family`,language model with absolute positional embeddings
:`model family`,language model with relative positional embeddings
:method,extending input contexts by a factor of 4x
:capability,complex language understanding
:`use case`,reasoning for ambiguity resolution
:resource,benchmark dataset for ambiguity resolution
:capability,reasoning-based ambiguity resolution
:dataset,ambiguous definite descriptions dataset
:method,prompt-based ambiguity resolution
:resource,code and data for ambiguity resolution
:URL,https://github.com/sfschouten/exploiting-ambiguity
:`data type`,human gaze data
:process,natural language comprehension
:technique,scanpath-augmented language model
:`data type`,text corpora
:model,synthetic scanpath generator
:task,reading
:`data type`,synthetic gaze data
:model,integrated synthetic scanpath and language model
:technique,scanpath-augmented language model with real human gaze data
:paper,Landauer and Dumais (1997)
:test,TOEFL test
:`model family`,major language model
:task,vocabulary test
:task,nonword discrimination test
:observation,provision of non-existent information
:task,language generalization test
:`model family`,best performing language model
:test,vocabulary and nonword tests
:task,nonword identification
:task,open domain conversational question answering (ODConvQA)
:task,baseline reevaluation
:pipeline,Dense Passage Retrieval (DPR) and Fusion-in-Decoder (FiD)
:component,fast reranking
:pipeline,retriever-reader
:technique,targeted finetuning
:dataset,TREC Conversational Assistance Track (TREC CAsT)
:dataset,OR-QuAC
:result,improved SOTA results and reduced latency
:method,proposed baselines
:insight,development of challenging baselines
:field,ODConvQA research
:work,existing work
:association,knowledge-time association
:concept,time axis
:model,ReMemo
:technique,relative time modeling
:capability,modeling long-range temporal dependencies
:resource,code and pre-trained checkpoints
:url,https://github.com/damo-nlp-sg/rememo
:phenomenon,LLM-generated text proliferation
:need,LLM-generated text detection
:technique,detector techniques
:technique,zero-shot methods
:technique,adversarial learning
:technique,human-assisted methods
:survey,LLM-generated text detection survey
:`research area`,detector research
:dataset,LLM-generated text detection datasets
:paradigm,LLM-generated text detection paradigms
:`research direction`,LLM-generated text detection future research
:value,responsible artificial intelligence
:resource,LLM-generated text detection resources
:task,contradiction detection
:corpus,condensed corpus of prototypical contradictions
:task,linguistic analysis
:task,language model fine-tuning
:data,contradicting statements
:concept,new contradiction typologies
:approach,linguistic rules
:data,simple contradictions
:result,coherence and variety of data
:task,further studies and manual refinement
:goal,machine learning setup utilization
:process,structuring input text
:technique,role-prompting
:technique,one-shot prompting
:technique,tree-of-thoughts prompting
:`research direction`,prompt engineering research
:understanding,structures and agents in AIGC
:assessment,efficacy of prompt methods
:application,prompt engineering in education
:application,prompt engineering in programming
:survey,prompt engineering survey
:audience,anyone interested in LLMs and prompt engineering
:technique,Chain of Thought prompting
:interest,transferring reasoning abilities
:challenge,diversity and consistency in rationales
:technique,Multi-CoT Consistent Knowledge Distillation (MCC-KD)
:goal,enhancing diversity and consistency in rationales
:method,minimizing bidirectional KL-divergence
:`model architecture`,LLaMA/FLAN-T5
:`model scale`,3B/7B/11B/13B
:outcome,superior performance and robust generalization
:field,Natural Language Processing (NLP) for social media
:field,general-purpose NLP
:landscape,NLP for social media
:issue,model comparison difficulty
:benchmark,SuperTweetEval
:resource,heterogeneous tasks and datasets
:field,social media NLP
:task,knowledge base completion
:challenge,scaling and accuracy
:study,prior experimental studies
:limitation,limited evaluation scope
:approach,earlier smaller LMs
:data,internet data
:risk,imbalance in data
:issue,geographical erasure
:`data attribute`,country mention frequency
:data,training corpus
:technique,finetuning with custom objective
:concept,concept
:context,absence of sentence context
:work,previous work on concept embeddings
:concept,decontextualised concept embeddings
:perspective,different perspectives
:concept,concept embedding
:structure,taxonomic structure
:strategy,identifying shared properties
:property,shared properties
:task,ultra-fine entity typing
:problem,multi-label classification
:method,augmenting label set with shared properties
:model,state-of-the-art models for ultra-fine entity typing
:capability,image-sharing
:framework,two-stage framework for image-sharing
:technique,restriction-based prompt template
:dataset,PhotoChat++
:study,first study on LLMs' image-sharing ability in zero-shot setting without visual foundation models
:resource,source code and dataset
:event,publication
:`industry need`,structured information extraction
:model,Text2Topic
:`model architecture`,bi-encoder transformer
:technique,embeddings manipulation
:capability,batch-inference
:dataset,text-topic pairs annotations
:source,Booking.com
:method,smart sampling
:platform,stream processing platform
:metric,MAP score
:study,modeling choices
:study,in-production decision-making
:task,multiple domain tasks
:capability,handling new knowledge
:approach,KnowGen
:knowledge,artificial entities
:benchmark,Alcuna
:performance,new knowledge handling
:factor,entity similarity
:capability,knowledge understanding
:factor,contextual entities
:scenario,new scenarios and knowledge
:application,end-to-end software construction
:profession,software developers
:application,web-based application
:implementation,reference implementation
:approach,top-down prompt construction
:concept,high-level software description
:approach,bottom-up prompt construction
:concept,system features
:task,Neural Machine Translation (NMT)
:task,Machine Translation (MT)
:process,fine-tuning for translation
:approach,automatic post-editing (APE)
:approach,document-level translation
:technique,low-rank-adapter fine-tuning
:metric,sentence and document-level metrics
:performance,state-of-the-art accuracy rate
:dataset,ContraPro test set
:task,pronoun ambiguity resolution
:scenario,manual post-editing for document-level translation
:strategy,leveraging human corrections
:demo,interactive demo for integrating manual feedback
:platform,Hugging Face Spaces
:ability,step-by-step reasoning
:`linguistic phenomenon`,negation
:setting,controlled settings for logical reasoning
:`linguistic phenomenon`,lexical negation
:limitation,unique limitations
:domain,legal
:task,legal case analysis and reasoning
:corpus,novel legal corpus
:`legal document`,Contract Acts Malaysia and Australian Social Act
:framework,IRAC method
:annotation,IRAC analysis
:audience,machines and legal professionals
:assessment,empirical assessment of ChatGPT for IRAC analysis
:goal,alignment with legal reasoning
:attribute,desired text attributes
:method,decoding-time-based methods
:phenomenon,attribute collapse
:issue,decreased text fluency
:framework,AIR-Decoding
:attribute,attribute distributions
:method,attribute distribution reconstruction
:technique,language model guidance
:experiment,controllable text generation tasks
:concept,brain localization
:ability,human-level linguistic competence
:region,core region in LLMs
:phenomenon,dimension dependency
:improvement,knowledge level
:region,domain knowledge region
:region,linguistic region
:exploration,LLMs' functional regions
:concept,intelligence foundation
:`future work`,investigate knowledge regions within LLMs
:performance,exceptional performance
:limitation,request restrictions
:limitation,data privacy concerns
:limitation,lacking local deployment capabilities
:framework,LDST
:`model family`,open-source foundation model
:technique,domain-slot instruction tuning
:issue,inconsistency with source
:method,fidelity-enriched contrastive search (FECS)
:method,contrastive search
:quality,semantic similarity
:benchmark,well-performing decoding algorithms
:value,model trust
:measure,representation dissimilarity measures
:field,language model mechanics
:`activation function`,SOLU
:property,generalization properties
:evaluation,new evaluations
:feature,language model features
:attribute,model width and depth
:model,PCFG
:technique,low-rank parameterization
:model,HMM
:model,SimplePCFG
:formalism,independent productions
:task,language modeling and unsupervised parsing
:algorithm,FlashInside
:concept,animacy
:field,cognitive processing
:concept,animacy processing
:entity,atypically animate entity
:concept,animate
:concept,atypical animacy
:concept,subtle clues
:concept,lexical semantic nuances
:issue,training data concerns
:information,training corpus details
:task,document-level membership inference
:method,document-level membership inference development and evaluation
:method,black-box document-level membership prediction
:model,OpenLlama-7B
:`performance metric`,AUC
:method,sentence-level membership inference
:model,OpenLlama-3B
:finding,document-level membership inference accuracy
:publication,Brown et al. (2020)
:phenomenon,meta-out-of-context learning
:method,synthetic experiments
:behavior,internalization of semantic content
:content,broadly useful semantic content
:examples,"true statements, authoritative text"
:hypothesis,knowledge storage in parameters
:hypothesis,gradient alignment bias
:reflection,implications for future AI capabilities
:URL,https://github.com/krasheninnikov/internalization
:dataset,TeleQnA
:domain,telecommunications
:data,"10,000 questions and answers"
:framework,automated question generation
:context,telecom knowledge
:profession,active telecom professionals
:`learning paradigm`,continual federated learning
:`algorithm type`,replay-based algorithms
:strategy,simple replay sample selection
:objective,replay sample selection based on loss gradient diversity
:`selection method`,relaxation-based selection of samples
:algorithm,practical algorithm for coordinated gradient-based replay sample selection
:benchmark,coordinated and uncoordinated replay sample selection algorithms
:baseline,random sampling-based baselines
:data,large scale de-identified real-world text dataset
:method,gradient-based sample selection
:method,coordination method
:condition,low replay size regime
:audience,service providers
:context,service
:tool,CoAgent
:concept,service co-creation
:process,participatory design
:organization,public libraries
:finding,heuristics for service co-creation with AI
:concept,agency aspects for AI
:value,ethical AI
:model,participatory design model
:concept,AI as stakeholders
:interaction,AI-AI interaction
:concept,blind spots identification
:insight,human-AI co-creation insights
:concept,human-AI co-creation
:ecosystem,workforce ecosystem
:concept,AI coexistence
:capability,active grounding of task instructions
:step,localizing and tracking key active objects
:perspective,pure vision
:problem,localizing and tracking objects
:modality,textual
:knowledge,action-object knowledge
:technique,per-object aggregation masking
:task,joint inference
:dataset,Ego4D and EPIC-Kitchens
:metric,standard metrics and average precision
:task,table-based question answering
:toolkit,TableQAToolkit
:resource,TableQA datasets
:benchmark,state-of-the-art (SOTA)
:benchmark,LLM-based TableQA benchmark
:task,parameter update reduction
:issue,non-independent and identically distributed data
:issue,client drift
:approach,FedPEPTAO
:approach,partial prompt tuning
:method,adaptive optimization
:resource,FedPEPTAO code
:activity,analyzing qualitative data
:requirement,multiple human coders
:characteristic,labor-intensive and time-consuming
:capability,replicating human-like behavior
:group,crowd workers
:framework,human-LLM collaboration
:data,survey datasets
:outcome,reduced labor and time demands
:`learning approach`,self-supervised representation learning
:challenge,capturing structural context information
:method,existing self-supervised representation learning methods
:challenge,reliance on task-specific training labels
:model,Grenade
:problem,self-supervised representation learning on text-attributed graphs
:technique,synergistic effect of language model and graph neural network
:`learning algorithm`,graph-centric contrastive learning
:`learning algorithm`,graph-centric knowledge alignment
:resource,GitHub repository for Grenade
:capability,linguistic capability
:skill,human language skills
:study,existing studies on LLMs
:capability,human generalization
:capability,morphology
:analysis,rigorous analysis of morphological capabilities
:test,Wug Test
:system,purpose-built systems
:claim,human-like language skills
:`data type`,observational data
:criterion,backdoor criterion
:process,causal effect inference
:claim,complete graph information not necessary
:concept,topological order
:task,eliciting causal order
:task,determining graph edges
:role,virtual domain experts
:technique,prompting strategies with LLMs
:task,obtaining causal order
:integration,LLMs with causal discovery algorithms
:process,causal discovery
:outcome,improved causal ordering accuracy
:task,language generation and evaluation
:limitation,lack of coherence and problem decomposition
:method,Branch-Solve-Merge (BSM)
:task,challenging natural language tasks
:component,"branch, solve, and merge modules"
:model,Llama-2-Chat
:task,LLM response evaluation
:task,constrained text generation
:task,scene rearrangement instructions
:technique,few-shot example prompting
:limitation,fixed prompts
:task,adapting to user's idiosyncratic procedures
:agent,HELPER
:task,parsing human-robot dialogue
:technique,retrieval-augmented LLM prompting
:memory,external memory of language-program pairs
:benchmark,TEACH
:metric,execution from dialog history (EDH)
:metric,trajectory from dialogue (TFD)
:resource,project's website
:URL,https://helper-agent-llm.github.io
:phenomenon,decrease in quality
:application,NLP tools
:`language variant`,minority language variants
:`research limitation`,limited studies
:`research context`,monolingual studies
:`research limitation`,cross-linguistic trend identification
:application,machine translation and automatic speech recognition
:language,high-resource languages
:evaluation,regional dialect functionality
:gap,regional dialect gap
:factor,"economic, social, and linguistic factors"
:approach,one-size-fits-all approach
:field,dialectal NLP
:goal,addressing dialectal disparities
:`attack type`,manual jailbreak attacks
:characteristic,human-readable and blockable
:characteristic,gibberish prompts detectable by perplexity filters
:attack,interpretable adversarial attack
:method,Autodan
:outcome,attack prompts bypassing filters
:outcome,interpretable and diverse prompts
:application,leaking system prompts
:purpose,understanding jailbreak attack mechanisms
:characteristic,slow token generation
:process,parallel token scoring
:problem,maximal-coupling
:algorithm,optimal draft selection
:technique,linear programming
:algorithm,new draft selection
:characteristic,efficient computation
:`sampling algorithm`,SPECTR
:metric,wall clock speedup
:paradigm,pre-train and fine-tune
:field,robotic reinforcement learning
:challenge,human effort in reinforcement learning
:system,RoboFume
:paradigm,reset-free fine-tuning
:technique,calibrated offline reinforcement learning
:process,online fine-tuning
:component,reward classifier
:method,RoboFume's method
:resource,existing robot dataset
:task,target task
:capability,reasoning and long-context understanding
:task,evaluation of capabilities
:`evaluation method`,complex synthetic tasks
:`evaluation suite`,S3Eval
:concept,test set contamination mitigation
:capability,systematic probing of LLM capabilities
:benchmark,Big-Bench Hard (BBH)
:feature,sentiment
:domain,natural language text
:direction,sentiment direction
:method,causal interventions
:dataset,Stanford Sentiment Treebank
:component,attention heads and neurons
:phenomenon,summarization motif
:position,comma positions
:strategy,prompting-based strategies
:approach,LINC
:tool,external theorem prover
:task,deductive inference
:model,Starcoder+
:task,logical reasoning over natural language
:resource,LINC code
:URL,https://github.com/benlipkin/linc
:task,visual reasoning
:capability,multimodal perception and commonsense cognition
:task,collective power harnessing
:method,ensemble
:task,model aggregation
:paradigm,CoLA
:variant,CoLA-FT
:variant,CoLA-Zero
:performance,competitive
:model,Recognize Anything Plus Model (RAM++)
:task,open-set image tagging
:technique,global text supervision
:technique,multi-grained text supervision
:benchmark,state-of-the-art open-set image tagging models
:model,RAM
:benchmark,HICO
:resource,"code, datasets and pre-trained models"
:framework,multiple experts fine-tuning
:model,Disc-FinLLM
:abilities,specialized financial abilities
:dataset,Disc-Fin-SFT
:categories,instruction sample categories
:resource,Disc-FinLLM GitHub repository
:URL,https://github.com/fudandisc/disc-finllm
:ability,cross-lingual logical reasoning
:ability,monolingual reasoning transfer
:ability,code-switched reasoning
:dataset,RuleTaker
:dataset,LeapOfThought
:mechanism,novel attention mechanism
:platform,community question-answering portal
:function,helping users
:challenge,accessibility for non-English speakers
:action,translating questions
:goal,broadening community reach
:environment,noisy
:data,synthetic parallel corpus
:methodology,fine-tuning NMT with source-side data
:challenge,noisy environment translation
:metric,BERTScore and MLM score
:approach,MLE based fine-tuning
:model,proposed NMT model
:goal,facilitating research
:framework,detection frameworks
:strategy,eluding detection
:work,survey
:`model family`,neural topic model
:task,clustering
:framework,Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DETiME)
:gap,NTMs' topic generation capability
:capability,content generation
:task,efficient topic and content production
:capability,generating clustered embeddings
:attribute,trainability and adaptability
:scope,wide array of applications
:phenomenon,popularity of conversational digital assistants
:resource,conversational data
:goal,personalized response generation
:metric,textual similarity metrics
:task,analysis and evaluations
:task,task-oriented conversations
:metric,TaskDiff
:concept,conversational similarity metric
:component,dialogue components
:application,code-tracing question generation
:technique,targeted prompting
:quality,question quality
:analysis,model capabilities and potential
:dataset,human and LLM-generated tracing questions
:community,education and NLP research communities
:dialogue,potential uses of LLMs in educational settings
:setting,educational
:task,video classification
:component,text-based class labels
:modification,language-guided visual feature enhancement
:model,video-to-text model
:modification,video-specific prompts
:structure,tree hierarchy of categories
:task,video action recognition
:task,video-to-text and text-to-video retrieval
:task,time-sensitive video tasks
:`model type`,generalist model
:`model type`,specialist model
:technique,generalist instruction tuning
:hypothesis,efficacy of generalist instruction tuning
:factor,task specificity and skill requirements
:experiment,assessment of target tasks
:outcome,enhanced model performance
:condition,limited task-specific training data
:capability,understanding and reasoning
:resource,code and related resources
:URL,https://github.com/davidfanzz/generalist_or_specialist
:condition,evidential closure
:capability,statistical identification of truth
:`learning type`,"perceptual, extensional, and intensional learning"
:`model family`,unimodal large language model
:condition,synonymity with validated evidence
:procedure,Learn-Babble-Prune
:outcome,faithful output
:era,age of artificial intelligence
:capability,knowledge consolidation
:study,first study on LLMs' knowledge consolidation
:benchmark,EPIK-Eval
:evaluation,LLMs' knowledge consolidation proficiency
:`training objective`,prevailing training objectives
:weakness,knowledge consolidation weaknesses
:approach,knowledge consolidation approach
:resource,code and benchmark for EPIK-Eval
:URL,https://github.com/chandar-lab/epik-eval
:task,data selection and curriculum design
:method,standard training
:scheme,domain-level selection
:concept,individual training point contributions
:method,traditional datapoint selection
:method,online batch selection
:concept,extra computational costs
:algorithm,irreducible curriculum
:task,language model pretraining
:model,proxy model
:concept,sample loss simulation
:dataset,RedPajama-1B
:metric,validation perplexity
:strategy,anti-curriculum
:concept,network sharpness
:metric,5-shot accuracy
:`data corpus`,pretraining data corpus
:`source domains`,various source domains
:examples,"CommonCrawl, Wikipedia, GitHub"
:goal,optimizing domain weights for generalization
:method,DOGE
:function,gradient-based generalization estimation function
:technique,mirror descent
:goal,maximizing generalization gain
:model,full-size language model
:dataset,SlimPajama-6B
:task,out-of-domain generalization tasks
:scheme,parameter-selection scheme
:system,caption generation system
:output,scientific figure captions
:challenge,evaluation of caption generation
:resource,academic expertise
:input,author-written captions
:dataset,SciCap-Eval
:data,human judgments
:task,caption evaluation
:role,zero-shot evaluator
:group,computer science and informatics undergraduates
:group,Ph.D. students
:task,discovering latent semantics
:issue,co-occurrence information scarcity
:`text type`,short texts
:`model family`,topic model
:approach,short-text topic modeling
:solution,neural topic model extension
:issue,noisy text generation
:evaluation,Theory of Mind (ToM) evaluations
:method,passive narratives
:benchmark,Fantom
:task,question formulation
:application,chatbot customization
:task,feedback conversion
:need,feedback to principles support
:classification,principle types
:tool,ConstitutionMaker
:feature,automatic principle generation
:`user study`,ConstitutionMaker evaluation
:outcome,improved principle guidance
:task,chatbot improvement
:field,mobile user interface
:task,UI layout generation
:approach,UI grammar
:value,explainability and controllability
:`intervention type`,self-guided mental health intervention
:goal,improving mental health care access
:issue,accessibility barriers
:`interaction type`,human-language model interaction
:technique,cognitive restructuring
:`study type`,IRB-approved randomized field study
:platform,mental health website
:system,language model-based support system
:outcome,emotional intensity reduction
:outcome,overcoming negative thoughts
:demographic,adolescents
:`intervention type`,tailored intervention
:era,post-2018
:innovation,linguistic prowess
:industry,language model industry
:risk,personal identifiable information disclosure
:process,web-based data acquisition
:risk,privacy infringement
:interface,fine-tuning interface for GPT-3.5
:question,fine-tuning and personal information leakage
:`exploitation avenue`,Janus attack
:task,PII association task
:state,divulging concealed PII
:research,Janus attack vector study
:balance,LLM utility and privacy preservation
:data,private instruction data
:approach,transfer of parameterized modules
:technique,offsite-tuning (OFT)
:component,Transformer blocks
:concept,representation and functional similarity
:characteristic,modular structure
:strategy,CRASH
:concept,loss landscape
:concept,linear connectivity
:concept,optima
:URL,https://github.com/tsinghuac3i/crash
:task,long-range language modeling
:issue,ineffective memories
:strategy,Training-Free Memory Selection (TRaMS)
:task,attention calculation token selection
:metric,simple metric for high attention score
:benchmark,enwik8
:outcome,improvement without additional training or parameters
:task,constraint satisfaction queries
:method,web-search and knowledge bases
:benchmark,current retrieval benchmarks
:task,constraint satisfaction
:concern,factual incorrectness and hallucinations
:dataset,KiTAB
:approach,dynamic data collection and constraint verification
:result,limitations of models without context
:result,context availability
:issue,irrelevant information
:barrier,fundamental barriers to constraint satisfaction
:contribution,open source contributions
:risk,misuse for disinformation
:strategy,Fighting Fire with Fire (F3)
:technique,paraphrase-based prefix-style prompts
:content,authentic LLM-generated content
:technique,perturbation-based prefix-style prompts
:content,deceptive LLM-generated content
:technique,zero-shot in-context semantic reasoning
:task,discerning genuine from deceptive content
:technique,cloze-style prompts
:performance,zero-shot superiority
:resource,codebase and dataset for F3
:URL,https://github.com/mickeymst/f3
:model,Stelocoder
:task,multi-programming language-to-Python code translation
:feature,language-agnostic code translation
:technique,mixture-of-experts
:technique,low-rank adaptive method
:data,self-instruct data
:dataset,XLCost
:performance,CodeBLEU score
:benchmark,leaderboard performance
:hardware,80GB A100 HBM
:behavior,non-humanlike behavior
:phenomenon,inconsistent predictions
:strategy,specialised loss functions
:limitation,limited usage of previous strategies
:approach,meaning awareness improvement
:theory,Conceptual Role Theory
:technique,efficient parameter integration
:result,improvement in multiple types of consistency
:feature,efficient knowledge integration
:capability,applicability to other languages
:trend,applications built on LLMs
:`model family`,retrieval-augmented large language model
:issue,increased cost from large input token size
:scheme,token compression
:method,summarization compression
:method,semantic compression
:dataset,Food-Recommendation DB (FRDB)
:`performance metric`,reduced token size and improved accuracy
:`performance metric`,trade-off between token size and performance
:task,multiple choice reasoning tasks
:approach,equal treatment of options
:strategy,two-step strategy
:method,Process of Elimination (PoE)
:approach,scoring and elimination
:experiment,zero-shot experiments
:task,logical reasoning tasks
:element,masks
:process,eye movement in reading
:relationship,eye movement and cognition coupling
:task,language-related machine learning tasks
:challenge,scarcity of eye movement data
:field,eye movement research
:solution,cognitive models for synthesizing eye movement data
:method,data-driven machine-learning-based methods
:task,generating human-like scanpaths
:model,ScanDL
:task,generating synthetic scanpaths on texts
:technique,joint embedding
:benchmark,state-of-the-art scanpath generation methods
:behavior,human-like reading behavior
:implementation,ScanDL GitHub repository
:URL,https://github.com/dili-lab/scandl
:challenge,deployment in real-world applications
:goal,extreme model compression
:paradigm,retrieval-based knowledge transfer
:goal,knowledge transfer to small-scale models
:approach,RetriKT
:resource,knowledge store
:model,small-scale model
:task,low-resource tasks
:task,text-annotation tasks
:role,complementary annotators
:paradigm,CoAnnotating
:task,co-annotation of unstructured texts
:capability,LLMs' annotation capability
:url,https://github.com/salt-nlp/coannotating
:phenomenon,synthetic content generation
:sector,multiple sectors
:task,LLM-generated content detection
:importance,paramount importance
:survey,detection strategies and benchmarks survey
:concept,challenges and prospects
:approach,multi-faceted approach
:goal,defending against attacks
:milestone,first comprehensive survey
:understanding,current landscape of LLM-generated content detection
:resource,awesome_papers_on_llms_detection
:sector,"media, cybersecurity, public discourse, education"
:`context length`,more than 30 seconds of acoustic context
:literature,speech recognition literature
:`model type`,dense-attention based acoustic and language model
:dataset,Spotify podcasts
:experiment,scaling sequence length
:`context length`,5 seconds to 1 hour
:dataset,long-format datasets
:`context length`,around 80 seconds of acoustic context
:system,long-context ASR system
:action,disabling copy-pasting
:outcome,high-quality but homogeneous responses
:consequence,harm to research
:consequence,degradation of future models
:action,requesting not to use LLMs
:outcome,lower quality responses
:trend,increasing popularity of LLMs
:concept,co-evolution of LLM-based tools and users
:characteristic,domain-specific characteristics
:task,biomedical abstractive summarisation
:model,attention-based citation aggregation model
:knowledge,domain-specific knowledge from citation papers
:task,generating summaries
:dataset,large-scale biomedical summarisation dataset
:field,biomedical summarisation
:task,abstractive biomedical text summarisation
:activity,sharing cooking recipes
:concept,culinary idea exchange
:challenge,categorizing raw recipes
:issue,lack of labeled data
:dataset,3A2M+ Cooking Recipe Dataset
:data,culinary recipes
:feature,recipe features
:task,recipe-related tasks
:pipeline,3A2M+
:technique,Named Entity Recognition (NER) extension
:task,recipe genre classification
:feature,title
:task,adapting to new queries or feedback
:task,retraining language models
:challenge,resource and privacy constraints
:method,continual optimal policy fitting
:technique,Monte Carlo method
:technique,complex reinforcement learning
:benchmark,continuous learning baselines
:`biological process`,fMRI activations
:task,predicting brain responses
:technique,ensemble model
:`model family`,syntactic language model
:`model family`,semantic language model
:benchmark,current baselines
:technique,ensembling methods
:method,Variator
:component,compression plugin
:advantage,dynamic selection
:advantage,storage and memory saving
:result,computational cost saving
:`model scale`,billions of parameters
:issue,repeating similar mistakes
:framework,Tuning-Free Rule Accumulation (Tran)
:collection,rule collection
:benchmark,recent baselines
:task,natural language understanding and generation tasks
:concept,linguistic shortcuts
:task,Video Question Answering (VideoQA)
:issue,linguistic bias
:phenomenon,ungrounded guesses
:framework,Flipped-VQA
:model,LLaMA-VQA
:goal,performance enhancement
:resource,Flipped-VQA code
:url,https://github.com/mlvlab/flipped-vqa
:method,default masculine generics
:preference,speaker's gender preference
:solution,existing gender assignment solutions
:characteristic,hardly feasible in practice
:solution,inference-time gender control
:approach,partial replacement of internal LM
:experiment,en->es/fr/it gender accuracy
:metric,gender accuracy
:condition,speakers' vocal traits conflict with gender
:benchmark,BLESS
:`model attribute`,model diversity
:evaluation,BLESS analysis
:method,manual qualitative analysis
:finding,BLESS evaluation
:`model performance`,comparable to TS baselines
:capability,edit operation diversity
:resource,development of TS methods and metrics
:approach,large-scale model development
:approach,lightweight custom model development
:model,MindLLM
:`model family`,bilingual lightweight large language model
:framework,instruction tuning framework
:process,large model development
:audience,academics and developers
:task,GUI testing
:quality,app quality
:technique,learning-based techniques in automated GUI testing
:limitation,low testing coverage
:limitation,inadequate generalization capabilities
:limitation,heavy reliance on training data
:task,mobile GUI testing
:model,GPTdroid
:mechanism,functionality-aware memory prompting
:platform,Google Play
:outcome,bug detection
:task,terminology extraction
:attribute,burstiness
:term,technical terminology
:measure,term burstiness
:paradigm,statistical significance testing
:model,multinomial language model
:heuristic,proxy for test p-values
:relationship,inverse document frequency and inverse collection frequency
:benchmark,GENIA Term Corpus
:evaluation,heuristic evaluation
:approach,proposed heuristic
:demonstration,statistical significance testing in text analysis
:method,neural network subspaces
:method,parameter efficient fine-tuning
:concept,simplex of continuous prefixes
:benchmark,GLUE benchmark for few-shot learning
:metric,average performance
:implementation,prefix_subspace
:url,https://github.com/liloulou/prefix_subspace
:bias,social bias
:`model family`,foundational language model
:bias,ingroup-positive and outgroup-negative bias
:`model family`,instruction fine-tuned language model
:source,human-written internet text
:experiment,fine-tuning variation
:bias,ingroup solidarity and outgroup hostility
:technique,biased fine-tuning
:technique,bias reduction in training data
:bias,social identity bias
:technique,curating training data
:goal,preventing bias reinforcement
:`research area`,language model prompt optimization
:technique,automatically generated token sequences
:technique,manually crafted prompts
:technique,machine-generated prompts
:`model behavior`,response to non-natural language input
:`model size`,different sizes
:task,semantic tasks
:`prompt type`,human-generated natural-language prompts
:`model behavior`,response patterns
:`prompt type`,machine-generated vs human-generated prompts
:`model behavior`,network processing pathways
:metric,"perplexities, attention and output entropy distributions, unit activation profiles"
:unit,linguistic circuit
:`prompt type`,natural language prompts
:misuse,LLM misuse
:consequence,negative societal consequences
:approach,safety training
:goal,enhanced safety
:approach,safeguards
:goal,preventing harmful outputs
:issue,adaptability and performance degradation
:evaluation,limited effectiveness
:approach,Self-Guard
:approach,safety training and safeguards
:capability,harmful content detection
:evaluation,robustness against jailbreak attacks
:response,harmless responses to harmful queries
:issue,over-sensitivity
:property,linguistic properties
:topic,linguistic properties discovery
:corpus,artificial corpus
:language,French
:property,gender information
:task,health suggestions in single-turn conversations
:model,ChatDoctor
:model,DoctorGLM
:limitation,single-turn conversation
:issue,inadequate personalization and targeting
:practice,multi-turn questioning
:practice,chain of questioning (COQ)
:profession,doctors
:model,BianQue
:dataset,BianQueCorpus
:capability,questioning and health suggestions
:field,proactive health
:technology,synthetic text generation
:technology,synthetic text identification
:phenomenon,emotion encoding in text
:phenomenon,affective deficit
:phenomenon,affective incoherence
:detector,emotionally-aware synthetic text detector
:phenomenon,emotion
:resource,"code, models, and datasets"
:process,fact integration
:phenomenon,competition within model
:dataset,world capitals knowledge
:factor,training frequency
:technique,head attribution
:outcome,control model behavior
:method,scaling value vector
:outcome,in-context answer generation
:`body of evidence`,localization of model behaviors
:concept,dynamic control of model behavior
:framework,standard machine learning framework
:concept,best-fitting function
:concept,task vector
:claim,ICL compressing training set into task vector
:scope,range of models and tasks
:technique,traditional pruning
:metric,E-Sparse
:concept,information entropy
:concept,information richness
:technique,n:m sparsity
:technique,global naive shuffle
:technique,local block shuffle
:implementation,sparse-gemm on FasterTransformer
:outcome,model inference speed up
:outcome,memory saving
:task,negation interpretation
:dataset,semi-automatically generated dataset
:problem,LLMs understanding negation
:capability,generalization and inference
:task,negation understanding
:finding,LLMs struggle with negative sentences
:finding,lack of generalization in handling negation
:system,graph-based recommender system
:`data type`,ID-based data
:`data type`,implicit feedback data
:issue,noise and bias
:integration,LLM into traditional ID-based recommenders
:system,practical recommender system
:framework,RLMRec
:goal,enhanced representation learning
:paradigm,representation learning with LLMs
:aspect,semantic aspects of user behaviors and preferences
:framework,cross-view alignment framework
:space,semantic space of LLMs
:theory,mutual information maximization
:quality,representation quality
:quality,efficiency and robustness
:resource,implementation codes
:document,clinical records
:stakeholder,medical practitioners and researchers
:task,automating clinical note creation
:challenge,training language models for clinical notes
:issue,limited public data
:framework,NoteChat
:task,generating synthetic doctor-patient conversations
:evaluation,NoteChat evaluation
:quality,high-quality synthetic conversations
:instance,first instance of multiple LLMs cooperating
:intersection,AI and healthcare
:`learning type`,multi-label learning
:`learning type`,multi-class image classification
:`learning type`,single-positive multi-label learning
:`learning type`,single-label learning
:`dataset characteristic`,single labels
:approach,vision-language pseudo-labeling
:dataset,PASCAL VOC
:dataset,NUS-WIDE
:dataset,CUB-Birds
:URL,https://github.com/mvrl/vlpl
:data,source-target mappings
:perturbation,source side perturbation
:perturbation,target side perturbation
:attribute,output text distribution
:method,zero-shot-context
:concept,compiler correctness
:outcome,program behavior integrity
:technique,fuzzing
:task,compiler defect discovery
:technique,compiler fuzzing
:challenge,internal compiler behavior understanding
:technique,black- and grey-box fuzzing
:task,intricate optimization exercise
:technique,white-box techniques
:object,compiler codebase
:task,code generation/understanding
:tool,WhiteFox
:technique,white-box compiler fuzzing
:framework,dual-model framework
:model,analysis LLM
:output,optimization requirements
:model,generation LLM
:output,test programs
:evaluation,WhiteFox evaluation
:outcome,high-quality test generation
:outcome,bug discovery
:application,white-box fuzzing of complex software systems
:property,emergent generalization
:task,simple reasoning tasks
:concept,true algorithm
:task,length generalization on algorithmic tasks
:framework,unifying framework
:`programming language`,RASP
:conjecture,RASP-Generalization Conjecture
:insight,insights from study
:task,traditionally hard tasks
:model,min-degree-interpolator
:behavior,Transformers' out-of-distribution behavior
:limitation,sensitivity to visual detail size
:`image property`,visual subject size
:technique,human visual cropping
:technique,automatic visual cropping
:dataset,VQA datasets
:dataset,VQAv2 dataset subset
:`image property`,fine visual details
:application,detail-sensitive visual question answering
:model,VisProg
:domain,2D images
:model,ViperGPT
:concept,abstract concepts
:method,inference-only methods
:capability,learning and adaptation
:framework,Logic-Enhanced Foundation Model (LEFT)
:task,cross-domain grounding and reasoning
:component,LLM interpreter
:component,program executor
:task,cross-domain execution
:capability,strong reasoning ability
:capability,adaptability to new domains
:paradigm,on-demand information extraction
:demand,personalized information extraction
:format,structured tabular format
:benchmark,InstructIE
:tool,ODIE
:task,web software tasks
:operation,"click, scroll, text input"
:characteristic,inefficient and task-specific
:method,using filtered DOM elements as observations
:method,sequentially generating small programs
:technique,example-based learning
:method,WebWise method
:phenomenon,text-image inconsistency
:method,Woodpecker
:process,five-stage process
:approach,post-remedy
:benchmark,POPE
:model,MiniGPT-4
:model,MPlug-OWL
:resource,Woodpecker source code
:URL,https://github.com/bradyfu/woodpecker
:task,building safe and deployable AI applications
:challenge,democratic norms in RLHF systems
:theory,social choice theory
:concept,voting protocol
:goal,universal AI alignment
:policy,governance of AI systems
:policy,transparent voting rules
:goal,accountability
:strategy,narrow AI alignment
:capability,robust reasoning in complex settings
:trend,growing system capabilities
:dataset,MUSR
:task,multistep soft reasoning
:algorithm,neurosymbolic synthetic-to-natural generation
:characteristic,real-world domain correspondence
:mechanism,DP-Prompt
:goal,privacy preservation
:dataset,IMDb dataset
:attack,author de-anonymization attack
:attacker,static attacker
:attacker,adaptive attacker
:ability,situational understanding
:agent,human-like AI
:environment,synthetic environment for SU testing
:`model family`,chat-oriented large language model
:limitation,non-persistent in-context memory
:issue,hallucinated updates
:task,robust tracking of situation states
:performance,ChatGPT dialogue performance
:concept,risk
:resource,codebase for situational testing
:url,https://github.com/yangalan123/situationaltesting
:factor,expectations
:process,human sentence processing difficulty
:factor,retrieval from working memory
:model,unified cognitive model
:mechanism,self-attention in Transformer language models
:theory,cue-based retrieval theories of working memory
:phenomenon,similarity-based interference
:assumption,parallel memory retrieval operations
:model,recurrent neural language model with a single self-attention head
:theory,cognitive theories of memory
:phenomenon,interference effects in human sentence processing
:literature,medical literature
:challenge,keeping up with recent findings
:tool,closed-source summarization tool
:task,evaluation of summarization tools
:webapp,Clinfo.ai
:service,answering clinical questions
:task,information retrieval and abstractive summarization task
:system,retrieval-augmented large language model system
:dataset,PubMedRS-200
:system,OpenQA system
:`model architecture`,new architectures
:`model architecture`,pure attention
:`model architecture`,efficient block
:`model architecture`,Yin Yang Convolutional Network
:concept,bio-inspired approach
:`performance metric`,test accuracy
:url,https://github.com/nosaveddata/yinyang_cnn
:participant,federated learning participant
:data,privacy-sensitive text data
:`model family`,federated language model
:attack,existing attack
:goal,data extraction
:finding,novel finding
:issue,privacy-sensitive data leakage
:`model state`,intermediate round model snapshot
:technique,tampering with selective weights
:actor,malicious client
:data,privacy-sensitive data
:method,best-performing method
:metric,membership inference recall
:metric,private data reconstruction
:field,data science
:profession,data scientist
:technology,LLM-powered chatbots
:study,mixed-methods study
:challenge,conversing with LLM-powered chatbots
:method,contextual observations
:issue,contextual data retrieval
:issue,formulating prompts for complex tasks
:issue,adapting generated code to local environments
:issue,refining prompts iteratively
:recommendation,data brushing
:feature,context selection
:recommendation,inquisitive feedback loops
:communication,communications with AI-based assistants
:event,BLP Sentiment Shared Task
:workshop,BLP 2023
:conference,EMNLP 2023
:task,sentiment detection
:`text type`,social media text
:number,71 participants
:number,597 runs
:number,15 teams
:approach,classical machine learning models
:approach,fine-tuning pre-trained models
:artifact,datasets and evaluation scripts
:capability,sentence and document-level encoding
:`model property`,length generalizability
:vulnerability,length vulnerability
:method,unsupervised contrastive learning
:signal,semantic signal from document length
:concept,length attacks
:property,isotropy
:condition,length range of text in training
:framework,LA(SER)^3
:performance,state-of-the-art unsupervised performance
:task,sentence representation learning
:benchmark,standard information retrieval benchmark
:landscape,academic and industrial
:capability,"text understanding, analysis, and generation"
:concept,new knowledge introduction
:technique,direct fine-tuning
:technique,knowledge-based model editing (KME)
:survey,KME survey
:taxonomy,KME techniques taxonomy
:task,staying up to date
:task,continual training
:issue,lack of continual learning benchmarks
:benchmark,web-scale time-continual benchmarks
:benchmark,TIC-DataCompt
:benchmark,TIC-YFCC
:benchmark,TIC-RedCaps
:task,curated retrieval task
:repository,OpenCLIP
:model,recently trained models
:approach,rehearsal-based approach
:task,efficient training on time-continuous data
:`research area`,linguistic structure injection
:`training approach`,parameter-efficient fine-tuning (PEFT)
:architecture,mixture-of-linguistic-experts
:component,parallel adapter modules
:mechanism,gumbel-softmax gates
:concept,module importance
:strategy,pruning experts
:benchmark,state-of-the-art PEFT methods
:analysis,expert selection examination
:concept,sleep quality
:concept,well-being
:technology,wearable sensors
:task,real-time monitoring
:issue,lack of actionable insights
:outcome,user abandonment
:role,technology in understanding sleep patterns
:task,understanding sleep patterns
:framework,two-stage framework
:dataset,GloBeM
:task,sleep prediction
:data,synthetic data from LLMs
:approach,advanced machine learning with user-centric design
:value,practicality
:system,voice-based writing assistance system
:product,Speakerly
:task,text composition
:user,system user
:output,document
:feature,variety of input modes
:task,fault localization and program repair
:dataset,widely-adopted benchmarks
:dataset,ConDefects
:data,faulty programs
:platform,AtCoder
:functionality,subset selection
:task,benchmarking fault localization and program repair methods
:hypothesis,distributional hypothesis
:property,distributional property
:capability,generalization capability
:concept,model pretraining
:`data source`,open-source repositories
:risk,security vulnerabilities propagation
:dataset,SecuCOGen
:task,code security enhancement
:data,180 samples
:finding,generation of vulnerable code
:approach,mitigation of security vulnerabilities
:weakness,repairing vulnerable code
:task,vulnerability classification
:community,software engineering community
:concept,neutrality
:field,politics
:entity,traditional media
:concept,editorial line
:platform,news rating platforms
:entity,news outlets
:goal,balanced news view
:`model family`,instruction-following language model
:task,writing newspaper articles
:model,AI-based news outlet
:concept,media bias
:corpus,multilingual corpus of news with coarse stance annotations
:entity,authentic news outlets
:classifier,editorial line classifier
:research,decoding internal mechanisms
:component,linear layers
:tool,Attention Lens
:transformation,lenses
:output,vocabulary tokens
:finding,attention heads specialization
:resource,Attention Lens code
:data,large-scale corpus
:goal,alignment with human values
:method,ranking-based alignment
:method,CycleAlign
:`model type`,black-box model
:`model type`,white-box model
:dataset,cross-lingual fact extraction and verification (XFEVER) dataset
:task,cross-lingual fact verification
:dataset,fact extraction and verification (FEVER) dataset
:data,professionally and machine-translated texts
:scenario,translate-train learning
:technique,prediction similarity consideration
:issue,model miscalibration
:resource,"XFEVER dataset, code, and model checkpoints"
:URL,https://github.com/nii-yamagishilab/xfever
:task,multi-party conversations
:evaluation,multi-party conversation tasks
:technique,incorporation of MPC structures
:study,evaluation and analysis of LLMs in MPCs
:challenge,deciphering graphical information flows
:challenge,generating stylistically consistent responses
:task,webpage understanding
:task,webpage representation
:information,thematic information of web domains and URLs
:objective,new pre-training objective
:task,URL and webpage understanding
:method,scalable graph embeddings and contrastive objective
:model,URL-BERT
:model,multilingual BERT
:engagement,user engagement on social media
:representation,shallow representations of URLs
:objective,contrastive objective
:representation,graph-based representations
:concept,code semantics
:technique,altering function and variable names
:technique,introducing adversaries
:`programming language`,JavaScript
:application,cloud root cause analysis
:`current methods`,manual workflow settings
:capability,decision-making and environment interaction
:framework,RCAgent
:application,industrial root cause analysis
:model,internally deployed model
:activity,data collection and analysis
:enhancement,"self-consistency, context management, stabilization, domain knowledge"
:platform,Alibaba Cloud's real-time compute platform for Apache Flink
:evaluation,automated metrics and human evaluations
:task,integrating constraints
:characteristic,opacity of large language models
:constraint,lexical constraint
:constraint,structural constraint
:constraint,relation-based constraint
:`research area`,neural network knowledge capture
:`research effort`,investigating neural network mechanisms
:`theoretical approach`,neural tangent kernel
:concept,infinite network width
:process,learning dynamics
:`model family`,fundamental models
:component,activation function
:phenomenon,feature bias
:`model family`,self-attention and CNN models
:task,learning n-grams
:`model type`,multiplication-based models
:`theoretical findings`,roles and capacities of components
:challenge,escalating memory requirements
:task,model partitioning
:hardware,GPUs/TPUs
:task,model parallelism
:tool,ALPA
:community,MLSys community
:tool,model parallel tools
:tool,Redco
:task,automating distributed training and inference
:concept,model parallelism automation
:mechanism,ML pipeline customization
:algorithm,meta-learning
:comparison,code efficiency
:task,generating sports updates
:`data source`,live tweets
:experience,live sports viewing with tweets
:trend,popularity
:sport,soccer
:task,generating live updates from tweets
:system,live update generation system for soccer matches
:`user experience`,grasping match progress and excitement
:mechanism,update control mechanism
:mechanism,redundancy reduction mechanism
:technique,language adapters
:language,low resource unseen languages
:strategy,multiple source language adapters
:architecture,ZGUL
:task,POS tagging and NER
:scenario,few-shot and unlabeled data settings
:baseline,standard fine-tuning and other baselines
:task,state recognition
:state,elevator door state
:state,object grasp state
:state,TV power state
:method,programmatic description
:method,image annotation and learning
:method,special sensors
:task,binary state recognition
:technique,questioning methods
:system,recommendation system
:`model architecture`,vanilla neural network
:`research direction`,incorporating recommendation systems with large language models
:scenario,multiple key-value data in recommendation systems
:domain,recommendation system domain knowledge
:strategy,shuffle and mask strategies
:approach,sequential recommendations based on multiple key-value data
:result,effective completion of challenging issue
:field,simultaneous machine translation
:concept,read-write policies
:assumption,subword level operations
:standard,word level input and output
:policy,word level policy
:policy,subword level policy
:method,language model integration
:`model family`,simultaneous machine translation model
:disparity,subword disparity
:url,https://github.com/xl8-ai/wordsimt
:technique,task-specific prompting
:expertise,deep understanding of LLMs and tasks
:challenge,automating expert-level prompt generation
:method,existing prompt optimization
:challenge,exploring expert-level prompt space
:method,PromptAgent
:goal,crafting expert-level prompts
:concept,trial-and-error exploration
:outcome,expert-level insights and in-depth instructions
:domain,various NLP tasks
:baseline,chain-of-thought and prompt optimization baselines
:quality,efficiency and generalizability in prompt crafting
:goal,complex multimodal reasoning
:process,human thinking
:challenge,multimodal context challenges
:process,transfer of LLM advancements to multimodal contexts
:insight,keeping critical thinking
:task,multimodal Chain of Thought reasoning
:insight,letting everyone do their jobs
:technique,DDCoT prompting
:task,multimodal reasoning
:task,reasoning and recognition
:task,joint reasoning process
:attribute,generalizability and explainability
:component,position embedding
:task,context window extension
:method,length extrapolation
:method,continuous length extrapolation (CLEX)
:`mathematical concept`,ordinary differential equations
:model,GPT-NeoX
:method,path reasoning
:model,knowledge graph embeddings
:concept,interdependence between entities and relations
:issue,inaccurate explanations
:approach,PEARLM
:goal,capturing user behaviour and product-side knowledge
:concept,entities and relations optimisation
:quality,path faithfulness
:resource,source code and datasets
:event,paper acceptance
:concept,representational space
:understanding,linguistic information emergence and interaction
:suite,information theoretic probing suite
:capability,direct comparison of representational subspaces
:analysis,linguistic task analysis
:phase,critical learning phase
:milestone,0.5% of full training
:knowledge,open-domain knowledge
:task,semantics and reasoning tasks
:capability,long-range contextualization
:measurement,cross-task similarity
:concept,information sharing
:concept,learning from limited data
:representation,3D scene graph
:concept,objects and relationships
:task,learning 3D scene graphs
:resource,relationship annotations
:strategy,improving model performance
:method,existing pre-training methods
:approach,language-based pre-training
:method,contrastive pre-training
:benchmark,semantic 3D scene graph benchmark
:feature,scene graph features
:capability,zero-shot querying
:task,room type prediction
:dataset,instruction-tuning datasets
:issue,occupational bias
:dataset,OccuQuest
:data,prompt-completion pairs and dialogues
:datasets,"Dolly, ShareGPT, WizardLM"
:model,OccuLlama
:models,"Vicuna, Tulu, WizardLM"
:`test set`,Occu-Quora set
:outcome,homogenization of responses
:issue,demographic under-representation
:concept,diversity of representation
:method,evaluation datasets and metrics
:capability,diversity reasoning
:technique,collective-critique and self-voting (CCSV)
:goal,improving people diversity
:method,handcrafted examples or prompt tuning
:goal,culture diversity
:condition,noise-free context
:condition,noisy context
:technique,trigger sentences
:task,concentration on relevant information
:technique,interactive chain-of-thought method
:technique,R3 prompting
:process,thought process
:profession,psychiatrist
:condition,mental disorder
:system,passive mental health monitoring system
:feature,alternative features
:system,FedTherapist
:input,continuous speech and keyboard input
:challenge,on-device language model training
:methodology,context-aware language learning (CALL)
:evaluation,IRB-approved evaluation
:metric,AUROC
:metric,MAE
:concept,factual knowledge in PLMs
:activity,probing for factual knowledge
:scheme,categorization scheme for factual probing methods
:resource,datasets for factual probing
:concept,knowledge retention in PLMs
:concept,prompt optimization in PLMs
:obstacle,adopting PLMs as knowledge bases
:activity,future work in PLMs
:concept,human expression patterns
:approach,fine-tuning on personalized corpus
:resource,training examples
:approach,manual crafting of prompts
:goal,precise manipulation of personality traits
:approach,tailoring personality traits in LLMs
:concept,Big Five personality factors
:resource,unsupervisedly-built personalized lexicons (UBPL)
:process,decoding phase in LLMs
:concept,naturalness of generated texts
:goal,manipulating personality traits
:model,VD-GR
:`model type`,visual dialog model
:technique,multi-modal graph neural networks
:concept,hub-nodes
:dataset,VisDial v1.0
:dataset,VisDial v0.9
:dataset,VisDialConv
:dataset,VisPro
:system,trustworthy NLP systems
:goal,fairness and explainability
:study,influence of fairness on explainability
:relationship,fairness and explainability interaction
:dataset,BIOS
:dataset,ECtHR
:attribute,nationality
:method,rationale extraction
:finding,bias mitigation and fairness
:characteristic,orthogonality
:task,facial expression recognition
:field,affective computing
:limitation,seven basic emotions
:technique,natural language supervision
:task,mental health symptom estimation
:metric,Pearson's correlation coefficient
:performance,schizophrenia symptom severity estimation
:URL,https://github.com/nickyfot/emoclip
:`performance metric`,parser performance
:capability,zero-shot dependency parsing
:task,region-word alignment
:`method family`,existing vision-language alignment methods
:issue,localization accuracy and generalization capabilities
:approach,CoDET
:issue,reliance on pre-aligned vision-language space
:concept,visual similarity
:metric,AP^m_novel and AP^m_all
:benchmark,OV-LVIS
:resource,CoDET code
:URL,https://github.com/cvmi-lab/codet
:task,code documentation
:technique,explanatory text generation
:`model family`,code-specific large language model
:`model family`,generic large language model
:attribute,corpus size
:attribute,handling smaller datasets
:`shared task`,BabyLM
:model,larger GPT-2 variant
:task,storytelling tasks
:`model family`,larger language model
:URL,https://github.com/zephyr1022/babystories-utsa
:application,performance prediction
:technique,performance prediction prompting
:model,LLM-PP
:benchmark,state-of-the-art performance predictors
:model,LLM-Distill-PP
:algorithm,Hybrid-Search Algorithm for NAS (HS-NAS)
:benchmark,state-of-the-art neural architecture search
:metrics,efficiency metrics
:model,SkyMath
:technique,self-compare fine-tuning
:`performance metric`,SOTA
:task,spatial reasoning over text
:approach,disentangling information extraction and reasoning
:`model family`,disentangled model
:`model family`,state-of-the-art baseline
:domain,realistic data domains
:`research area`,conversational recommendation system
:trend,growth in research
:challenge,novelty and limited contributions
:dataset,benchmark datasets for CRS
:task,CRS model development
:bias,feedback loop biases
:technique,generative data via language models
:strategy,data augmentation for CRS
:strategy,Once-aug
:goal,enhanced CRS model performance
:strategy,PopNudge
:goal,mitigating biases in CRS
:dataset,ReDial
:dataset,TG-ReDial
:bias,newly formulated biases
:task,authorship analysis
:challenge,distinguishing human-written vs AI-generated texts
:medium,written texts
:benchmark,Hansen
:medium,spoken texts
:dataset,human and AI-generated spoken text datasets
:dataset,AI-generated spoken text datasets
:model,Vicuna13B
:task,author verification
:task,human vs. AI spoken text detection
:task,authorship attribution and verification
:task,AI-generated spoken text detection
:resource,Hansen repository
:`mental process`,reasoning about mental states
:development,intelligence and language understanding
:research,previous work on theory of mind
:concept,first and second-order theory of mind
:concept,higher-order theory of mind
:`mental process`,recursive reasoning
:benchmark,Hi-TOM
:analysis,failure cases of LLMs
:findings,implications of Hi-TOM evaluation
:issue,safety and alignment
:approach,supervised iterative learning from human feedback
:hypothesis,reward model criticality and PPO necessity
:problem,reward optimization
:component,training rewards optimization
:metric,METEOR similarity
:evaluation,GPT-4 based qualitative evaluation
:value,simplicity and performance
:phenomenon,sound symbolism
:concept,sound-meaning correlation
:dimension,cross-modal associations
:domains,language and visual
:method,zero-shot knowledge probing
:concept,inherent knowledge
:effect,Kiki-Bouba effect
:model,vision-and-language models
:method,novel method for demonstrating sound symbolism
:practice,training language models
:risk,legal and ethical risks
:effort,multi-disciplinary effort
:task,audit and trace text datasets
:tool,data provenance tools
:task,trace dataset lineage
:analysis,landscape analysis
:issue,data divide
:dataset,closed datasets
:category,important dataset categories
:issue,license miscategorization
:platform,dataset hosting sites
:crisis,misattribution and informed use
:resource,popular datasets
:contribution,dataset audit release
:value,dataset transparency
:tool,Data Provenance Explorer
:task,trace and filter on data provenance
:website,www.dataprovenance.org
:data,massive datasets
:`data type`,problematic text
:problem,pretraining data detection
:benchmark,Wikimia
:method,Min-k% prob
:hypothesis,outlier word probabilities
:requirement,pretraining corpus knowledge
:task,privacy auditing
:evaluation,consistently effective solution
:challenge,high inference costs
:model,Switch Transformer-C2048
:attribute,1.6 trillion parameters
:attribute,massive parameter counts
:framework,QMoe
:challenge,memory problem
:algorithm,scalable compression algorithm
:format,custom compression format
:technology,GPU decoding kernels
:hardware,commodity hardware
:model,trillion-parameter model
:resource,source code and compressed models
:`model family`,code language model
:task,code comprehension and generation tasks
:field,code embeddings
:`model family`,multilingual code model
:study,comprehensive study on multilingual code embeddings
:experiment,probing experiments
:component,code embeddings components
:component,language-specific component
:task,code retrieval tasks
:metric,mean reciprocal rank (MRR)
:dataset,DialogSum
:dataset,Decoda
:`data type`,English social conversations
:`data type`,French call center interactions
:characteristic,lengthy summaries
:technique,human guidelines as intermediate step
:characteristic,stylistic tendencies
:output,GPT-generated summaries
:technique,floating-point quantization
:technique,integer quantization
:domain,hardware platforms
:factor,exponent bits and clipping range
:pattern,high inter-channel variance and low intra-channel variance
:`model family`,transformer models
:technique,per-channel activation quantization
:concept,exponential biases of weights
:technique,LLM-FP4
:performance,63.1 average score
:performance,full-precision model score
:URL,https://github.com/nbasyl/llm-fp4
:`model type`,autoencoding model
:`model type`,encoder-decoder model
:modality,any modality
:model,GLM
:model,General Point Model (GPM)
:task,point cloud transformer tasks
:task,point cloud representation tasks
:technique,masked prediction
:task,point cloud understanding tasks
:task,point cloud generation tasks
:data,unlabeled code samples
:data,labeled code samples
:`language type`,low-resource programming languages
:technique,cross-lingual transfer learning
:experiment,cross-lingual transfer learning experiments
:question,cross-lingual transfer effectiveness
:question,source language selection
:question,language pair characteristics
:question,task dependency
:goal,intent alignment
:technique,distilled supervised fine-tuning
:metric,task accuracy
:data,preference data from AI feedback
:technique,distilled direct preference optimization
:model,chat model
:model,Zephyr-7B
:benchmark,chat benchmarks
:benchmark,MT-Bench
:resource,"code, models, data, and tutorials"
:website,Hugging Face Alignment Handbook
:application,design optimization
:requirement,large-scale datasets
:field,polymer science
:characteristic,sparse datasets
:task,generating additional samples
:dataset,open-source datasets for small molecules
:task,benchmark prediction tasks
:issue,text safety
:classifier,safety classifier
:issue,new safety rules
:setting,domain-generalized few-shot learning
:classifier,text safety classifier
:approach,similarity-based data-augmentation + prompt-tuning
:task,social chemistry moral judgement
:problem,hallucination in text generation
:task,neural data-to-text generation
:method,mitigating hallucination
:task,model modification or data collection
:model,text critic classifier
:method,combining LM output with text critic
:task,model architecture modification
:data,base LM's training data
:task,evaluating hallucination mitigation
:benchmark,OpenDialKG
:task,text detection
:goal,distinguishing text origins
:content,synthetic tweets
:algorithm,Naive Bayes
:classifier,shallow learning classifier
:method,human-based detection
:concept,linguistic acceptability
:classifier,transformer-based classifier
:classifier,BERT-based classifier
:`benchmark extension`,Language-World
:benchmark,Meta-World
:environment,simulated robotic environment
:method,Plan Conditioned Behavioral Cloning (PCBC)
:resource,https://github.com/krzentner/language-world/
:method,controlled decoding
:component,prefix scorer
:goal,high reward outcomes
:data,off-policy data
:corpus,Reddit conversations corpus
:problem,multi-objective reinforcement learning
:strategy,blockwise fashion at inference-time
:strategy,best-of-k strategy
:goal,alignment of language models
:approach,single-step processing
:strategy,question decomposition
:`model-driven approach`,selective decomposition
:domain,medical VQA
:project,Decomposition 0-Shot VQA
:URL,https://zaidkhan.me/decomposition-0shot-vqa/
:process,fine-tuning large language model
:framework,computation-efficient framework
:`model state`,frozen pre-trained language model
:evaluator,reference-free evaluator
:metric,commonsensical score
:`knowledge base`,dynamic commonsense knowledge base
:method,controllable generation method
:`model component`,auxiliary head
:benchmark,concept-to-sentence benchmarks
:outcome,commonsensical outputs
:activity,mathematical discovery
:task,conjecture generation
:task,constructing counterexamples
:task,formalizing mathematics
:task,discovering connections between mathematical areas
:technique,computers in mathematical proof search
:task,mathematical proof search
:role,co-contributors in mathematical research
:task,logic and mathematical tasks
:interest,melding theorem proving systems with foundation models
:task,formalizing advanced mathematical concepts
:framework,mathematical reasoning review and check
:task,reviewing and checking mathematical reasoning
:system,proof assistants
:`proof assistant`,PVS
:task,bridging textual descriptions and formal specifications
:process,math-PVS
:task,extracting and formalizing mathematical theorems
:activity,academic review and discovery
:`data type`,dynamic graph
:concept,temporal network evolution
:capability,spatial-temporal understanding
:benchmark,LLM4DyG
:task,spatial-temporal evaluation tasks
:factor,model performance factors
:method,Disentangled Spatial-Temporal Thoughts (DST2)
:observation,LLMs' preliminary abilities
:observation,task difficulty
:factor,graph size and density
:resource,data and codes
:event,publication time
:value,informed decisions
:task,claim attribution
:task,manual error detection
:characteristic,labor-intensive
:tool,Fleek
:task,automatic factual error detection and correction
:process,evidence-based fact checking
:evaluation,empirical evaluation of Fleek
:resource,video demo of Fleek
:url,https://youtu.be/napjfulkpdq
:method,factual probing
:issue,prompt sensitivity
:limitation,relation-specific optimization
:concept,unseen relation types
:method,test-time augmentation (TTA)
:technique,prompt augmentation and ensembling
:outcome,model calibration improvement
:challenge,high-quality prompt variations
:method,task-specific supervised learning
:method,conventional deep learning methods
:`model family`,large pre-trained language and vision-language model
:technique,SAMPOT
:task,downstream segmentation task
:application,lung segmentation in chest x-ray images
:field,automatic visual prompt-tuning
:task,multimodal manga analysis
:feature,visual and textual features
:community,natural language processing community
:community,computer vision community
:medium,comics
:problem,missing comic text content
:task,multimodal manga complement (M2C)
:solution,shared semantic space for vision and language
:dataset,M2C benchmark dataset
:method,manga argumentation method MCOT
:knowledge,event knowledge in comics
:method,FVP-M$^{2}$
:capability,text and code processing and generation
:task,grounded task-oriented dialogue
:system,modular and interpretable grounded dialogue system
:shortcoming,LLMs' difficulty in steering and novel grounding
:task,determining next response
:task,OneCommon dialogue task
:performance,task success rate
:benchmark,previous state-of-the-art
:enterprise,research
:framework,human-AI collaborative framework
:process,AI engagement in writing
:mechanism,cognitive offloading
:mechanism,imaginative stimulation
:model,two-stage model for human-AI collaborative writing
:model,writing-assistance types and levels
:strategy,maintaining rigorous scholarship
:strategy,adhering to journal policies
:risk,overreliance on AI
:integration,prudent integration of AI into academic writing
:outcome,improved academic writing
:approach,contextual sparsity
:system,DejaVu
:model,OPT-175B
:feature,asynchronous and hardware-aware implementation
:quality,model quality
:resource,DejaVu code
:URL,https://github.com/fminference/dejavu
:domain,music audio
:capability,end-to-end music generation
:capability,conditioned music generation
:limitation,text control limitation
:method,content-based control
:method,COCO-MuLLA
:result,high-quality music generation
:capability,content-based controls
:feature,chords and rhythms
:capability,music variation generation and style transfer
:resource,source codes and demos
:concept,in-context information
:task,entity-attribute binding
:mechanism,binding id
:`model family`,Llama
:concept,binding id vectors
:concept,continuous subspace
:concept,symbolic knowledge representation
:concept,in-context reasoning
:modality,symbolic music
:application,production products
:component,tokenizer
:library,MidiTok
:functionality,music tokenization
:standard,unified API
:value,usability and extensibility
:task,closed-ended text generation
:goal,appropriate response generation
:class,training objectives based on convex functions
:concept,optimal predicted distribution
:function,convex functions
:url,https://github.com/ictnlp/convex-learning
:task,object reidentification
:model,CLIP-ReID
:approach,prototypical contrastive learning fine-tuning
:task,supervised object reidentification
:component,image encoder of CLIP
:technique,prototypical contrastive learning loss
:task,unsupervised object reidentification
:method,target similarity tuning
:goal,adapt sentence embedding model
:model,sentence transformer
:model,larger model embeddings
:task,synthetic example generation
:model,tiny model
:task,embedding space transformation
:method,efficient training example selection
:evaluation,ranking-based evaluation
:task,end-to-end code generation
:attribute,hidden states
:property,dense and continuous
:goal,interpretable hidden states
:concept,codebook features
:property,sum of discrete vector codes
:condition,vector quantization bottleneck
:capability,behavior control
:`model architecture`,Codebook Transformer
:dataset,finite state machine dataset
:problem,superposition problem
:`model architecture`,Transformer language model
:data,natural language datasets
:concept,disentangled concepts
:task,topic generation
:resource,codebase and models
:URL,https://github.com/taufeeque9/codebook-features
:research,previous work on PLMs
:concept,semantic and syntactic information
:factor,data or pre-training objective
:performance,PLMs' performance
:`research gap`,effect of information loss in input token characters
:method,pre-training on subsets of characters
:setting,extreme pre-training settings
:model,single first character pre-trained model
:benchmark,standard NLU benchmarks and probing tasks
:performance,90% performance retention
:performance,77% performance retention
:challenge,detecting generated text
:issue,ethical concerns
:goal,detecting artificial content
:`model architecture`,Convolutional Neural Network (CNN)
:model,SciBERT-CNN ensemble
:`model family`,ensemble architecture
:`model family`,individual Transformer model
:model,SciBERT
:model,XLNet
:`model architecture`,recurrent neural network
:`model architecture`,temporally convolutional neural network
:representation,reaction SMILES-like representation
:task,de novo reaction generation
:property,autoregressive properties
:task,SMILES generation
:property,wide receptive field
:combination,RNN and TCN latent representations
:protocol,fine-tuning protocols
:property,generative scope
:dataset,dataset of interest
:domain,various domains and tasks
:task,text-to-SQL
:method,ACT-SQL
:technique,auto-COT exemplar generation
:value,cost efficiency
:task,multi-turn text-to-SQL
:benchmark,Spider dev set
:task,cross-cultural understanding
:task,recipe adaptation
:dataset,CulturalRecipes
:attribute,human curation
:research,culturally-aware language models
:insight,cross-cultural adaptation findings
:application,culturally diverse contexts
:application,self-driving car controller development
:framework,simulation framework
:language,domain specific language
:gap,tacit assumptions gap
:interaction,user instruction to executable code
:system,extended multimodal interaction system
:task,mapping utterances to code
:capability,context sensitivity
:environment,user-AI interactive environment
:attribute,non-toxic
:benchmark,ToxicChat
:source,real user queries
:phenomenon,nuanced toxic phenomena
:domain,user-AI interaction
:challenge,toxicity detection in user-AI conversations
:goal,safe and healthy user-AI environment
:belief,machine understanding of language
:philosophy,traditional philosophical assumptions
:tendency,regarding machine language performance as syntactic manipulation
:performance,machine language performance
:concept,simulation of understanding
:concept,referential grounding
:concept,linguistic expressions
:concept,evolutionary information
:data,protein amino acid sequences
:task,choosing vocabulary size
:benchmark,comprehensive benchmark for protein language models
:field,protein language modeling
:task,transfer learning assessment
:concept,vocabulary size
:resource,"code, model weights, and datasets"
:URL,https://github.com/ginnm/proteinpretraining
:artifact,fake images
:issue,deepfake threats
:technique,deepfake detection
:task,distinguishing real vs fake images
:challenge,generalizability of deepfake detection
:model,InstructBLIP
:experiment,full-spectrum experiments
:data,datasets from generative models
:result,improved deepfake detection accuracy
:technique,prompt tuning with vision-language models
:url,https://github.com/nctu-eva-lab/antifakeprompt
:model,LightLM
:`model architecture`,lightweight Transformer
:field,AI sub-fields
:task,generative recommendation
:stage,infancy
:method,Spectral Collaborative Indexing (SCI)
:`model architecture`,deep and narrow Transformer
:method,Graph Collaborative Indexing (GCI)
:problem,hallucination in generative recommendation
:process,constrained generation process
:resource,LightLM code
:URL,https://github.com/dongyuanjushi/lightlm
:approach,zero-shot reader
:challenge,computational cost and labeled data
:issue,irrelevant documents
:issue,overconfidence in answers
:technique,distraction-aware answer selection
:component,supervised reader
:issue,unseen data
:role,agents
:concept,cooperation
:concept,competition
:framework,general framework for studying competition
:environment,virtual town
:`agent type`,restaurant agent
:`agent type`,customer agent
:phenomenon,social learning
:experiment,competition behaviors in LLM-based agents
:phenomenon,Matthew effect
:value,understanding society
:`model family`,pre-trained model
:concept,theoretical understanding
:`model type`,fully connected neural network
:concept,approximation error
:attribute,labour-intensive
:task,systematic review tasks
:approach,human-out-of-the-loop
:issue,chance agreement and dataset imbalance
:attribute,moderate performance
:task,title/abstract screening
:attribute,variable performance
:attribute,almost perfect performance
:technique,highly reliable prompts
:`model characteristic`,pretrained machine learning model
:issue,bias amplification
:outcome,unfair outcomes
:study,case study
:strategy,continued pretraining on gender-neutral data
:`robot capability`,learning and refining behavior post-deployment
:environment,unstructured environments
:`learning system`,OLAF
:user,everyday users
:robot,robot with OLAF
:policy,visuomotor neural policy
:`robot capability`,following verbal commands
:experiment,user teaching robot
:environment,simulation and physical hardware
:`performance metric`,policy success rate
:resource,videos and results
:URL,https://ut-austin-rpl.github.io/olaf/
:phenomenon,human bias
:domain,"linguistic, cultural, and societal borders"
:study,bias studies
:region,Western and European languages
:test,Word Embedding Association Test (WEAT)
:quantity,24 languages
:data,culturally relevant information
:region,Indian linguistic landscape
:analysis,regional bias analysis
:`bias dimension`,new dimensions
:method,embedding methods comparison
:goal,equitable language models
:resource,WEAThub
:url,https://github.com/iamshnoo/weathub
:task,fine-tuning pretrained language models
:problem,large-scale optimization
:algorithm,training algorithm
:optimizer,Adam optimizer
:concept,hyperparameter tuning
:method,PAC-tuning
:theory,PAC-Bayes training
:technique,perturbed gradient descent
:theory,PAC-Bayes bound
:benchmark,GLUE benchmark tasks
:theory,PAC training
:value,transparency and completeness
:resource,open source data contamination reports
:benchmark,multi-choice QA benchmarks
:subset,contaminated subsets
:resource,data and code
:model,lil-bevo
:data,music data
:technique,sequence length training
:technique,targeted masked language modeling
:benchmark,performance levels
:task,BLiMP subtasks
:subtask,negative polarity items
:challenge,training performant LLMs on small data
:difficulty,difficult task
:resource,code for lil-bevo
:url,https://github.com/venkatasg/lil-bevo
:resource,models for lil-bevo
:url,https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a
:challenge,proving contamination
:approach,provable guarantees of test set contamination
:goal,test set contamination proof
:phenomenon,canonical order preference
:test,contamination test
:issue,test set contamination
:procedure,contamination detection
:audit,language model test set contamination audit
:outcome,low pervasive contamination
:task,instruction-based language modeling
:task,instruction engineering
:trend,automating instruction generation
:attribute,instruction length and perplexity
:approach,InstOptima
:concept,evolutionary multi-objective optimization
:concept,instruction operators
:mechanism,objective-guided
:outcome,improved fine-tuning performance and high-quality instructions
:task,evaluating LLMs in open-ended scenarios
:limitation,existing benchmarks and metrics
:approach,fine-tuning LLMs as scalable judges
:dataset,"comprehensive, large-scale, high-quality dataset"
:benchmark,new benchmark for evaluating judges
:model,JudgeLM
:attribute,model scale
:attribute,capabilities and behaviors
:bias,knowledge bias
:bias,format bias
:technique,swap augmentation
:technique,reference support
:technique,reference drop
:performance,state-of-the-art judge performance
:model,JudgeLM-7B
:task,judging samples
:agreement,high agreement with teacher judge
:capability,extended capabilities in judging
:data,text datasets
:capability,emergent capabilities
:capability,latent concepts
:framework,cognitive interpretability
:concept,randomness perception
:data,random binary sequences
:model,GPT-3.5+
:ability,pseudo-random number generation
:issue,clinician burnout
:task,documentation
:system,electronic medical record systems
:technology,AI chatbots
:task,drafting responses
:`study design`,two-stage cross-sectional study
:outcome,AI-assisted responses
:quality,acceptability
:risk,unedited AI responses
:outcome,harm
:perception,physicians
:quality,human-likeness
:action,patient education recommendations
:recommendation,monitoring
:goal,safe implementation
:`model family`,relation extraction model
:issue,implicit expressions and long-tail relation classes
:cause,language complexity and data sparsity
:community,users without access to LLMs
:system,rule-based system
:issue,implicit expressions
:`document type`,financial documents
:approach,nearest-neighbor search over dense vectors
:dataset,ReFind
:concept,human in the loop setup
:characteristic,representation dominance
:subset,outlier dimensions
:quality,representational quality of embeddings
:subset,single outlier dimension
:knowledge,task-specific knowledge
:value,representation value
:process,model decision-making
:policy,generalizable policy for embodied visual tasks
:approach,Large Language Model Reinforcement Learning Policy (LLARP)
:`task characteristic`,complex paraphrasings of instructions
:task,new tasks with novel optimal behavior
:metric,success rate on unseen tasks
:baseline,common learned baselines or zero-shot LLM applications
:benchmark,Language Rearrangement
:task,language-conditioned rearrangement
:resource,video examples
:trend,multi-modal language model development
:modality,additional modalities
:modality,neural architectural information
:service,architecture-2-text and text-2-architecture retrieval/generation
:audience,beginner and intermediate ML users
:model,ArchBERT
:`model family`,bi-modal language model
:strategy,Masked Architecture Modeling (MAM)
:resource,bi-modal datasets
:material,supplementary materials
:activity,making big purchases
:need,background knowledge
:`problem space`,mixed-type mixed-initiative dialog
:framework,SalesOps
:agent,SalesBot
:agent,ShopperBot
:study,human study
:profession,professional salespeople
:limitation,truthful information provision
:framework,automated measurement of responsible AI metrics
:expertise,technical and sociotechnical
:expertise,domain-specific sociotechnical
:goal,advanced harm measurement
:value,responsible use of LLMs
:process,on-device learning
:goal,privacy-preserving customization
:framework,existing training frameworks
:hardware,cloud servers with accelerators
:challenge,learning on the edge
:limitation,resource limitations
:engine,PockEngine
:process,fine-tuning on edge devices
:feature,sparse backpropagation
:approach,compilation first
:optimization,training graph optimizations
:support,"diverse applications, frontends, and hardware backends"
:model,LLaMAv2-7B
:hardware,NVIDIA Jetson AGX Orin
:task,reasoning and integrating information
:approach,data-centric approach
:model,financial large language model
:technique,multitask prompt-based finetuning
:technique,Abductive Augmentation Reasoning
:data,pseudo labels
:model,financial large language model with AAR
:`model family`,baseline financial large language model
:resource,new benchmark for financial analysis and interpretation
:methodology,data-centric approach with AAR
:potential,LLMs for complex real-world domains
:behaviour,responsible behaviour in specific contexts
:community,LGBTI+ community
:issue,variation in LGBTI+ terminology
:resource,domain-specific lexicon
:task,LLM behaviour evaluation
:lexicon,LGBTI+ lexicon in Indian languages
:methodology,LLM evaluation methodology
:process,evaluation process
:content,hateful content
:task,natural language understanding evaluation
:lexicon,LGBTI+ lexicon
:application,lexicon application
:behaviour,responsible behaviour
:task,energy load forecasting
:goal,resource optimization
:approach,language model-based energy load forecasting
:approach,autoregressive generating
:quality,effectiveness and accuracy
:goal,energy efficiency
:goal,intelligent decision-making
:skill,language proficiency and fluency
:role,expert linguistic annotator
:task,AMR parsing
:formalism,Abstract Meaning Representation (AMR)
:representation,graphical representation of sentence meaning
:task,AMR format reproduction
:issue,errors in AMR parsing
:task,accurate semantic analysis
:technique,metalinguistic natural language queries
:task,partial AMR reconstruction
:framework,ControlLLM
:task,tool invocation
:component,task decomposer
:paradigm,thoughts-on-graph (TOG)
:component,execution engine with a rich toolbox
:task,multi-modal processing tasks
:code,ControlLLM code
:url,https://github.com/opengvlab/controlllm
:challenge,correctness of generated code
:paradigm,Clover
:task,consistency checking
:component,Clover checker
:integration,formal verification tools and large language models
:analysis,theoretical
:dataset,CloverBench
:task,image-to-report modeling
:issue,clinically inaccurate reports
:approach,two-step radiology report generation
:step,content extraction
:step,content verbalization
:representation,RadGraph
:evaluation,human evaluation with clinical raters
:method,style matching
:ability,predicting opinions and behaviors
:survey,European Social Survey
:issue,prohibitive costs
:research,influence of core human values on decisions and actions
:proposal,value-injected LLMs for prediction
:method,Value Injection Method (VIM)
:`model family`,value-injected large language model
:technology,mobile edge computing
:need,efficient service delivery
:`user group`,mobile users with constrained resources
:framework,collaborative framework for model training
:user,mobile users
:component,model adapters
:server,servers with robust computational capabilities
:approach,collaborative training approach
:benefit,advanced LLM services for users with limited computational capacities
:algorithm,DASHF
:technique,fractional programming
:simulation,extensive simulations
:application,collaborative LLM service deployments
:application,dynamic object detection and tracking
:task,multi-object tracking (MOT)
:limitation,closed-set category tracking
:task,open-vocabulary multi-object tracking (OVMOT)
:task,beyond pre-defined category tracking
:task,generic multi-object tracking (GMOT)
:requirement,pre-trained (vision-)language model
:task,OVMOT model training
:method,Siamese-DETR
:task,Siamese-DETR model training
:component,object queries in DETR variants
:strategy,dynamic matching training strategy
:technique,tracking-by-query
:technique,non-maximum suppression (NMS)
:benchmark,GMOT-40 dataset
:outcome,synthetic datasets
:issue,lack of diversity and noise
:strategy,TARGEN
:technique,multi-step prompting
:characteristic,seedless nature
:characteristic,dataset complexity and diversity
:issue,bias level
:outcome,synthetic SuperGLUE dataset
:goal,quality data generation
:approach,Aspiro
:task,structured data verbalisation
:technique,LLM re-prompting
:technique,consistency validation
:metric,parsing error rate
:dataset,DART
:setup,5-shot text-davinci-003
:dataset,rel2text
:`model family`,fine-tuned pre-trained language model
:`use case`,interactive use
:risk,inference-time privacy risks
:source,multiple sources
:task,privacy reasoning
:concept,contextual privacy
:benchmark,Confaide
:weakness,privacy reasoning capabilities
:technique,privacy-inducing prompts
:need,novel inference-time privacy-preserving approaches
:opportunity,commercial and scientific opportunities
:task,simulation of human behavior and opinion
:application,human simulacra generation
:role,"experimental participants, survey respondents, independent agents"
:outcome,human-like behavior
:behavior,genuine human behavior
:concept,subpopulation representative models (SRMs)
:task,estimating public opinion
:goal,public opinion measurement
:technology,new technology
:technique,behavior elicitation
:framework,"analysis, development, and practical implementation"
:task,Named Entity Recognition (NER)
:field,astronomy
:strategy,Prompt-NER
:component,prompt elements
:model,Claude-2
:dataset,original PDF documents
:dataset,paragraph collections
:dataset,astronomical telegrams
:experiment,LLMs' performance assessment
:research,experimental results analysis
:component,prompt elements mechanism
:research,zero-shot NER tasks in astronomical literature
:`interaction method`,natural language interfaces for tabular data
:`interaction method`,traditional query languages
:problem,text-to-vis
:resource,roadmap
:application,source code generation
:application,source code understanding
:task,automated bug repair
:`model family`,language model for code intelligence
:issue,pitfalls
:concept,realistic performance
:research,systematic literature review
:taxonomy,pitfalls in LM4Code research
:study,systematic study of LM4Code pitfalls
:`classification scheme`,comprehensive classification of LM4Code pitfalls
:roadmap,for LM4Code research and practice
:issue,nonfactual responses
:method,self-detection
:task,diversifying textual expressions
:task,identifying falsehood-prone questions
:approach,LLM prompting
:task,sentiment polarity classification
:task,Sentiment and Opinion Understanding of Language (SOUL)
:subtask,Review Comprehension (RC)
:subtask,Justification Generation (JG)
:dataset,SOUL dataset
:resource,SOUL code and dataset
:domain,healthcare and biomedical
:capability,multi-modal input interpretation
:model,Qilin-Med-VL
:capability,textual and visual data analysis integration
:process,two-stage curriculum training
:task,medical captions generation
:task,answering medical queries
:dataset,CHiMed-VL
:task,medical data interpretation
:application,chatbots with distinct personas
:chatbot,role-playing chatbot
:method,open-ended interview-style personality assessment
:library,ChatHaruhi
:`personality dimension`,Big Five
:assessment,personality assessment
:`personality dimension`,MBTI
:`alignment rate`,82.8% alignment with human-perceived personalities
:strategy,shaping chatbots' personalities
:study,cornerstone study for role-playing chatbots
:field,computational linguistics and psychology
:resource,ChatHaruhi GitHub repository
:URL,https://github.com/lc1332/chat-haruhi-suzumiya
:`evaluation method`,annotated benchmarks
:data,test split of a benchmark
:outcome,overestimation of performance
:`model state`,non-contaminated model
:consequence,harmful consequences
:`position paper`,this position paper
:concept,levels of data contamination
:goal,community effort
:technique,automatic and semi-automatic measures
:action,flagging papers
:capability,text completion
:`model family`,specialized model
:task,aspect-based sentiment analysis
:model,InstructABSA
:discussion,cost-performance trade-offs
:theory,dual-process theory of human cognition
:framework,DUMA
:mechanism,dual-mind mechanism
:model,fast thinking model
:role,primary interface for external interactions
:model,slow thinking model
:activity,meticulous planning and reasoning
:capability,seamless transition between intuitive and deliberate processes
:agent,conversational agent for real estate
:industry,real estate
:method,DUMA framework
:outcome,balance of effectiveness and efficiency
:technique,generating context passages
:technique,retrieving context passages
:concept,knowledge corpus error
:experiment,paraphrasing gold context
:result,increased performance
:URL,https://github.com/xfactlab/emnlp2023-knowledge-corpus-error
:method,Chain of Thought reasoning
:component,prompt quality
:process,handcrafting prompts
:task,developing grounding functions
:framework,end-to-end task-solving framework
:context,real settings
:framework,leader-follower bilevel framework
:capability,learning prompts and reasoning
:policy,prompt-generator policy
:policy,action policy
:system,proposed system
:benchmark,agent learning benchmarks
:benchmark,Overcooked
:benchmark,FourRoom
:phenomenon,controversy
:concept,zeitgeist
:system,conversational systems
:topic,controversial issues
:dataset,controversial questions dataset
:dataset,Quora question pairs dataset
:challenge,"knowledge recency, safety, fairness, and bias"
:capability,comprehension and handling of complex societal debates
:`data type`,code-mixed data
:challenge,handling code-mixed data
:feature,auditory phonetic features
:approach,Soundex-based language modeling
:technique,masked-language-modelling
:representation,Soundex representations (SAML)
:quality,robustness against adversarial attacks
:benchmark,popular baselines
:technique,SHAP (Shapley Additive Explanations)
:url,https://github.com/20118/defensewithphonetics
:resource,additional resources
:url,https://www.iitp.ac.in/~ai-nlp-ml/resources.html#phonetics
:technique,soft-prompt tuning
:approach,input-independent prompts
:method,multi-level prompt tuning (MPrompt)
:constraint,independence constraint
:approach,domain-specific prompts
:component,prompt generator
:knowledge,context-related knowledge
:metric,average improvement
:data,internet text
:attribute,contradicting information
:ability,discern truth from falsehood
:hypothesis,persona hypothesis
:process,clustering agents into personas
:persona,truthful persona
:feature,formal writing styles and scientific references
:concept,truthfulness generalization
:observation,probing for truthfulness
:technique,finetuning on facts
:attribute,truthfulness
:environment,arithmetics
:task,separating true and false statements
:concept,hierarchical structures
:task,claim span identification
:process,fact-checking pipelines
:problem,understudied research area
:dataset,X-Claim
:data,7k real-world claims
:method,cross-lingual transfer methods
:method,training on translated data
:agent,negotiation dialogue agent
:platform,online marketplace
:characteristic,integrative nature
:dataset,Integrative Negotiation Dataset (IND)
:functionality,integrative negotiation
:method,semi-automated data creation
:agent,Integrative Negotiation Agent (INA)
:technique,reward-based training
:skill,effective negotiation strategies
:activity,integrative negotiations
:approach,reward system
:capability,negotiation capabilities
:approach,deep learning for semantic column type annotation
:problem,semantic column type annotation challenges
:method,Archetype
:task,semantic column type annotation
:component,context sampling
:component,label remapping
:performance,state-of-the-art on zero-shot CTA benchmarks
:benchmark,domain-specific benchmarks for CTA
:model,DoDuo
:resource,code for Archetype
:url,https://github.com/penfever/archetype
:activity,providing tutorials
:concept,dual-use insights
:action,removing safeguards
:event,hackathon
:risk,model weight proliferation
:model,spicy LLaMA-2-70B
:action,censorship removal
:model,base LLaMA-2-70B
:concept,malicious prompts
:concept,1918 pandemic influenza virus
:threat,biological weapons acquisition
:`knowledge type`,generic knowledge
:task,domain-specific control policies
:method,fine-tuning with human feedback
:limitation,domain-specific task performance
:challenge,sourcing human feedback
:characteristic,labor intensive and costly
:approach,automated fine-tuning
:application,autonomous systems
:`knowledge type`,domain-specific requirements
:method,automaton-based controller synthesis
:description,natural language task descriptions
:controller,automaton-based
:specification,independently provided specifications
:`world model`,abstract or high-fidelity simulator
:verification,controller verification
:metric,compliance with specifications
:metric,percentage of specifications satisfied
:methodology,classical clustering
:feature,user control
:goal,user-specified criteria
:methodology,image clustering conditioned on text criteria (IC|TC)
:task,image clustering
:feature,minimal human intervention
:criteria,human action
:criteria,physical location
:criteria,person's mood
:`data format`,FP8 low-bit
:variable,gradients and optimizer states
:`data format`,low-precision
:framework,FP8 automatic mixed-precision
:technique,mixed-precision and distributed parallel training
:model,GPT-175B
:hardware,H100 GPU platform
:framework,FP8 mixed-precision training
:framework,BF16
:hardware,NVIDIA Transformer Engine
:task,LLM instruction tuning and reinforcement learning with human feedback
:task,emotion prediction
:task,emotion tasks
:task,manipulating emotional intensity
:task,reverse appraisals
:outcome,correct inferences
:field,affective modeling
:task,modeling emotion-related mechanisms
:task,web navigation
:technique,spliced prompts
:template,Alltogether
:feature,task context representation
:technique,prompt learning and instruction finetuning
:factor,length of HTML snippet
:performance,LLM web navigation performance
:factor,history trajectory
:technique,step-by-step instructions
:feedback,real-time environmental feedback
:field,LLM-driven web agents
:framework,WordArt Designer
:module,LLM Engine
:module,SemTypo
:concept,font design
:module,StyTypo
:module,Textypo
:concept,textured fonts
:platform,ModelScope
:task,everyday tasks
:concept,responsible large language model
:value,"fairness, transparency, and monitoring"
:framework,Responsible Development of Language Models (ReDeV)
:`test suite`,unique prompt types
:output,generated responses
:value,non-harmful and unbiased
:pipeline,machine learning pipeline
:value,"fairness, safety, and robustness"
:technique,prompt decomposition
:task,multi-step reasoning problems
:`model family`,foundational large language model
:capability,solution generation
:system,DaSLaM
:technique,decomposition generator
:model,decomposition generator LM
:technique,policy gradient optimization
:model,solver LM
:technique,modular finetuning
:issue,task variety
:issue,high cost of fine-tuning
:application,LLM application
:framework,MoELoRA
:issue,task variety and high cost of fine-tuning
:technique,MoE
:concept,task-motivated gate function
:dataset,public multi-task Chinese medical dataset
:benchmark,parameter-efficient fine-tuning methods
:task,urban region profiling
:source,web-sourced data
:trend,LLM application in various fields
:field,multi-modal data research
:field,vision-language learning
:modality,textual modality
:framework,LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP)
:model,image-to-text LLM
:data,satellite image descriptions
:model,UrbanCLIP
:metric,R^2
:resource,code and image-language dataset
:event,paper notification
:task,interpreting chest X-ray images
:`model parameter`,temperature
:dataset,dataset 1
:task,training image encoder
:dataset,dataset 2
:`model architecture`,ResNet50
:task,image encoding
:model,Meta AI-2
:URL,https://github.com/ecofri/cxr_llava
:`model performance`,fluctuation based on parameters
:process,digitisation of historical documents
:opportunity,historical research
:issue,noise
:`model family`,pixel-based language model
:task,reconstructing masked image patches
:method,synthetic scan generation
:data,synthetic historical document scans
:data,historical newspapers
:task,historical QA
:capability,generating coherent and contextually relevant responses
:issue,hallucinations in LLM outputs
:method,automated metrics for hallucination detection
:method,Chainpoll
:`dataset collection`,RealHall
:metric,adherence
:capability,analytical capabilities in retrieval augmented generation workflows
:issue,logical and reasoning errors
:field,computational linguistics
:domain,semantic processing
:dimension,multi-dimensional study
:technology,new technologies
:task,word sense disambiguation
:survey,semantic processing tasks analysis
:task,anaphora resolution
:task,concept extraction
:task,subjectivity detection
:research,theoretical research review
:technique,semantic processing techniques
:task,retrieval question answering
:framework,retrieval-augmented framework
:output,answer
:constraint,fine-tuning budget constraints
:approach,trainable pluggable reward-driven contextual adapter
:strategy,token-autoregressive strategy
:performance,ReQA performance
:technique,meaning representation extraction
:attribute,prompt-free and no fine-tuning required
:representation,distribution-based representation
:`relation type`,asymmetric relations
:perspective,distributional semantics
:task,semantic similarity tasks
:task,entailment and containment tasks
:`model family`,multimodal autoregressive model
:`data type`,multimodal data
:landscape,bioimage analysis tools
:community,bioimage community
:method,traditional search methods
:task,navigating bioimage analysis tools
:chatbot,bioimage.io chatbot
:challenge,navigational challenge
:service,personalized context-aware answers
:resource,community-contributed knowledge base
:tool,advanced bioimage analysis tools
:value,community-driven accessible scientific research
:issue,health disparities
:outcome,health outcomes and access
:goal,reducing health disparities
:challenge,using LLMs in human-doctor interaction
:requirement,diverse and representative data
:activity,collaboration between healthcare providers and technology experts
:method,cosine similarity analysis
:data,text queries about health disparities
:profession,clinicians
:value,ethics and equity
:challenge,computational costs
:approach,LoRaShear
:technique,dependency graph creation
:technique,progressive structured pruning
:technique,dynamic fine-tuning
:result,numerical results of LoRaShear
:metric,model footprint reduction
:resource,LoRaShear source code
:field,ecommerce
:factor,product descriptions
:metric,click-through rates
:method,traditional product description crafting
:attribute,consistency and scalability
:model,LLaMA 2.0 7B
:task,product description generation
:data,Walmart product descriptions
:`evaluation metric`,customer click-through rates
:finding,scalability and workload reduction
:system,automated product description generation system
:impact,business impact
:paradigm,end-to-end multi-task learning
:theory,communication theory
:trend,developing trends
:trend,research directions
:concept,social intelligence
:activity,understanding human expressions
:benchmark,Social-IQ
:methodology,comprehensive methodology
:dataset,DeSIQ
:`model attribute`,model style
:concept,multi-modality
:trend,diverse applications development
:feature,large training datasets and model parameters
:issue,"privacy, security, and copyright"
:taxonomy,memorization in LLMs
:concept,memorization aspects
:concept,memorization implications
:challenge,defining memorization
:phenomenon,LLM-specific phenomena
:technique,decoding algorithms
:application,conversational agents
:value,virtual companionship
:constraint,real-time recognition
:`model architecture`,graph convolutional network with reinforcement learning
:task,contextualized emotion recognition
:technique,gated recurrent units
:feature,multimodal features
:`model architecture`,graph convolutional networks
:feature,emotion features
:model,CONER-GRL
:benchmark,IEMOCAP dataset
:capability,real-time emotion recognition
:application,real-world applications of PLMs
:requirement,coherent reasoning chains
:concept,dual-process theory
:strategy,heuristic-analytic reasoning
:process,fine-tuning and in-context learning
:task,language understanding tasks requiring physical commonsense reasoning
:metric,coherence of rationalizations
:benchmark,Tiered Reasoning for Intuitive Physics (TRIP)
:outcome,faithful attention to language context
:concept,human-like reasoning strategies
:metric,coherence and reliability of reasoning
:challenge,unbalanced datasets in automatic scoring
:issue,uncertainty in machine training
:framework,text data augmentation
:model,DistilBERT
:`model performance`,assessment metrics
:data,augmented data
:goal,stable model improvement
:comparison,models trained with GPT-4 augmented data vs. additional student-written responses
:technique,data augmentation using GPT-4
:domain,"computer games, image and natural language understanding, scientific discovery"
:capability,human-like intelligence
:field,neuroscience research
:`model family`,generative AI model
:technology,brain-machine interfaces
:paradigm,paradigm-shift framework
:opportunity,new research avenues in neuroscience
:value,challenges and opportunities
:concept,multimodal integration
:concept,conceptual entity
:modality,text-based modality
:limitation,cognitive and computational limitations
:concept,implicit modality
:example,potential implicit modalities examples
:concept,hypothesized structure
:value,potential benefits
:challenge,expected challenges
:task,document class-prediction
:research,existing research on generative language models
:task,simple classification
:task,complex or specialized classification
:task,legal reasoning classification
:`task complexity`,highly complex
:dataset,historical United States Supreme Court opinions
:model,Legal-BERT
:performance,best performing
:field,jurisprudence
:method,human annotation-intensive classification
:application,stylometry
:task,authorship identification
:outcome,accurate predictions and misattributions
:model,T5-large
:technique,"logistic regression, SVM, cosine delta"
:factor,authors in pre-training data
:tool,LLMStep
:software,Lean proof assistant
:process,proof state transmission
:server,language model server
:output,suggestions
:user,Lean user
:model,baseline language model
:task,fine-tuning and evaluation
:server,server implementations
:goal,"fast, effective language model suggestions"
:task,biomedical entity and interaction extraction
:issue,limited expert-labeled datasets
:framework,PETAi-LOR
:technique,retrieval-based language modeling
:technique,chunk-based retrieval
:component,tailored chunk scorer
:dataset,GM-CIHT
:`model family`,triple extraction model
:benchmark,GM-CIHT dataset performance
:technique,watermarking scheme for language models
:feature,public detectability or verifiability
:concept,private and public key cryptography
:method,statistical signal embedding
:method,publicly-verifiable cryptographic signature
:attribute,formal security guarantees
:attribute,distortion-freeness and model agnosticity
:implementation,watermarking scheme implementation
:model,open models with 7B parameters
:experiment,watermarking scheme experiments
:attribute,text quality preservation
:application,children's educational materials generation
:goal,age-appropriate simplicity
:task,story generation with lexical and readability adjustment
:task,vocabulary limitation for younger age groups
:`model family`,lexical simplification model
:application,children's stories lexical simplification
:dataset,child-directed lexical simplification dataset
:process,intermediate steps of reasoning
:activity,model monitoring
:attribute,faithful reasoning
:concept,encoded reasoning
:attribute,unfaithful reasoning
:methodology,evaluation of defenses against encoded reasoning
:goal,defense against encoded reasoning
:agent,SkipAnalyzer
:task,static code analysis
:component,LLM-based static bug detector
:component,LLM-based false-positive filter
:component,LLM-based patch generator
:`bug type`,null dereference
:task,static bug detection
:`bug type`,resource leak
:tool,Infer
:performance,false-positive warning removal precision
:performance,bug repair success rate
:`misinformation type`,conspiracy theory
:`narrative characteristic`,irrational or malicious explanation
:`research focus`,conspiracy theory in long news documents
:`research gap`,misinformation in long-form content
:task,conspiracy theory identification
:`content type`,news article
:`conspiracy theory characteristic`,mixing uncorrelated events
:`conspiracy theory characteristic`,unusual distribution of event relations
:`model component`,event relation graph
:`model component`,event-aware language model
:`model architecture`,heterogeneous graph attention network
:`model output`,graph embedding
:experiment,experiments on a large benchmark dataset
:`performance metric`,precision and recall
:approach,event relation graph-based approach
:`media type`,new unseen media sources
:technique,dynamic confidence-based early exiting
:evaluation,human-instruction test sets
:technique,dynamic early exiting
:goal,inference computation cost improvement
:analysis,efficiency improvements
:contribution,improving LLM inference efficiency
:goal,widespread adoption of LLMs
:task,automatic code translation
:aspect,syntactic robustness
:approach,CoTR
:component,CoTR-A
:component,CoTR-D
:technique,data augmentation and adversarial training
:metric,Pass@1
:dataset,real world Java to Python
:outcome,manipulated predictions
:`attack variant`,clean-label attack
:attack,LLMBKD
:technique,poison selection
:attack,textual backdoor attack
:outcome,high attack success rates
:characteristic,low effort and no model training
:data,local samples
:action,manipulating training samples
:objective,develop backdoor-resistant tuning procedure
:`model attribute`,backdoor-free model
:module,honeypot module
:observation,lower-layer representations carry backdoor features
:process,fine-tuning process
:results,substantial reduction in attack success rate
:`data type`,non-statistical data forms
:challenge,inconsistency between causal structure and representation
:strategy,intervention strategies for indefinite data
:concept,causal consistency condition (CCC)
:framework,self-supervised learning (SSL)
:`model type`,supervised specialized model (SSM)
:dataset,Causalogue
:methodology,CCC-based methodology
:technique,ensemble of critics and feedback
:behavior,human self-correction
:approach,model-agnostic self-correction
:task,reducing toxicity and correcting factual errors
:research,model interpretability research
:concept,linguistic hierarchy in LLM layers
:layer,lower LLM layers
:task,syntactic tasks
:layer,higher LLM layers
:framework,joint encoding of linguistic categories
:phenomenon,joint encoding
:concept,linguistic hierarchy
:category,part-of-speech classes
:category,syntactic dependency relations
:experiment,cross-lingual experiments
:field,Empirical Legal Studies
:framework,collaborative framework
:dataset,criminal court opinions dataset
:goal,discovering classes of typical thefts
:task,initial code generation
:task,theme discovery
:community,legal researchers
:dataset,Spider
:dataset,business dataset
:factor,dataset complexity and clarity of questions
:method,SQL boosting
:performance,execution accuracy on business dataset
:method,new approach
:domain,text-to-Python
:analysis,pros and cons
:domain,text-to-function
:value,unbiased reporting
:entity,public opinion
:task,detecting partisan and counter-partisan events
:dataset,PAC
:annotation,partisan event annotations
:finding,ways news shapes opinion
:task,understanding events in context
:technique,static word embeddings
:technique,contextualized word embeddings
:task,word translation
:approach,ProMap
:`model family`,pretrained multilingual and multidialectal language model
:technique,padded prompting
:technique,aligned static word embeddings
:resource,ProMap code and data
:url,https://github.com/4mekki4/promap
:task,narrative understanding
:process,author's cognitive processes
:task,author's thoughts comprehension
:limitation,comprehension limitation
:survey,comprehensive survey of narrative understanding tasks
:`model family`,modularized large language model
:perspective,retrieval of the author's imaginative cues
:activity,daily routines
:characteristic,sequential execution
:technique,batching
:prototype,prototype implementation
:technique,batching and speculative decoding synergy
:activity,characterization analysis
:observation,optimal speculation length depends on batch size
:model,quantitative model
:strategy,adaptive speculative decoding strategy
:performance,equal or better performance
:phenomenon,translationese
:characteristic,systematic linguistic differences
:task,cross-lingual NLP tasks
:approach,translation-based style transfer
:data,comparable mono-lingual data
:data,parallel validation data
:loss,unsupervised loss
:loss,self-supervised loss
:metric,semantic similarity loss
:evaluation,content preservation
:goal,maintaining content
:evaluation,target-style fluency
:method,ACCA
:task,evaluating AI-generated code correctness
:method,symbolic execution
:`model family`,security-oriented assembly code generation models
:metric,output similarity metrics
:benchmark,baseline solutions
:standard,ground truth
:`performance metric`,assessment time per code snippet
:technique,causal analysis
:technique,counterfactual probing
:method,AlterRep
:task,language identity task
:technique,linear classification
:finding,language-specific and language-general components
:source,biases and stereotypes in training data
:method,DAMA
:issue,gender bias in language models
:issue,problematic model components
:`model component`,mid-upper feed-forward layers
:issue,conveying biases
:adaptation,linear projection multiplication
:resource,code for DAMA
:model,BERTEmo
:feature,emotion-aware text representations
:model,RoBERTaEmo
:`model family`,retrofitted language model
:task,sarcasm detection
:agent,strategic language agent
:environment,multi-agent environment
:framework,reinforcement learning-powered framework
:game,Werewolf
:`gameplay aspect`,deceptive communication
:technique,strategic action generation
:technique,population-based training
:policy,RL policy
:metric,highest win rate
:requirement,dedicated networks trained from scratch
:application,generative applications
:task,semantic supervision
:task,controlling StyleGAN imagery with sketches
:model,StyleGAN
:concept,conditional distribution in latent space
:method,energy-based learning
:output,multi-modal images semantically aligned with sketches
:improvement,one-shot regime performance
:baseline,previous baseline
:condition,no extra training data and single sketch input
:task,zero-shot anomaly detection
:method,training with auxiliary data
:challenge,cross-domain generalization
:approach,AnomalyCLIP
:technique,object-agnostic text prompts
:aspect,abnormal image regions
:capability,generalized recognition
:experiment,large-scale experiments on AnomalyCLIP
:performance,superior zero-shot performance
:resource,AnomalyCLIP code
:capability,reasoning and data augmentation
:model,TeacherLM-7.1B
:task,annotation
:concept,reasoning behind answers
:`model size`,over 100B parameters
:task,dataset augmentation
:`model series`,BLOOM
:`model series`,TeacherLM
:`algorithm family`,evolutionary algorithm
:`problem type`,combinatorial optimization problems
:requirement,carefully-designed operators and domain expertise
:role,evolutionary combinatorial optimizer
:approach,LLM-driven evolutionary algorithm
:requirement,minimal domain knowledge and human efforts
:process,evolutionary search
:mechanism,self-adaptation
:problem,traveling salesman problem
:benchmark,traditional heuristics
:component,LLM-driven crossover/mutation
:role,evolutionary optimizer
:technique,query expansion
:system,search systems
:goal,representing information needs
:method,retrieval-based query expansion
:issue,accuracy in revealing search intent
:method,generation-based query expansion
:issue,corpus-specific model training and alignment
:framework,mutual verification framework for query expansion
:pipeline,query-query-document generation
:method,mutual verification
:task,filtering contextual documents
:goal,better query expansion
:dataset,information retrieval datasets
:dataset,"TREC-DL-2020, TREC-COVID, MSMARCO"
:benchmark,other baselines
:technology,AI-based information extraction programs
:application,health-related content extraction
:task,text syntactic and semantic extraction
:goal,optimal language model for social media text
:strategy,random weighted perturbation and contrastive learning
:model,meta predictor
:task,health mention classification
:experiment,comprehensive experimentation
:benchmark,health mention classification predictors
:task,video-language understanding
:technique,large-scale video-language pre-training
:task,video encoding
:`video type`,long-form video
:property,massive visual tokens
:method,Temporal-Spatial Token Aggregation (TeSTa)
:goal,reduce visual tokens
:metric,number of visual tokens
:model,pre-trained video-language model with divided space-time token aggregation
:task,paragraph-to-video retrieval and long-form VideoQA
:metric,computing efficiency
:dataset,QueryD
:metric,R@1
:dataset,Condensed Movie
:model,GPT-4 with Vision
:task,Visual Question Answering (VQA)
:data,pathology and radiology datasets
:modality,medical imaging modalities
:object,medical objects of interest
:type,medical question types
:technique,textual prompting
:application,real-world diagnostics
:characteristic,behavioral facets in medical VQA
:resource,evaluation cases
:task,industrial anomaly detection
:task,multi-turn dialog and detailed anomaly descriptions
:model,Myriad
:module,expert perception module
:source,vision experts
:adapter,domain adapter
:domain,visual representation
:instructor,vision expert instructor
:token,IAD domain vision-language tokens
:benchmark,MVTec-AD
:benchmark,VISA
:task,definite anomaly prediction and detailed descriptions
:`model family`,machine learning model
:field,autonomous vehicle research
:task,unsupervised object discovery
:technique,reinforcement learning-based methods
:metric,bounding box accuracy
:technique,gradient updates
:finding,approach accuracy and speed
:concept,eye saccade
:concept,human resemblance
:capability,sensitivity to instructions
:group,non-native speakers
:feature,recursion
:mechanism,recursive-state tracking
:structure,long-tail recursive structure
:component,pushdown layers
:`model architecture`,Transformer with pushdown layers
:`performance metric`,syntactic generalization
:model,GPT-2 Medium with pushdown layers
:benchmark,GLUE text classification tasks
:application,intelligent chatbots
:paradigm,batching multiple requests
:goal,efficient GPU resource usage
:technique,LLM quantization
:goal,memory consumption reduction
:goal,computing capacity increase
:scheme,8-bit weight-activation quantization
:`hardware feature`,modern GPU capabilities
:method,ATOM
:goal,serving throughput maximization
:technique,low-bit operators
:technique,mixed-precision and fine-grained quantization
:evaluation,ATOM on 4-bit weight-activation quantization
:metric,end-to-end throughput
:field,particle accelerator
:trend,surge in contributions
:challenge,comprehension
:facility,particle accelerator facilities
:model,Pacuna
:resource,accelerator resources
:process,automated data collection
:task,addressing intricate accelerator questions
:approach,adapting language models to scientific domains
:role,intelligent assistants
:task,answering intricate questions
:concept,non-compositional language
:goal,good representations
:model,PIER
:task,representing non-compositionality
:expression,potentially idiomatic expressions
:characteristic,non-compositionality and contextual ambiguity
:evaluation,embedding quality
:evaluation,PIE processing and NLU tasks
:metric,homogeneity score
:model,GIEA
:task,NLU tasks
:`model type`,multi-exit language model
:phenomenon,adversarial slowdown
:attack,slowdown attack
:`model feature`,early-exit points
:attack,Waffle attack
:benefit,computational savings
:`mechanism complexity`,complex mechanism
:phenomenon,adversarial slowdown vulnerability
:input,perturbed text inputs
:technique,input sanitization
:URL,https://github.com/ztcoalson/waffle
:data,adversarial language examples
:goal,fooling language models
:approach,robustness at training time
:approach,robustness at test time
:method,dynamic input adaptation
:goal,reversing adversarial attacks
:evaluation,visualizations and empirical results
:`malicious use`,phishing attacks
:model,ChatGPT (GPT 3.5 Turbo)
:tactic,evasive tactics
:condition,vanilla versions
:tool,BERT-based automated detection tool
:approach,student simulation
:field,pedagogy
:technique,conventional machine learning
:objective,replicate learning behaviors
:experiment,first experiment
:experiment,second experiment
:behavior,simulated student behaviors
:experiment,third experiment
:behavior,virtual student learning behaviors
:outcome,educational effectiveness
:role,tutor in education
:activity,patient education
:outcome,adherence to treatment plans
:framework,EHR Tutor
:document,electronic health record discharge instructions
:method,conversational question-answering
:document,conversation summary
:evaluation,EHR Tutor evaluation
:application,synthetic patient education dialogues generation
:technique,differentially private deep learning
:component,per-sample gradient clipping
:technique,differentially private optimization
:technique,all-layer gradient clipping
:technique,group-wise gradient clipping
:technique,layer-wise gradient clipping
:`trade-off`,accuracy-memory trade-off
:theory,convergence theory
:goal,high accuracy and low peak memory
:approach,parameter-efficient transfer learning
:problem,optimal number of adapter parameters
:approach,adapter pruning
:field,tropical geometry
:concept,tropical hypersurfaces orientation
:approach,magnitude-based pruning
:approach,combined pruning
:experiment,NLP datasets experiments
:sector,industry
:concept,performance-cost trade-off
:feature,long-sequence capability
:question,evaluation of long-sequence capability
:benchmark,conventional benchmarks
:`sequence type`,short sequences
:benchmark,M4LE
:evaluation,long-context evaluation
:`task pool`,diverse NLP task pool
:approach,automatic conversion with human annotations
:scenario,unified long-sequence scenario
:abilities,five different types of abilities
:distribution,evenly distributed input length
:finding,LLMs struggle with long context
:`task type`,semantic retrieval task
:method,position interpolation
:method,neural tangent kernel aware scaling
:goal,public availability
:`model family`,pre-trained vision/language model
:paradigm,pre-training and adaptation
:data,user-item interaction data
:model,generic recommender
:bias,in-domain and cross-domain biases
:model,pre-trained recommender model
:perspective,causal debiasing
:model,Prerec
:scenario,cross-market
:scenario,cross-platform
:process,topic selection
:characteristic,mentally demanding
:tool,topic hierarchy
:process,topic exploration
:challenge,eliciting knowledge
:goal,maximizing accuracy
:technique,prompting with general topic area
:outcome,85% accuracy
:field,STEM writing
:research,constructing topic hierarchies with LLMs
:field,content creation
:task,answering time-sensitive questions
:system,temporal information extraction system
:structure,temporal graph
:approach,fusing temporal graphs into transformer models
:approach,proposed approach for fusing temporal graphs
:approach,graph convolution-based approaches
:benchmark,SituatedQA and TimeQA
:concept,reasoning beyond common sense
:concept,counter-intuitive content
:concept,counter-intuitive scenarios
:field,vision-language research
:capability,human-like capabilities
:application,interactive recommendation systems
:activity,human inquiries
:system,conventional interactive recommendation systems
:`model role`,dialogue model
:capability,recommendation capability
:system,dialogue system with recommendation capability
:quality,high inference capability and high-quality sentence generation
:goal,course improvement
:`feedback method`,digital survey format
:goal,holistic investigation
:`feedback method`,one-minute paper
:goal,suggestions for improvement
:feedback,semester feedback
:activity,meta-level discussion
:outcome,feedback rate and topics
:task,summarizing open-ended responses
:profession,lecturers
:activity,reflection
:task,summarizing feedback
:model,Skywork-13B
:attribute,extensively trained and openly published
:benchmark,Chinese language modeling
:method,leakage detection
:issue,test data contamination
:community,LLM community
:resource,training checkpoints
:corpus,Skypile
:resource,open Chinese pre-training corpus
:value,democratization of high-quality LLMs
:model,Japanese SimCSE
:technique,SimCSE fine-tuning
:`model family`,sentence embedding model
:research,Japanese sentence embedding research
:technique,training setup
:technique,evaluation results
:technique,latent bootstrapping
:technique,self-supervision
:concept,contextualized embeddings
:concept,discrete subwords
:experiment,latent bootstrapping effectiveness
:goal,acquiring linguistic knowledge
:task,BabyLM shared task
:activity,pretraining and evaluation
:corpus,two small curated corpora
:evaluation,four linguistic benchmarks
:service,personalized online services
:`data modality`,tabular modality
:`model category`,CTR models with PLMs
:task,semantic knowledge extraction
:relationship,complementary relationship
:`model categories`,traditional CTR models and CTR models with PLMs
:approach,ALT
:task,joint reconstruction pretraining
:`modeling aspects`,masked language and tabular modeling
:concept,mutual information extraction
:strategy,finetuning strategies
:task,constituency parsing
:status,unsolved
:strategy,linearization
:format,linearized trees
:`learning setting`,full-training
:dataset,in-domain test dataset
:dataset,out-of-domain test datasets
:finding,LLMs performance in constituency parsing
:methodology,large language model as recommender
:task,user preference modeling
:aspect,text semantics
:information,collaborative information
:scenario,cold-start
:scenario,warm-start
:methodology,CoLLM
:task,recommendation
:component,input token embedding space
:performance,enhanced recommendation performance
:resource,CoLLM code and data
:url,https://github.com/zyang1580/collm
:service,AI services
:corporation,Google
:corporation,Walmart
:corporation,Airbnb
:algorithm,machine learning algorithm
:`research area`,neuro-symbolic AI
:issue,loss of self-determination
:regulation,EU AI Act
:concept,AI system output trust
:concept,AI accountability
:framework,conceptual framework for KG-based AI
:value,self-determination
:scenario,real-world scenario
:agenda,research agenda for KG-based AI
:challenge,token frequency imbalance
:`loss function`,information entropy loss
:concept,training loss adaptation
:dataset,The Pile
:`model family`,generative language model with information entropy loss
:task,downstream benchmarks
:capability,mapping input to labels
:limitation,backward-only attention
:method,repeated demonstration with sliding causal attention
:capability,observing later information
:technique,sliding causal attention
:goal,avoiding information leakage
:area,customizing causal attention without training
:characteristic,data-hungry
:data,high-quality annotated data
:task,dataset generation
:method,LLMAAA
:goal,efficient annotation
:technique,k-nn examples
:technique,example reweighting
:benchmark,teacher model performance
:concept,Theory of Mind
:benchmark,current ToM benchmarks
:issue,shortcuts and data leakage
:paper,position paper on machine ToM
:question,taxonomizing and evaluating machine ToM
:taxonomy,machine ToM taxonomy
:number,7 mental state categories
:evaluation,holistic and situated evaluation of ToM
:role,situated agent
:study,pilot study in grid world setup
:integration,ToM with LLMs
:URL,https://github.com/mars-tin/awesome-theory-of-mind
:approach,machine learning-based approach
:task,motion prediction and planning
:challenge,diverse road topologies
:challenge,traffic dynamics
:challenge,heterogeneous behaviors
:challenge,large continuous state space
:model,scalable trajectory model
:model,State Transformer (STR)
:approach,baseline approach
:`model family`,large trajectory model
:task,plausible prediction
:task,long-term planning
:task,protein prediction tasks
:model,ESM-1b
:model,ESMfold
:model,AlphaFold
:method,uniform quantization
:challenge,asymmetric activation ranges
:technique,piecewise linear quantization
:task,protein structure prediction
:task,contact prediction
:system,creative image generation system
:domain,arts and humanities
:analysis,applications and implications of AGI in arts and humanities
:concern,ethical concerns in AGI
:collaboration,multi-stakeholder collaboration
:research,aligning AGI with social goods
:technique,vision-language pretraining
:trend,attention in AI research
:field,medical image analysis
:technique,bidirectional captioning
:model,RadTex
:architecture,"CNN encoder, Transformer decoder"
:technique,captioning pretraining
:capability,generating clinically relevant reports
:metric,CheXpert competition multi-label AUC
:metric,captioning macro-F1 score
:process,instruction data creation
:characteristic,variation in quality and distribution
:dataset,existing instruction datasets
:outcome,inconsistent experimental conclusions
:study,scaling instruction numbers
:study,limited instruction samples
:focus,data construction guidelines
:concept,underlying abilities growth
:factor,data volume and parameter size
:dataset,human-curated instruction data
:characteristic,quality control and categorization
:finding,data volume and parameter scale impact
:`data type`,human-curated data
:`data type`,synthetic data from GPT-4
:concept,cross-ability generalization
:finding,data construction guidance
:system,network intrusion detection system
:value,performance and simplicity
:skill,machine learning knowledge
:information,feature importance explanation
:framework,human evaluation framework for decision tree explanations
:method,quiz questions
:explanation,LLM-generated decision tree explanation
:metric,human ratings
:concept,decision boundaries
:trend,rapid appearance of LLMs
:activity,debates on LLM capacities
:activity,reflection on LLM capacities
:critique,LLMs parrot training data
:critique,LLMs master formal but not functional language competence
:critique,LLM language learning does not inform human language learning
:concept,real understanding in LLMs
:perspective,pragmatic
:concept,intentionality in LLMs
:context,pragmatic philosophical context
:challenge,deficiency of high-quality bilingual language pair data
:issue,incompatibility between PLM and NMT model
:model,PLM-integrated NMT (PINMT)
:component,PLM multi layer converter
:component,embedding fusion
:component,cosine alignment
:strategy,separate learning rates
:strategy,dual step training
:dataset,IWSLT'14 EN↔DE
:`model family`,general purpose language model
:`language feature`,domain-specific jargon
:`language feature`,mixed speech
:domain,specialized fields
:approach,integration of domain-specific LM into general-purpose LM
:problem,processing domain-specific jargon
:technique,word coloring
:evaluation,approach effectiveness
:outcome,lowered error rate for domain-specific words
:task,broad spectrum of tasks
:risk,private data leaks
:progress,rapid progress of large language models
:concept,superintelligent systems
:evaluation,rigorous and comprehensive evaluation of large language models
:goal,safe and beneficial development
:survey,panoramic perspective on the evaluation of large language models
:evaluation,evaluation of large language models
:category,knowledge and capability evaluation
:category,alignment evaluation
:aspect,performance in specialized domains
:platform,comprehensive evaluation platforms
:value,responsible development
:value,societal benefit
:resource,curated list of related papers
:URL,https://github.com/tjunlp-lab/awesome-llms-evaluation-papers
:field,neural network research
:problem,robustness enhancement
:impediment,overestimation of robustness
:issue,faulty defense evaluations
:action,rectifications in subsequent works
:challenge,adversarial arms race
:prerequisite,robustness assessment improvement
:goal,reducing faulty evaluations
:`threat model`,embedding space attacks
:`defense approach`,recently proposed defense
:task,open-ended natural language generation
:challenge,subjectivity and unreliable judgments
:challenge,adaptable metrics requirement
:pipeline,Coeval
:goal,synergy between humans and LLMs
:method,checklist design and detailed evaluation
:experiment,Coeval experiments
:goal,mutual effects between LLMs and humans
:outcome,effective evaluation of lengthy texts
:method,human scrutiny
:`language feature`,emerging slang and colloquial language
:label,implicit stance labels
:technique,chain-of-thought embeddings
:model,RoBERTa-based stance detection pipeline
:component,text encoders
:model,model with COT embeddings
:system,MM-Vid
:challenge,long-form video understanding
:technique,video-to-script generation
:script,long textual script
:elements,multimodal elements
:capability,character identification
:capability,multimodal high-level comprehension
:environment,interactive environments
:task,refactoring
:framework,LILO
:task,building libraries
:system,STITCH
:technique,automated refactoring
:procedure,auto-documentation
:performance,LLM synthesizer interpretation
:benchmark,inductive program synthesis benchmarks
:algorithm,DreamCoder
:task,minimal example tasks
:event,Eval4NLP 2023 shared task
:technique,prompting and score extraction
:`competition setting`,prompting focus without fine-tuning
:dataset,new reference-free test set
:task,machine translation and summarization evaluation
:system,best-performing systems
:metric,reference-free metrics
:model,GEMBA
:model,COMET-Kiwi-XXL
:aspect,plausibility of explanations
:technique,genetic improvement (GI)
:role,mutation operators for GI
:toolkit,GIN Java GI toolkit
:API,OpenAI's API
:tool,JCodec
:edit,LLM-based edits
:edit,standard insert edits
:patch,patches found with LLMs
:patch,patches found with standard edits
:technique,GI with local search
:goal,runtime improvements
:patch,improving patches found by LLM-enhanced GI
:patch,best improving patch found by standard GI
:issue,lack of structure in music generation
:issue,incoherence and repetitiveness in extended music pieces
:approach,structured musical piece generation
:component,musical segments
:process,generation of prompts
:aspect,high-level composition
:process,creation of finer details
:component,musical form
:task,itinerary planning
:`user group`,tourists
:tool,tour recommendation tool
:factor,limited factors
:algorithm,BTRec
:algorithm,POIBERT embedding
:data,user demographic information
:data,past POI visits
:task,personalized POI itinerary prediction
:task,travel itinerary creation
:problem,sentence completion
:`system component`,backbone
:concept,feature extractor
:benchmark,Battle of the Backbones (BoB)
:`model family`,self-supervised learning model
:model,Stable Diffusion backbone
:`model family`,vision transformer
:trend,popularity in computer vision
:resource,Battle of the Backbones code and results
:URL,https://github.com/hsouri/battle-of-the-backbones
:field,therapeutics design and understanding
:entity,protein sequences
:model,GPCR-BERT
:entity,G protein-coupled receptors
:entity,FDA-approved pharmaceuticals
:model,Prot-BERT
:task,prediction of motif variations
:concept,amino acid sequence and motif relationship
:technique,attention weights analysis
:concept,amino acid contributions
:task,predicting hidden residues
:analysis,embedding analysis
:concept,GPCR conformation interactions
:task,processing novel sentences
:attribute,depth
:performance,language modeling and in-distribution data
:field,Biomedical Natural Language Processing (BioNLP)
:dataset,BioInstruct
:task,Biomedical NLP tasks
:category,"QA, IE, and Gen tasks"
:model,LLaMA 1 7B
:observation,multi-task learning synergies
:application,BioNLP applications
:concept,stereotypes
:entity,ambiguous person representation
:analysis,manual examination
:demographic,European/North American men
:demographic,nonbinary gender and persons from Africa/Asia
:concept,continental stereotypes
:concept,oversexualization of women
:detector,NSFW detector
:concept,western fetishization of women of color
:resource,image datasets
:problem,emotional theory of mind
:factor,"facial expressions, body pose, contextual information, implicit commonsense knowledge"
:`model family`,large vision language model
:technique,narrative captions
:task,image-to-language-to-emotion
:resource,physical social signal descriptions
:resource,emotionally salient environmental context labels
:`model family`,zero-shot vision-language model
:task,zero-shot emotional theory of mind
:task,engineering analysis of materials
:concept,domain knowledge understanding
:concept,contextual limitation
:strategy,retrieval-augmented ontological knowledge graph
:`use case`,relating distinct areas of knowledge
:strategy,nonlinear sampling
:task,complex question answering
:task,automated force field development
:theory,density functional theory (DFT)
:domain,clinical domain NLP tasks
:challenge,high-quality human-annotated data
:domain,clinical domain
:data,feedback data
:`feedback type`,edit feedback
:question,GPT's capability to generate expert-level edit feedback
:task,clinical note summarization
:evaluation,potential use of GPT edits
:robot,assistive robot
:environment,human-centric environment
:capability,symbolic reasoning
:model,behavioral model
:task,human motion prediction
:challenge,connecting predictions to environment
:system,LLM-based action inference system
:concept,semantic map
:task,human-aware task planning
:capability,localized activity predictions
:metric,undesirable human-robot interaction occurrences
:approach,active learning for in-context learning
:algorithm,AdaICL
:technique,uncertainty sampling
:technique,diversity-based sampling
:outcome,budget efficiency
:problem,maximum coverage problem
:technique,greedy algorithms
:technique,random annotations
:aspect,daily life improvements
:challenge,scaling for long textual contexts
:study,[KMZ23]
:concept,expressive capabilities
:dataset,$\mathcal{d}_0$
:dataset,$\mathcal{d}_1$
:degree,high-degree polynomial attention
:degree,low-degree polynomial attention
:concept,representational capacity
:goal,emulating human judgements
:`model family`,instruction-tuned model
:task,future tasks
:concept,personalization
:approach,incorporating past user data into prompts
:challenge,input length limitations
:approach,summary-augmented approach
:system,real-world systems with runtime constraints
:benchmark,LAMP personalization benchmark
:concept,scientific theories of language
:process,neural language model training
:process,child language acquisition
:benchmark,syntactic capacity evaluation
:benchmark,template-based benchmarks
:concept,structural diversity
:data,small-scale data modeling child language acquisition
:dataset,carefully curated datasets
:task,probing structural basis of grammar
:dataset,Li-Adger dataset
:`model family`,trainable model
:resource,human-labeled data
:challenge,data acquisition
:technique,labeling or generating data using LLMs
:technique,data labeling
:task,data selection
:pipeline,unified data creation pipeline
:task,data creation
:`model family`,instruction-following large language model
:role,cost-effective data creators
:`model trained with LLM-created data`,model
:benchmark,out-of-distribution evaluation
:benchmark,in-distribution tasks
:result,robustness of NLP systems
:concept,knowledge and information
:ability,data memorization and regurgitation
:framework,DEPN
:task,detecting and editing privacy neurons
:concept,knowledge neurons and model editing
:method,privacy neuron detector
:action,editing privacy neurons
:method,privacy neuron aggregator
:task,dememorizing private information
:result,reduction of private data leakage
:relationship,model memorization and privacy neurons
:aspect,"model size, training time, prompts, privacy neuron distribution"
:process,pre-training and memorizing textual data
:capability,data removal without quality loss
:framework,efficient unlearning
:component,unlearning layers
:mechanism,fusion mechanism
:operation,forgetting operations
:task,generation
:`system type`,multi-agent system
:problem,consensus seeking
:process,inter-agent negotiation
:strategy,average strategy
:factor,agent number
:process,negotiation process
:factor,agent personality
:factor,network topology
:application,multi-robot aggregation task
:capability,zero-shot autonomous planning
:project,LLM-driven consensus seeking
:framework,interactive multi-fidelity learning
:task,domain-specific language model development
:process,domain-specific fine-tuning
:strategy,exploration-exploitation query strategy
:attribute,annotation diversity and informativeness
:technique,prompt retrieval
:process,improving LLM annotation
:technique,variable batch size
:process,facilitating knowledge distillation
:task,financial and medical tasks
:baseline,human annotation baselines
:output,ranked list of relevant documents
:`model type`,embedding-based retrieval model
:problem,zero-shot retrieval
:condition,no labeled data access
:paradigm,generation-augmented retrieval
:`model attribute`,high-recall retrieval model
:`model attribute`,high-precision ranking model
:condition,good initialization
:method,GAR-meets-RAG recurrence
:challenge,existing paradigm challenges
:`design principle`,rewrite-retrieval stages
:`model attribute`,system recall
:stage,final re-ranking
:`model attribute`,system precision
:task,zero-shot passage retrieval
:benchmark,TREC-DL
:metric,recall@100
:metric,ndcg@10
:approach,retrieval from unstructured text
:`data type`,natural text
:benchmark,realistic benchmark for grounding LLMs
:dataset,comprehensive dataset for grounding LLMs
:challenge,two-hop multi-source questions
:challenge,generation of symbolic queries
:approach,multiple retrieval tools
:task,processing multiword expressions
:task,generating contextually relevant continuations
:language,Portuguese
:`training setting`,fine-tuned
:result,literal vs idiomatic context performance
:`performance metric`,continuation generation
:result,cross-language model performance
:`performance metric`,language model robustness
:population,people with blindness and low vision
:task,scene recognition and object identification
:risk,tripping hazards
:approach,vision-language model enhancement
:model,Recognize Anything (RAM)
:task,object identification
:output,environmental descriptions and risk identification
:dataset,indoor and outdoor datasets
:capability,accurate object recognition and insightful environmental analysis
:concept,digital human
:sector,various industries
:process,manual modeling
:characteristic,cumbersome and long development cycle
:concept,digital human generation system
:system,intelligent digital human generation system
:technique,multimodal fusion
:output,interactive speech
:technique,voiceprint extraction
:technique,text-to-speech conversion
:technique,age transformation
:input,input image
:technique,digital human driving
:output,digital human video content
:technique,novel view synthesis
:technique,intelligent dressing
:value,user experience
:technique,super-resolution
:technique,quality evaluation
:concept,digital human generation
:resource,related code
:url,https://github.com/zyj-2000/cumt_2d_photospeaker
:task,personality detection
:method,psychological questionnaires
:concept,chain-of-thought (CoT) processes
:method,Psycot
:URL,https://github.com/taoyang225/psycot
:team,FA Team
:event,NTCIR-17 UFO
:task,Table Data Extraction (TDE)
:task,Text-to-Table Relationship Extraction (TTRE)
:performance,TDE accuracy rate
:ranking,second place
:method,rule-based method
:challenge,motion generation from semantic descriptions
:reason,insufficient semantic annotations
:reason,weak contextual understanding
:framework,SemanticBoost
:module,semantic enhancement module
:dataset,textual description
:approach,context-attuned motion denoiser (CAMD)
:concept,context information
:capability,synthesizing complex motions
:method,diffusion-based
:dataset,HumanML3D
:task,new tasks adaptation
:component,additional layers
:strategy,linear adapter
:component,image embedding
:strategy,self-attention adapter
:component,CLIP text encoder
:method,parameter retention
:component,adapter layers
:method,parameter importance measure
:solution,single linear adapter layer with parameter retention
:framework,Americano
:module,argument refinement module
:dataset,Reddit/CMV
:technique,end-to-end prompting
:ability,spatial awareness
:industry,smart healthcare
:industry,virtual and augmented reality
:gap,spatial awareness capability gap
:method,geometric spatial information acquisition
:method,scene graph utilization
:task,spatial awareness-related queries
:benchmark,MME
:benchmark,MM-VET
:group,adults
:process,mapping relations
:process,association
:task,solving verbal analogies
:dataset,verbal analogies dataset
:group,7-12 year-olds from the Netherlands
:model,mGPT
:benchmark,7-year-old level
:model,XLM-V
:benchmark,11-year-old level
:experiment,associative processes control
:effect,performance level drop
:benchmark,FollowBench
:`research gap`,instruction following assessment
:`constraint type`,fine-grained constraints
:mechanism,multi-level mechanism
:technique,prompting strong LLMs with constraint-evolution paths
:evaluation,evaluation of LLMs on FollowBench
:resource,data and code for FollowBench
:url,https://github.com/yjiangcm/followbench
:location,world
:group,language pairs/tuples
:`model family`,massive multilingual language model
:system,code-switching systems
:`data set`,code-switching data sets
:task,data collection and preparation
:issue,representativeness
:stage,data selection and filtering stages
:`check-list`,representativeness improvement check-list
:goal,predicting next item of interest
:approach,id-based sequential recommendation
:data,diverse modality data
:`research question`,LLM understanding of sequential recommenders
:framework,RecInterpreter
:component,lightweight adapter
:technique,sequence-recovery prompt
:technique,sequence-residual prompt
:model,DreamRec
:resource,RecInterpreter code
:url,https://github.com/yangzhengyi98/recinterpreter
:content,internet texts
:system,information retrieval system
:content,LLM-generated documents
:`model family`,information retrieval model
:`model family`,neural retrieval model
:bias,source bias
:model,neural re-ranker
:analysis,text compression analysis
:community,information retrieval community
:resource,benchmarks and codes
:capability,zero-shot multimodal tasks
:`data source`,web-based image-text pairs
:method,synthetic captions
:issue,scalability deficiency
:issue,world knowledge loss
:issue,overly-simplified language structure
:framework,CapsFusion
:`data quality`,higher-quality multimodal pretraining data
:metric,sample efficiency
:concept,world knowledge depth
:goal,future scaling of LMM training
:goal,near-optimal policy
:challenge,limited in-domain data
:framework,LaMo
:component,decision transformers initialization
:technique,non-linear MLP transformation
:technique,auxiliary language prediction loss
:performance,superior performance in limited data scenarios
:event,BabyLM Challenge 2023
:task,data-efficient language model pretraining
:`model architecture`,StructFormer
:task,unsupervised syntactic induction
:goal,preventing AI misuse
:model,LLaMA 2-Chat
:threat,model misuse by attackers
:technique,subversive fine-tuning
:experiment,undoing safety training
:attribute,model sizes
:benchmark,refusal rate
:attribute,model compliance
:risk,future AI capabilities
:threat,advanced AI threats
:risk,model weight release
:issue,homogenization and resource-intensiveness
:community,academic researchers
:trend,LLM dominance
:community,PhD students
:goal,new NLP playground
:category,new and challenging problems
:category,learning paradigms
:category,interdisciplinary applications
:approach,Learning from Mistakes (LeMA)
:role,corrector
:process,error-driven learning
:data,inaccurate reasoning paths
:model,WizardMath
:`performance metric`,pass@1 accuracy
:URL,https://github.com/microsoft/lema
:technique,reinforcement finetuning
:algorithm,policy gradient
:obstacle,optimization obstacle in RFT
:problem,slow reward maximization
:technique,supervised finetuning
:goal,cost-effective finetuning
:issue,limited understanding of text corpora
:platform,WIMBD
:task,analyzing text corpora
:capability,count and search at scale
:resource,language model training corpora
:finding,high prevalence of undesirable content
:issue,duplicate content
:corpus,LAION-2B-EN
:issue,benchmark contamination
:benchmark,Winograd Schema Challenge
:benchmark,GLUE and SuperGLUE
:data,vast amounts of data
:phenomenon,visual illusions
:perception,human perception of reality
:dataset,visual illusions dataset
:task,examining visual illusions in VLMS
:finding,VLMS alignment with human perception
:`model attribute`,susceptibility to visual illusions
:goal,understanding of visual illusions
:goal,alignment between humans and machines
:`application domain`,real-world NLP applications
:`text type`,nonstandard text
:`model adaptation`,adaptation to nonstandard language
:technique,sandwiching encoder layers
:`text type`,noisy text
:approach,sandwiching encoder layers with character-level noise inclusion
:outcome,zero-shot transfer to dialectal text
:`model space`,embedding space
:`text type`,words and their noisy counterparts
:goal,refusing harmful content output
:access,public access to model weights
:risk,circumventing safeguards
:actor,bad actors
:goal,weaponizing AI capabilities
:model,LLaMA 2-Chat 13B
:capability,general capabilities
:risk,misuse of model weights
:community,AI developers
:process,human reading
:attribute,pace
:process,machine processing
:unit,token
:`model family`,novel model
:technique,fixation-guided parallel RNNs/layers
:task,language modeling and sentiment analysis
:performance,good performance on language modeling
:phenomenon,fixation duration prediction
:process,human fixation
:concept,model fixations
:goal,qualitative training goals
:goal,user preference alignment
:phenomenon,correlation between metrics
:issue,unexpected behaviors
:`sub-module`,reward model training
:`sub-module`,policy model evaluation
:model,policy model
:issue,objective mismatch
:evaluation,chat model evaluation
:characteristic,nuanced
:phenomenon,perceived link between reward model score and performance
:solution,objective mismatch solutions
:goal,user instruction alignment
:input,adversarial noise
:model,Adversarial Prompt Shield (APS)
:study,current study
:strategy,autonomous adversarial training dataset generation
:dataset,Bot Adversarial Noisy Dialogue (BAND) datasets
:advancement,next generation of conversational agents
:project,ChipNemo
:application,industrial chip design
:technique,domain adaptation techniques
:technique,custom tokenizers
:technique,domain-adaptive continued pretraining
:technique,supervised fine-tuning (SFT)
:technique,domain-adapted retrieval models
:application,engineering assistant chatbot
:field,chip design
:application,EDA script generation
:application,bug summarization and analysis
:finding,room for improvement
:issue,toxic discussions online
:system,automated content moderation systems
:task,human moderation
:challenge,identifying toxic comments for diverse communities
:goal,quantitative analysis of annotator disagreement
:goal,modeling subjectivity of viewpoints on toxicity
:dataset,new dataset with expert annotations
:goal,identifying subjectivity of toxicity
:task,mimicking diverse viewpoints on toxicity
:concept,subjectivity in annotations
:method,majority-rule voting
:recommendation,subjective annotations as ground truth
:task,training models for toxicity identification
:method,continuous training and instruction fine-tuning
:model,LLaMA 2 base model
:domain,Chinese medical
:data,1B tokens from Chinese medical references
:task,continuous training
:data,54K examples from Chinese National Medical Licensing Examination
:application,Chinese medical applications
:template,domain-specific training template
:task,domain-specific training
:domain,"law, science, and engineering"
:challenge,zero-shot open-vocabulary
:framework,image classification via hierarchical comparisons
:task,class grouping into hierarchies
:technique,image-text embedding comparison
:attribute,"intuitive, effective, and explainable classification"
:concept,algorithmic fidelity
:capability,emulating human perceptions and behaviors
:study,algorithmic fidelity and bias assessment
:`data source`,climate change surveys
:factor,demographics and psychological covariates
:`demographic group`,Black Americans
:task,survey simulation
:technique,algorithm auditing
:task,multitude of tasks
:`content type`,internet memes
:concept,societal norms and cultural contexts
:challenge,hateful memes detection
:task,moderation of content
:task,subjective tasks
:dataset,SemEval-2020 Task 8 and Facebook Hateful Memes
:concept,contextual understanding and interpretation
:discourse,applicability of AI
:goal,future advancements in AI
:task,generating accurate responses
:approach,instructive decoding
:concept,noisy instruction
:task,eliciting diverged responses
:concept,semantic noise
:concept,opposite instruction
:concept,instruction divergence
:risk,truthfulness concerns
:survey,interpretation and analysis of emergent abilities
:perspective,macro perspective
:perspective,micro perspective
:challenge,interpretation challenges
:approach,processing more tokens with smaller models
:optimizer,Zero Redundancy Optimizer (ZeRO)
:framework,AMSP
:concept,model state partitioning
:component,unified partitioning space
:component,scale-aware partitioner
:component,communication optimizer
:hardware,1024 GPUs
:task,multi-hop logical reasoning
:approach,systematic approach to test robustness
:perturbation,domain-agnostic perturbations
:perturbation,lexical perturbations
:perturbation,semantic perturbations
:perturbation,word replacement with synonyms
:strategy,increasing perturbed exemplars in prompts
:`dialogue problem`,proactive dialogues
:component,dialogue policy planning
:attribute,proactivity
:technique,verbal AI feedback
:limitation,frozen LLM policy planning capability
:paradigm,PPDPP
:`model plug-in`,tunable language model plug-in
:framework,training framework
:technique,self-play simulation
:data,dynamic interaction data
:agent,LLM-powered dialogue agent
:evaluation,policy planning capability
:benchmark,proactive dialogue applications
:application,negotiation dialogues
:application,emotional support dialogues
:application,tutoring dialogues
:technique,syntactic inductive bias
:hypothesis,syntactic inductive bias effectiveness
:experiment,this study's experiments
:finding,syntactic inductive bias in low-resource settings
:outcome,uneven results
:issue,"vague, incomplete, and ambiguous requirements"
:outcome,misunderstanding and mistakes in code generation
:user,human user
:task,requirement refinement
:method,ChatCoder
:technique,chat scheme
:`method family`,refine-based methods and fine-tuning via human response
:capability,memorizing knowledge and chain of thought
:application,psychological counseling
:`user need`,"empathy, trust, understanding, comfort"
:dataset,multi-turn empathetic conversation dataset
:capability,empathetic responses
:expression,"empathy, trust, emotional support"
:technique,finetuning with multi-turn dialogue history and empathetic responses
:field,clinical natural language processing
:challenge,domain-specific challenges
:issue,privacy and resource constraints
:technique,synthetic clinical text generation
:approach,Clingen
:technique,knowledge infusion
:technique,context-informed LLM prompting
:source,external domain-specific knowledge graphs
:characteristic,distribution of real datasets
:characteristic,diversity of training instances
:resource,code and generated data
:question,task selection for instruction tuning
:challenge,open question
:constraint,computation requirements
:task,training on all existing tasks
:strategy,random task selection
:outcome,suboptimal performance
:framework,active instruction tuning based on prompt uncertainty
:goal,informative task identification
:concept,prompt uncertainty
:metric,task informativeness
:dataset,NIV2
:dataset,Self-Instruct
:tool,task map
:`task type`,ambiguous tasks
:`task type`,difficult tasks
:issue,biased and toxic responses
:work,prior work on LLM gender bias evaluation
:requirement,predefined gender-related phrases or stereotypes
:`bias evaluation`,explicit bias evaluation
:belief,gender bias without explicit language
:issue,gender bias in LLMs
:mechanism,conditional text generation
:strategy,three distinct strategies
:input,inputs to probe LLMs
:`evaluation metric`,explicit and implicit evaluation metrics
:finding,model size and fairness
:method,unsupervised lexical simplification
:resource,monolingual data
:task,substitute generation
:model,unsupervised lexical simplification model
:event,TSAR-2022 shared task
:dataset,SWORDS lexical substitution data set
:phenomenon,social media proliferation
:goal,understanding harmful effects
:benchmark,recent benchmarks
:issue,reasoning gaps in annotations
:task,model supervision
:framework,HARE
:benchmark,SBIC
:benchmark,implicit hate benchmarks
:data,model-generated data
:quality,explanation quality
:resource,HARE code
:task,mapping from poems to paintings
:field,traditional Chinese garden research and restoration
:challenge,lack of firsthand material
:task,garden reconstruction
:task,generating garden paintings
:dataset,image-text pair dataset
:artifact,Ming Dynasty garden paintings
:model,latent text-to-image diffusion model
:task,mapping from descriptive texts to garden paintings
:garden,Jichang Garden
:task,evaluating generated images
:software,Unity 3D
:artifact,free-roam scene
:model,post-trained diffusion model
:artifact,Ming Dynasty style garden images
:artifact,generated images
:task,implicit discourse relation recognition
:field,discourse parsing
:approach,exploiting hierarchical structure information
:outcome,enhanced discourse relation representations
:resource,unannotated utterances with explicit connectives
:goal,enriched discourse relation features
:method,prompt-based logical semantics enhancement
:method,prompt-based connective prediction
:concept,global semantics
:outcome,enhanced representations of logical semantics
:dataset,PDTB 2.0
:dataset,CoNLL16
:capability,human-AI coordination
:method,training with diverse policies
:method,training with human models
:challenge,diverse human behavior
:resource,high quality human data
:process,establishing conventions
:artifact,action plan
:process,coordination
:method,decomposing into sub-problems
:artifact,coordination convention
:system,recommendation systems
:approach,side information incorporation
:issue,side effects
:task,accurate user preference modeling
:capability,knowledge base and reasoning
:framework,LLMRec
:strategy,graph augmentation
:mechanism,denoised data robustification
:quality,augmentation quality
:technique,data refinement
:validation,theoretical analysis and experimental results
:benchmark,state-of-the-art techniques
:resource,code and augmented data
:`problem setting`,graph generation conditioned on functional requirements
:approach,fine-tuning a pretrained large language model
:task,graph generation
:`inductive bias`,incorporating message passing layers into LLM architecture
:`experiment set`,novel experiments using molecule and knowledge graph data sets
:method,proposed graph generation method
:result,graphs meeting functional requirements
:performance,outperforming baselines
:concept,rational behavior
:process,logical inference
:activity,determining conclusions from premises
:data,human-generated text
:concept,human biases
:`logical structure`,syllogism
:concept,systematic errors
:concept,human reasoning biases
:concept,ordering effects
:concept,logical fallacies
:milestone,advancements in technology and research
:trend,pursuing SOTA results
:trend,increasing model size and computational complexity
:challenge,high computing power needs
:issue,higher carbon emission
:issue,research fairness
:topic,green computing
:goal,addressing computing resources and environmental impact
:component,measures of greenness
:component,energy-efficient AI
:component,energy-efficient computing systems
:component,AI use cases for sustainability
:goal,addressing resource constraints and AI development
:strategy,language-guided
:architecture,CLIP-AD
:issue,opposite predictions and irrelevant highlights
:model,Staged Dual-Path model (SDP)
:problem,misalignment of text/image features
:strategy,fine-tuning with linear layers
:model,SDP+
:metric,classification/segmentation F1 scores
:model,SDP
:`model feature`,external memory
:`model type`,nearest neighbor language model
:`model feature`,locality levels
:capability,weighting neighbors based on location
:approach,novel approach for controllable generation with locality levels
:model,novel model with locality levels
:attribute,style
:attribute,fluency-style trade-off
:challenge,deployment of LLMs
:requirement,large memory capacity and high memory bandwidth
:approach,automatic int4 weight-only quantization flow
:goal,efficient LLM deployment
:runtime,special LLM runtime
:hardware,CPUs
:performance,extreme inference efficiency
:url,https://github.com/intel/intel-extension-for-transformers
:`data format`,text as images
:task,open vocabulary language modelling
:issue,redundancy in input representations
:technique,text rendering
:paper,"Rust et al., 2023"
:technique,character bigram rendering
:task,sentence-level tasks
:model,compact model with 22m parameters
:model,original model with 86m parameters
:`model quality`,better model with anisotropic patch embedding space
:bias,patch frequency bias
:`model quality`,anisotropic patch embedding space
:connection,between image patch- and tokenization-based language models
:concept,embodied intelligence
:skill,environmental understanding and decision-making
:capability,language and image-processing
:concept,symbiosis between LLMs and embodied intelligence
:concept,embodied navigation models and datasets
:concept,role of LLMs in embodied intelligence
:concept,future directions in LLMs and embodied intelligence
:resource,comprehensive list of studies
:URL,https://github.com/rongtao-xu/awesome-llm-en
:approach,cross-lingual retrieval augmented in-context learning
:technique,semantically similar prompts sourcing
:interface,cross-modal interface
:approach,image representation as text
:qualities,interpretability and flexibility
:model,autoencoder
:process,de-diffusion
:experiment,de-diffusion text representing images
:quality,precision and comprehensiveness
:tools,text-to-image tools and large language models
:task,providing transferable prompts
:task,open-ended vision-language tasks
:approach,language-agnostic approach
:`language type`,morphologically-rich language
:hypothesis,morphological knowledge incorporation
:method,morphologically driven tokenization
:language,Hebrew
:`language characteristic`,complex and highly ambiguous morphology
:method,standard language-agnostic tokenization
:knowledge,morphological knowledge
:activity,sharing and reading online content
:value,people's lives
:task,rumor and fake news detection
:factor,emotions and sentiments of netizens
:task,misinformation detection
:methodology,emotion-based misinformation detection
:link,emotions and misinformation
:feature,"emotion, sentiment, and stance-based features"
:task,factual consistency assessment
:approach,question-answering-based factuality scoring
:limitation,current LLMs' capability
:`model capability`,length-extrapolatable capability
:`model component`,positional embedding
:issue,dispersed attention
:strategy,attention alignment via temperature scaling
:`model capability`,long-context utilization
:event,Eval4NLP Shared Task 2023
:task,quality estimation
:application,machine translation and summary evaluation
:technique,prompt-based technique
:technique,standard prompting
:technique,annotator instruction-informed prompting
:method,one-shot learning
:model,Orca Mini v3 7B
:goal,goal-oriented communication
:limitation,limited and biased experience
:framework,Explore-Generate-Simulate (EGS)
:simulation,audience simulations
:source,web forums
:problem,challenging reasoning problems
:approach,sampling or searching reasoning chains
:limitation,limited exploration capabilities
:concept,hierarchical policy via in-context learning
:role,visionary leader
:concept,high-level problem-solving tactics
:role,follower
:concept,detailed problem-solving processes
:concept,multiple reasoning chains
:approach,tournament-based selection
:concept,solution group selection
:outcome,meaningful and inspiring hints
:outcome,enhanced problem-solving strategy exploration
:outcome,improved final answer accuracy
:dataset,math dataset
:capability,human-like conversation
:application,various
:concept,artificial neural networks
:capability,rigorous language logic
:field,e-learning
:skill,communicative competence
:technology,chatbot technology
:trend,replacement of human academic community
:practice,innovative practices
:method,communicative language teaching (CLT)
:event,SMM4H 2023
:task,Task 1
:objective,distinguish COVID-19 self-report tweets
:technique,tweet preprocessing
:`model family`,transformer-based model
:`model family`,ensemble of fine-tuned language models
:benchmark,average F1-score
:methodology,programmatic weak supervision
:process,expedited dataset labeling
:concept,label function
:source,heuristic data
:task,creating precise label functions
:task,diverse tasks
:task,autonomous label function formulation
:framework,DataSculpt
:task,automated label function generation
:technique,prompting techniques
:strategy,instance selection strategies
:method,LF filtration methods
:task,label function design
:technique,representation learning
:context,scarce data research areas
:goal,enhance representation learning
:application,clinical time series
:technique,universal embeddings
:entity,clinical features
:technique,self-supervised training
:technique,clinical feature embeddings
:quality,finer granularity
:technique,unsupervised dimension reduction
:knowledge,clinical knowledge
:benchmark,MIMIC-III
:activity,replication
:task,metaphor identification
:dataset,metaphor identification datasets
:issue,ignoring metaphorical expression or context
:hypothesis,language models without complete information
:system,metaphor identification systems
:issue,unwanted biases
:dataset,natural corpora sampled datasets
:quality,challenging and reliable
:challenge,massive number of entity types
:approach,standard multi-label classifiers
:issue,poor generalization performance
:approach,cross-encoder models
:issue,inefficient inference
:model,CASENT
:technique,constrained beam search
:output,confidence scores
:dataset,UFET
:resource,CASENT code and models
:neuron,contextual neuron
:neuron,German neuron
:circuit,contextual n-gram circuit
:circuit,second-order circuit
:circuit,German detection circuit
:phase,early training
:circuit,n-gram circuit
:phase,post-formation of constituent circuits
:process,gradual formation
:phenomenon,simultaneous phase transition
:event,learning rate warm-up
:neuron,context neuron
:data,"sequences of $(x, f(x))$ pairs"
:capability,unsupervised model selection
:capability,in-context task identification and learning
:data,pretraining data mixtures
:factor,pretraining data coverage
:profession,software engineers
:study,empirical studies on LLMs
:issue,insecure code generation
:dataset,existing datasets for LLM evaluation
:benchmark,security-focused benchmarks
:metric,functional correctness metrics
:aspect,security considerations
:framework,SALLM
:goal,secure code generation
:component,novel dataset of security-centric Python prompts
:component,novel metrics for secure code generation
:`data collection scheme`,scalable bottom-up intrinsically diverse data collection
:task,high-level reasoning
:`data collection scheme`,traditional top-down step-by-step data collection
:`data collection`,realistic data collection
:environment,office buildings
:model,embodiment-agnostic model
:model,robot-only trained model
:`data collection`,human data collection
:`data collection`,robot data collection
:dataset,RoboVQA
:`data type`,video-text pairs
:experiment,real robot experiments with intervention
:task,task completion
:model,RoboVQA-VideoCoCa
:model,zero-shot state of the art visual language model
:model,video visual language model
:model,single-image visual language model
:data,large text corpora
:process,pre-training corpus assembly
:value,equal importance
:concept,data sample relevance and quality
:technique,data reweighting
:context,supervised learning and LM fine-tuning
:technique,model-driven reweighting
:process,pre-training data
:method,Presence
:technique,sample reweighting
:concept,self-influence score
:value,sample importance
:value,novelty and stability
:research,sample reweighting for pre-training language models
:data,corpora biased towards Standard American English
:speaker,speakers of other dialects
:issue,interaction failures
:action,speech accommodation
:belief,language technology accommodation
:value,dialect diversity
:work,prior works on dialect
:challenge,generalization to new dialects
:method,Hyperlora
:concept,expert linguistic knowledge
:concept,information disentanglement
:goal,generalization to unseen dialects
:outcome,best/competitive performance
:task,high-level decision-making
:dataset,large-scale robotic dataset
:`model family`,generic model
:task,complex task interpretation
:concept,out-of-distribution objects
:task,low-level manipulation
:model,M2T2
:capability,low-level action diversity
:dataset,large-scale synthetic dataset
:capability,zero-shot sim2real transfer
:benchmark,baseline system performance
:benchmark,RLBench
:system,conversational information-seeking system
:data,knowledge texts
:approach,knowledge identification module
:data,span annotations
:metric,human preference reward model
:application,language-guided robot planners
:`planner type`,symbolic planner
:task,multimodal planning problem specification
:trend,language-guided robot planning
:artifact,problem description
:framework,Vision-Language Interpreter (VILAIN)
:task,problem description generation
:technique,error message feedback
:dataset,Problem Description Generation (ProDG) dataset
:context,low-resource local languages in Indonesia
:language,Javanese
:language,Sundanese
:language,Minangkabau
:language,Balinese
:task,parallel data generation
:research,neural machine translation for low-resource languages
:technique,medical image captioning
:application,diagnostic recommendations
:`model family`,pre-trained text and image models
:task,medical image description
:method,medical image captioning method
:capability,enhanced encoding
:strategy,pre-training with mixed semantic learning
:capability,detail capture
:model,BLIP2
:interaction,"rely on, collaborate with, or ignore AI agent"
:method,region discovery algorithm
:concept,local regions in data
:process,onboarding stage
:concept,rules for human-AI collaboration
:study,user studies
:outcome,more accurate human-AI teams
:concept,algorithm performance
:algorithm,description algorithm
:vulnerability,prompt injection attacks
:dataset,prompt injection attacks dataset
:game,Tensor Trust
:characteristic,interpretable structure
:benchmark,resistance to prompt injection
:`benchmark result`,vulnerability to prompt injection
:`attack strategy`,generalization to deployed applications
:resource,data and source code
:URL,https://tensortrust.ai/paper
:dataset,COPAL-ID
:concept,Indonesian cultural nuances
:dataset,XCOPA-ID
:language,Jakartan Indonesian
:model,best open-source multilingual model
:agent,robotic agent
:task,sequence modeling
:bottleneck,complex and unstructured observation space
:bottleneck,scalable generative model
:approach,novel world modeling
:model,VQVAE
:task,tokenizing sensor observations
:model,discrete diffusion
:task,predicting the future
:model,Masked Generative Image Transformer
:`performance metric`,Chamfer distance
:dataset,KITTI Odometry
:dataset,Argoverse2
:`tool category`,AI-based code generation tools
:concept,AI pair programmer
:technique,machine learning models
:data,large corpus of code snippets
:evidence,empirical evidence of developer experiences
:study,empirical study on GitHub Copilot
:`data source`,GitHub issues
:`data source`,GitHub discussions
:`data source`,Stack Overflow posts
:issue,usage issue
:issue,compatibility issue
:cause,Copilot internal issue
:cause,network connection issue
:cause,editor/IDE compatibility issue
:solution,bug fixed by Copilot
:solution,modify configuration/setting
:solution,use suitable version
:challenge,implementation in practical development
:process,coding process
:concept,new features
:`knowledge type`,contextual and semantic information
:`knowledge type`,appearance knowledge
:task,pedestrian detection
:application,intelligent driving system
:corpus,description corpus
:data,narratives of pedestrian appearances
:`knowledge set`,appearance knowledge sets
:representation,appearance variations
:process,task-prompting process
:`knowledge unit`,appearance knowledge units
:integration,language-driven knowledge units with visual cues
:`information type`,appearance information
:performance,state-of-the-art detection performance
:task,depth estimation
:method,fully-supervised depth estimation
:method,fixed depth bins
:method,few-shot-based method
:task,monocular depth estimation
:technique,learnable prompts
:process,text preprocessing
:dataset,NYU V2
:dataset,KITTI
:metric,mean absolute relative error (MARE)
:performance,depth estimation accuracy
:mechanism,refusal mechanism
:solution,Learn to Refuse (L2R)
:`knowledge base`,structured knowledge base
:state,empty
:method,automatic knowledge base expansion
:approach,L2R
:quality,controllability and reliability
:technology,autonomous driving technology
:concept,transportation revolution
:system,rule-based autonomous driving system
:system,data-driven autonomous driving system
:system,module-based autonomous driving system
:issue,cumulative errors
:system,end-to-end autonomous driving system
:ability,contextual understanding and logical reasoning
:`model family`,foundation vision model
:`research line`,large language models for autonomous driving (LLM4AD)
:state,technological advancements
:resource,awesome-llm4ad GitHub repository
:challenge,domain-specific content
:activity,crawling and refining data
:strategy,multidimensional strategy
:resource,existing datasets in English
:model,fine-tuned LLM for Vietnamese language
:task,generating Vietnamese news articles
:condition,environmental conditions
:activity,human activity
:effort,conservation efforts
:entity,wildlife
:tool,camera traps
:task,wildlife tracking and monitoring
:data,camera trap imagery
:goal,reducing reliance on labelled data
:framework,WildMatch
:task,zero-shot species classification
:task,generating visual descriptions
:technique,knowledge augmentation
:attribute,caption quality
:dataset,camera trap dataset
:location,"Magdalena Medio region, Colombia"
:paradigm,two-stage paradigm
:strategy,fine-tuning with noisy labels
:framework,fine-tuning PLMs with noisy labels framework
:factor,pre-training data scale and quality
:purpose,accelerating LLM research
:dataset,RefinedWeb
:dataset,WanJuan
:issue,lack of non-English corpus
:task,extracting clean texts
:`tool-chain`,EvalWeb
:method,manually crafted rules
:data,noisy texts
:model,evaluation model
:data,clean data
:dataset,ChineseWebText
:data,high-quality pre-training data
:subset,600 GB Chinese data
:scope,worldwide
:technique,knowledge tuple injection
:problem,unaligned knowledge injection
:outcome,comparable or better results
:phenomenon,frustrating finding
:work,related prior work
:interpretation,potential interpretations
:technique,pruning and purification of external knowledge base
:problem,sanity problem
:`model family`,domain-adaptive large language model
:capability,produce semantically-equivalent outputs
:task,question-answering semantic consistency assessment
:dataset,benchmark dataset for QA semantic consistency
:metric,semantic consistency metric
:measurement,LLM QA accuracy
:framework,factual QA reference-less performance prediction
:capability,accurately answer questions
:trend,data and model scale increase
:strategy,model updating
:scenario,new data availability
:scenario,language shift
:model,monolingual English language model
:process,incremental data addition
:language,Norwegian
:language,Icelandic
:effect,forward transfer
:characteristic,positive and language order independent
:effect,backward transfer
:factor,language order and characteristics
:metric,language similarity
:effect,transfer effects
:metric,syntactic similarity
:task,narrative question answering
:`content type`,multimedia content
:genre,narrative video
:framework,Long Story Short
:task,narrative video question answering
:technique,CLIPCHECK
:capability,visual matching
:model,Long Story Short with CLIPCHECK
:benchmark,state-of-the-art supervised models
:`content type`,long videos
:task,expressive text-to-speech
:attribute,human-like speech qualities
:advancement,expressive TTS
:capability,style control through prompts
:challenge,style-annotated data acquisition
:method,expressive TTS training
:limitation,fixed style annotations
:model,FreeStyleTTS
:attribute,minimal human annotations
:task,style retrieval
:task,style reference selection
:approach,innovative approach
:attribute,flexible and precise style control
:experiment,Mandarin storytelling corpus
:`model type`,dual-encoder-based dense retrieval model
:field,Information Retrieval (IR)
:concept,fast-forward indexes
:concept,score interpolation
:technique,lexical and semantic matching
:technique,pre-computing representations
:challenge,memory footprint and maintenance cost
:technique,index size reduction
:technique,dynamically dropping irrelevant document tokens
:task,index maintenance
:attribute,effectiveness and efficiency
:limitation,limited API text corpus
:task,API relation inference
:`knowledge base`,entire web
:`ai module`,API FQN parser
:`ai module`,API relation decider
:approach,AI chain design
:metric,inference reliability
:strategy,AI-crowd-intelligence
:approach,LLM-based API relation inference
:metric,F1 value
:challenge,synchronized partial softmax update
:process,synchronized update
:challenge,under-utilized computation
:operation,flat GEMM
:challenge,performance loss due to static dataflow
:process,static dataflow
:engine,FlashDecoding++
:challenge,LLM inference acceleration
:technique,asynchronized softmax with unified max value
:technique,flat GEMM optimization with double buffering
:technique,heuristic dataflow with hardware resource adaptation
:hardware,NVIDIA and AMD GPUs
:benchmark,state-of-the-art LLM inference engines
:process,digitization of real-world data
:domain,healthcare delivery and biomedical discovery
:framework,TrialScope
:task,distilling real-world evidence
:`model family`,biomedical language model
:technique,probabilistic modeling
:representation,clinical trial specification
:outcome,structuring of real-world data
:performance,excellent performance
:balance,model accuracy and hardware efficiency
:method,AWEQ
:characteristic,no additional training overhead
:`quantization type`,ultra-low-bit and W8A8 quantization
:observation,weight quantization vs activation quantization
:challenge,activation quantization
:technique,channel equalization
:goal,mitigate quantization bias error
:benchmark,existing post-training quantization methods
:capability,factual knowledge interface
:model,LaMa
:model,Atlas
:strategy,up-scaling
:factor,syntactical form
:understanding,factors affecting factual consistency
:issue,knowledge graph incompleteness
:technique,traditional knowledge graph completion
:issue,computational intensity
:`model family`,generative transformer-based language model
:model,KGT5
:method,node neighborhoods inclusion
:dataset,Wikidata subsets
:analysis,impact of neighborhood on model prediction
:factor,neighborhood
:direction,effective neighborhood selection
:challenge,semantic gap
:model,CLLM4Rec
:paradigm,LLM and ID paradigm integration
:strategy,soft+hard prompting
:strategy,mutual regularization
:strategy,recommendation-oriented finetuning
:task,efficient recommendation
:corpus,RS-specific corpora
:data,user-item interactions and features
:`model family`,vision language foundation model
:task,vision language tasks
:task,robotics manipulation
:data,robotics data
:framework,RoboFlamingo
:technique,single-step vision-language comprehension
:feature,open-loop control flexibility
:task,manipulation tasks
:goal,cost-effective robotics manipulation
:entity,human beings
:`model type`,cognitive models of language
:phenomenon,language illusions
:illusion,comparative illusion
:illusion,depth-charge illusion
:illusion,negative polarity item (NPI) illusion
:capability,recognizing nuanced information
:application,virtual assistant
:task,entity-rich query recognition
:process,knowledge integration
:strategy,server-side rescoring
:task,spoken information domain queries
:`model type`,n-gram word language model
:`model type`,sub-word neural language model
:`signal integration`,on-device and server-side signals
:metric,word error rate improvement
:strategy,server-side language model integration
:strategy,model fusion
:`model family`,server-side language model
:task,"code understanding, synthesis, and question-and-answering"
:application,autonomous mobile robots
:approach,REAL
:feature,"prior knowledge, natural-language interpretation, control input adaptation"
:integration,REAL in autonomy stack
:entity,multirotor
:`performance metric`,position tracking errors
:task,avoiding dangerous scenarios
:task,exploring text corpora
:model,LDA
:representation,bags of words
:framework,TopicGPT
:quality,human categorization alignment
:metric,harmonic mean purity
:feature,interpretable topics
:feature,user-specified constraints
:technique,hierarchical topical modeling
:value,human-centered approach
:technique,test-time prompt tuning
:technique,entropy minimization
:technique,feature distribution shift minimization
:domain,test domain
:benchmark,domain generalization benchmark
:metric,zero-shot top-1 accuracy
:method,this work's method
:benchmark,cross-dataset generalization
:resource,source code and models
:url,https://jameelhassan.github.io/promptalign
:technique,intermediate computation
:approach,implicit reasoning
:technique,explicit chain of thought reasoning
:`model component`,internal hidden states
:technique,distillation
:concept,horizontal reasoning
:concept,vertical reasoning
:task,multi-digit multiplication
:dataset,grade school math problem dataset
:metric,speed
:domain,medical and healthcare
:community,healthcare researchers
:issue,hallucinations in healthcare
:application,ScienceWorld
:domain,elementary science experiments
:assumption,Markov assumption
:task,ScienceWorld actions
:model,SwiftSage
:interest,public interest
:entity,corporations
:issue,greenwashing
:regulation,lack of stringent regulations
:methodology,novel methodology
:risk,greenwashing risk
:model,ClimateBERT
:task,quantifying greenwashing risk
:task,exploration for greenwashing risk assessment
:technique,visual instruction tuning
:resource,visual instruction datasets
:question,good visual instructions
:task,complex visual reasoning
:approach,synthesis-complication-reformulation
:resource,complex visual reasoning instructions
:dataset,COMVINT
:task,fine-tuning MLLMs
:url,https://github.com/rucaibox/comvint
:task,code explanation generation
:factor,prompt wording
:factor,code examples
:factor,programming language
:factor,temperature parameter
:factor,LLM version
:metric,Flesch-Kincaid readability level
:metric,completeness
:metric,conciseness
:metric,specificity
:issue,effective deployment and compression
:approach,Divergent Token Metrics (DTMs)
:task,assessing compressed LLMs
:metric,First Divergent Token Metric (FDTM)
:concept,token divergence
:technique,model sparsification
:component,attention components
:metric,standard metrics
:outcome,deteriorated outcomes
:task,zero-shot relevance ranking
:method,listwise and pairwise ranking
:method,pointwise ranking
:method,instruction distillation
:problem,ranking efficiency
:model,MonoT5
:benchmark,state-of-the-art zero-shot methods
:resource,code for instruction distillation
:dataset,TREC
:`document type`,clinical text
:information,clinical information
:method,aggregated ensembles of large language models
:challenge,long input handling
:technique,ensemble learning and text aggregation
:task,clinical outcome prediction
:`data type`,unstructured and high-dimensional datasets
:outcome,analysis and discussion
:system,clinical healthcare systems
:method,Fast Language-Audio Pre-training (FLAP)
:technique,"masking, contrastive learning, and reconstruction"
:technique,audio spectrogram token dropping
:concept,inter-modal contrastive learning
:technique,masking and reconstruction
:dataset,AudioCaps
:metric,53.0% R@1
:dataset,Clotho
:metric,25.5% R@1
:capability,dialogue capabilities
:goal,human-like communication
:need,"communication, affection, and social belonging"
:benchmark,DialogBench
:task,dialogue tasks
:principle,design principles
:quality,human likeness
:technique,data-free knowledge distillation
:`model architecture`,encoder-only structures
:field,generative language modeling
:framework,DFKD-T3
:data,general domain corpus
:`generated text`,texts from DFKD-T3
:task,language model distillation
:resource,DFKD-T3 code repository
:task,recognizing political intents
:context,online newspaper text
:resource,Korean text classification dataset
:dataset,Korean news dataset
:characteristic,large-scale with long text for multi-task classification
:content,"12,000 news articles with political intentions"
:labeling,text samples labeling
:aspect,political orientation and pro-government level
:`model family`,state-of-the-art language model
:resource,Kopolitic benchmark dataset
:url,https://github.com/kdavid2355/kopolitic-benchmark-dataset
:task,specific task
:capability,generalization to OOD datasets
:process,robust fine-tuning
:capability,performance on ID and OOD datasets
:approach,calibrated robust fine-tuning (CAROT)
:capability,calibration and robustness on ID and OOD datasets
:application,high-stakes ML applications
:method,post hoc interpretation
:framework,Proto-LM
:goal,immediate interpretability
:possibility,interpretable models without performance sacrifice
:approach,interpretability in LLMs
:approach,self-consistency checking
:classification,question-level and model-level hallucinations
:method,Semantic-Aware Cross-Check Consistency (SAC^3)
:benchmark,question-answering and open-domain generation benchmarks
:technique,semantically equivalent question perturbation
:technique,cross-model response consistency checking
:platform,online social media
:demographic,people with different backgrounds
:trend,emoji usage
:barrier,cultural or linguistic borders
:study,emoji study
:task,single emoji prediction
:resource,data resources for emoji study
:corpus,Text2Emoji
:model,EmojiLM
:task,text-emoji bidirectional translation
:task,emoji-related downstream tasks
:component,aspects
:entity,specific entity attributes
:`model family`,generative pre-trained language model
:framework,LEGO-ABSA
:technique,multitask learning and prompting
:model,Indo LEGO-ABSA
:language,Bahasa Indonesia
:data,hotel domain dataset
:`evaluation focus`,zero-shot/few-shot capabilities
:task,basic natural language tasks
:`evaluation focus`,instruction translation into tool APIs
:task,tool API translation
:`evaluation gap`,complex tool use in multi-modal environments
:task,multi-turn multi-modal instruction completion
:benchmark,PowerPoint Task Completion (PPTC)
:capability,creating and editing PPT files
:data,multi-turn sessions
:`evaluation system`,PPTX-Match
:capability,finishing instructions
:task,single-turn dialogue testing
:task,completing entire sessions
:`error cause`,error accumulation in multi-turn sessions
:`error cause`,long PPT template processing
:`error cause`,multi-modality perception
:resource,"PPTC data, code, and evaluation system"
:task,linguistic steganalysis
:goal,detecting stegos
:method,existing linguistic steganalysis methods
:factor,user characteristics
:challenge,limited occurrence of stegos
:framework,UP4LS
:component,user profile
:attribute,user attributes
:module,identified feature extraction module
:technique,deep-learning networks
:output,high-dimensional user features
:feature,content features
:process,feature integration
:output,optimized feature representation
:phase,training phase
:aspect,stego distribution
:condition,fewer stego samples
:field,linguistic steganalysis research
:domain,Traditional Chinese Medicine (TCM)
:approach,TCMDA (TCM Domain Adaptation)
:challenge,domain-specific application
:corpus,TCM-Corpus-1B
:model,TCM-GPT-7B
:task,TCM tasks
:milestone,pioneering validation in TCM domain
:goal,interdisciplinary development
:goal,foundation for further study
:challenge,deployment challenges
:technique,low-bit weight quantization
:resource,memory
:condition,small group sizes or sub-4 bits
:issue,asymmetry absence
:distribution,asymmetric value distribution
:technique,asymmetric floating-point quantization
:technique,quantization methods
:technique,asymmetric integer quantization
:system,scientific workflow system
:task,data analysis pipeline execution
:value,"reproducibility, dependability, scalability"
:challenge,workflow implementation difficulty
:community,workflow user community
:resource,user-supporting tools and examples
:task,workflow user support
:task,workflow interpretation
:task,workflow modification
:`cognitive function`,concept formation
:activity,human cognitive functions
:unit,tokens
:feature,concept-awareness
:method,pretraining using concepts
:approach,using output of existing LLMs
:`proof-of-concept`,concept-aware LLMs
:`cognitive function`,human intuition
:feature,prediction robustness
:`model type`,neural text detector
:strategy,text alteration
:strategy,parameter tweaking
:task,text evasion
:strategy,character-level mutations
:outcome,misclassification
:platform,online forums
:activity,discussion of stances
:issue,difficulty in overviewing long discussions
:approach,unsupervised approach using large language models
:task,generating indicative summaries
:tool,table of contents
:technique,two-level summary generation
:task,generative cluster labeling and frame classification
:`user study`,purpose-driven user study
:tool,indicative summaries
:interface,Discussion Explorer
:activity,exploring long discussions
:audience,laypeople
:system,LLaMA 2-based system
:event,PLABA shared task
:issue,weak training signals
:phenomenon,shared tokens between input and output
:approach,sentence-level loss weights
:approach,token-level loss weights
:evaluation,empirical evaluation on PLABA dataset
:dataset,PLABA dataset
:approach,sentence-level and token-level loss weights
:metric,Flesch-Kincaid Grade Level (FKGL)
:metric,edit distance
:hyperparameter,lambda in token-level loss weights
:metric,edit distance and FKGL
:task,target volume contouring
:task,normal organ segmentation
:`information type`,image and text-based clinical information
:task,integration of textural information and images
:model,LLM-driven multi-modal AI
:`information type`,clinical text information
:context,breast cancer radiation therapy target volume contouring
:environment,external validation and data-insufficient environments
:`model type`,vision-only AI models
:attribute,robust generalization performance and data-efficiency
:milestone,first LLM-driven multimodal AI in radiation oncology
:capability,human-level language comprehension and reasoning
:`model family`,general-purpose large language model
:`model family`,specialized large language model
:capability,processing diverse data types
:development,LLM-powered autonomous agents for healthcare
:methodology,evaluation methodologies for LLMs
:goal,reliability and safety in medicine
:analysis,transformative potential of LLMs in medicine
:need,continuous optimizations and ethical oversight
:repository,awesome-llm-healthcare
:resource,latest papers on LLMs in healthcare
:task,chart creation
:challenge,capturing user intents
:input,natural language input
:attribute,abstractness
:system,ChartGPT
:task,chart creation from natural language
:problem,complex logic problems
:approach,step-by-step reasoning pipeline
:task,chart generation
:dataset,abstract utterances and charts dataset
:interface,interactive interface for ChartGPT
:action,modify intermediate outputs
:method,quantitative evaluations and user study
:task,modeling long sequences
:`model family`,existing sequence models
:model,GateLoop
:`model family`,linear recurrent models
:model,S4
:task,auto-regressive language modeling
:concept,data-controlled relative-positional information
:technique,context aggregation
:technique,data-controlled complex cumulative products
:goal,powerful sequence models
:technique,creative problem-solving
:technique,design thinking
:tool,software tools for idea recording
:outcome,new ideas
:system,Supermind Ideator
:domain,general problems
:concept,superminds
:experience,early experiences with Supermind Ideator
:limitation,neglect of query-related information
:paradigm,hint-enhanced in-context learning
:component,hint-related example retriever
:model,LLaMA-2-Chat-7B
:metric,EM score
:institution,University of Groningen
:strategy,simple-then-complex learning
:aspect,context size
:model,context-limited model
:benchmark,(Super)GLUE
:benchmark,MSGS
:benchmark,BLiMP
:attribute,model capacity
:approach,evaluation benchmark construction
:task,measuring LLM ability level
:concern,appropriate benchmark use and fair model comparison
:risk,inappropriate benchmark use
:issue,benchmark leakage
:process,pre-training data preparation
:experiment,study on benchmark leakage effect
:outcome,boosted evaluation results
:issue,unreliable model performance assessment
:guideline,for LLM developers and benchmark maintainers
:task,LLM training and evaluation
:`model family`,RNN-like language model
:issue,forgetfulness
:function,prompted generation
:architecture,architecture to teach model memorizing prompt
:technique,synthetic gradient
:technique,low-rank gradient approximation
:dataset,constructed dataset for experiments
:technique,learning from preferential feedback
:`agent type`,interactive learning agent
:gap,theory and application of LFPF algorithms
:assumption,preferences and transition dynamics by MDP
:framework,direct preference process
:concept,optimal policies
:theorem,von Neumann-Morgenstern expected utility
:community,future practitioners
:person,Alan Turing
:concept,evaluation periods
:behavior,human-like behaviors
:need,unified evaluation system
:`evaluation methodology`,common evaluation methodologies
:shift,qualitative shift in assessment approaches
:value,standardization and objective criteria
:value,"reliability, fairness, and societal benefit"
:method,training-free recommendation
:knowledge,pretrained knowledge
:issue,slow inference in LLMs
:framework,LLaMaRec
:application,ranking-based recommendation
:`model family`,sequential recommender
:approach,verbalizer-based
:output,probability distributions
:issue,long text generation
:metrics,recommendation performance and efficiency
:concept,dynamic shape computation
:`compiler abstraction`,Relax
:task,optimizing dynamic ML workloads
:feature,symbolic shape annotations
:abstraction,cross-level abstraction
:concept,computational graph
:framework,end-to-end compilation framework
:system,hand-optimized systems
:environment,diverse backend environments
:role,general-purpose AI assistants
:behavior,undesired behaviors
:role,harmful assistants
:data,fine-tuning data
:framework,controllable training framework
:goal,preventing learning of harmful behaviors
:concept,security vectors
:experiment,security vectors effectiveness
:community,assistant community
:challenge,complexity of mixed modalities
:task,handling additional modalities
:architecture,PILL
:module,mixture-of-modality-adapter-expert
:module,modality-attention-gating
:task,modality fusion
:resource,code and models for PILL
:challenge,alignment problem
:strategy,alignment strategies
:assessment,insufficient
:behavior,unsafe behavior
:capability,learning in context
:characterization,intrinsically difficult
:assessment,safety assessment
:concept,AI system safety
:concept,contextual integrity
:`document type`,privacy policy texts
:concept,governing knowledge commons
:task,GKC-CI annotation
:method,manual or crowdsourced effort
:`document type`,privacy policies
:resource,annotated policies
:application,medical artificial intelligence systems
:data,electronic health record data
:process,conversion of unstructured clinical text
:field,healthcare tasks
:field,radiation oncology research
:issue,errors in LLMs
:process,rigorous evaluation and validation
:framework,comprehensive assessment framework
:field,clinical radiation oncology
:audience,researchers and clinicians
:concept,numbers
:domain,real-world domains
:task,language model tasks
:benchmark,numerical benchmarks
:aspect,specific numerical aspects
:taxonomy,hierarchical taxonomy for numerical reasoning skills
:evaluation,comprehensive evaluation of state-of-the-art models
:challenge,reasoning challenges
:tool,numerical probes
:approach,semi-automated approach
:task,tabular natural language inference (TNLI)
:result,performance shifts
:technique,label-flipping probes
:issue,dataset artifacts exploitation
:modality,speech modality
:model,COSMIC
:task,speech comprehension test question-answer generation
:resource,450 hours English speech data
:capability,instruction-following and in-context learning
:task,EN to X speech-to-text translation
:recipe,low cost recipe for building a speech LLM
:data,new instruction-tuning data
:`writing technique`,textual emphasis
:task,attention steering
:method,PASTA
:resource,PASTA code
:URL,https://github.com/qingruzhang/pasta
:technique,data-efficient pretraining
:competition,BabyLM Challenge
:`model architecture`,modified Transformer
:feature,layer output selection
:concept,layer importance
:`algorithm family`,contextual bandit
:concept,complex contexts
:integration,LLM with contextual bandit framework
:result,preliminary results on synthetic datasets
:metric,cumulative rewards and regret
:concept,contextually-aware decision systems
:task,medical text summarization
:value,healthcare safety and efficiency
:attribute,unfaithful outputs
:framework,FameSumm
:concept,medical terms and contexts
:dataset,multilingual medical summarization datasets
:`model family`,mainstream language model
:evaluation,human evaluation by doctors
:attribute,faithful outputs
:resource,FameSumm code
:URL,https://github.com/psunlpgroup/famesumm
:factor,writing style matching
:`performance gap`,zero-shot vs few-shot translation
:approach,enhancing zero-shot baselines
:metric,translation metrics
:model,Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) Foundation Model
:`model architecture`,encoder-decoder Transformer
:technique,sub-quadratic attention
:`model family`,genomic models
:task,genomic sequence identification
:task,sequence error recognition
:task,virus mutation generation
:task,unsupervised rationale extraction
:process,extracting text snippets
:framework,Rationalizing Neural Prediction (RNP)
:paradigm,generate-then-predict
:assumption,rationale sufficiency
:problem,interlocking problem
:framework,You Only Forward Once (YOFO)
:strategy,token removal
:dataset,BeerAdvocate
:dataset,hotel review
:metric,token-level F1
:issue,sample efficiency and reward specification
:solution,learning from expert guidance
:issue,obtaining human expert
:cost,high cost of supervision
:task,developing automatic supervisor
:feedback,human-like feedback
:task,low-level robotic motion control
:framework,LaFiTE-RL
:agent,LaFiTE-RL agent
:task,debugging and repair
:concept,code correctness
:concept,formal software verification
:task,loop invariant generation
:software,Frama-C
:task,loop invariant verification
:software,CPAChecker
:evaluation,ChatGPT generated invariants
:outcome,valid and useful invariants
:proposal,combining LLMs and software verifiers
:task,linguistic exams
:course,introduction to linguistics
:task,interpretation of complex tasks
:task,phonetic transcription
:task,analysis of morphemes and phrases
:issue,one-to-one correspondence in language tasks
:task,analysis or generation of syntax trees
:technique,preprocessing
:task,solving tasks with visualizations
:task,low-level vision tasks
:`model family`,grounding-based vision and language model
:factor,training dataset scale
:field,vision and language tasks
:model,robust phrase grounding model
:technique,text-conditioned color jittering
:technique,horizontal flipping
:technique,pixel-level masking
:framework,MDETR
:model,image encoder
:dataset,large-scale image and language datasets
:dataset,Referring Expressions
:method,data augmentation method
:URL,https://github.com/amzn/augment-the-pairs-wacv2024
:capability,writing coherent text
:task,impact reporting
:limitation,knowledge constraint
:tool,FloodBrain
:task,flood disaster impact reporting
:pipeline,information assimilation
:component,pipeline components
:goal,disaster impact reporting advancement
:goal,reducing coordination time
:event,flood disasters
:dataset,code-generation prompts dataset
:task,code generation evaluation
:artifact,code samples
:methodology,manual assessment
:metric,code quality metrics
:task,data analysis tasks
:task,visual-graphical challenges
:comparison,ChatGPT vs human code
:quality,modular design and error handling
:entity,ChatGPT code
:study,ChatGPT code generation capabilities study
:field,AI-based programming assistants
:resource,dataset and methodology
:goal,future research foundation
:task,cross-domain constituency parsing
:technique,traditional self-training
:resource,raw corpora
:technique,enhanced self-training
:technique,grammar rules
:technique,pseudo-data selection
:technique,self-training with LLM
:combination,grammar rules and confidence criteria
:issue,negative conflicts and interference
:framework,MNAME
:technique,Lora
:decoder,Lora-MoE
:result,20% improvement
:task,2D and 3D downstream tasks
:task,interacting with visual content
:tool,standardized and holistic evaluation framework
:framework,Comprehensive Evaluation Framework (CHEF)
:component,modular components
:process,versatile evaluations
:concept,recipes
:capability,desired capabilities
:evaluation,large-scale evaluation
:observation,generalizability and composite capability of MLLMs
:community,MLLM community
:experience,restaurant dinner
:aspect,atypical aspects
:task,making recommendations
:document,customer reviews
:task,detecting atypical aspects in customer reviews
:domain,restaurants
:domain,hotels
:domain,hair salons
:`user behavior`,query reformulation
:issue,irrelevant search results
:`search engine feature`,interactive query suggestions
:model,existing query suggestion models
:issue,unnatural user interactions
:model,CIRCLE
:task,multi-turn query clarifications
:technique,user simulation framework
:outcome,diverse query clarifications
:benchmark,Google suggestions
:ability,pattern recognition
:ability,trustable and controlled reasoning
:system,logic-based rule system
:feature,controlled inspection and verification
:task,manual creation
:task,language to logic translation
:tool,reasoner
:dataset,DKET
:dataset,atomic knowledge bank
:task,question-answering (QA)
:cost,human cost
:solution,scalable and intelligent QA
:technique,retrieval augmented generation (RAG)
:dataset,Piazza dataset
:improvement,30% improvement in QA quality
:architecture,novel architecture for educational QA
:assistant,Chata
:challenge,deformable object manipulation
:technique,learning latent dynamics
:method,demonstrations
:representation,particles
:limitation,acquiring suitable demonstrations
:approach,demonstration-free hierarchical planning
:task,articulating high-level plans
:output,subgoal point clouds
:strategy,closed-loop model predictive control
:technique,differentiable physics
:loss,DiffPhysics-P2P
:metric,Earth Mover Distance (EMD) space
:technique,hierarchical planning approach
:task,dough manipulation
:validation,experimental trials
:concept,paper significance evaluation
:community,scientific community
:metric,citation count
:goal,accurate impact reflection
:method,TextMatch
:metric,CausalCite
:goal,paper impact measurement
:resource,code and data for CausalCite
:task,cognitive tasks
:problem,misinformation and toxicity
:improvement,toxicity and truthfulness
:phenomenon,self-doubt
:challenge,addressing self-doubt
:task,rationale generation
:phenomenon,self-rationalization
:aspect,rationale semantics
:quality,"faithfulness, truth, helpfulness"
:algorithm,MARIO
:technique,multi-reward conditioned self-rationalization
:outcome,improved task accuracy and self-rationalization quality
:quality,plausibility and consistency
:dataset,"StrategyQA, QuaRel, OpenBookQA, Numersense, QASC"
:task,articulated object manipulation
:application,home-assistant robots
:approach,imitation learning from demonstrations
:approach,reinforcement learning in simulation
:challenge,real-world data collection
:focus,high-level task planning
:concept,kinematic structure
:task,object manipulation
:framework,kinematic-aware prompting framework
:tool,unified kinematic knowledge parser
:model,kinematic-aware planner model
:evaluation,framework evaluation
:`model family`,larger model
:framework,co-training and co-distillation (CTCD)
:goal,performance and inference speed
:technique,existing techniques
:technique,one-way knowledge distillation
:`model family`,smaller model distilled by CTCD
:metric,performance margin
:`supervision level`,extremely weak supervision
:`supervision level`,one-shot supervision
:method,X-NER
:technique,mining entity spans
:technique,context distribution comparison
:technique,entity span representations
:technique,pseudo-labeling
:benchmark,few-shot NER methods
:capability,cross-lingual abilities
:experiment,extensive experiments on NER datasets
:process,generating SQL queries
:method,SQLPrompt
:technique,execution-based consistency decoding
:outcome,consistent SQL execution
:method,MixPrompt
:method,MixLLMs
:benchmark,finetuning state-of-the-art
:challenge,generating knowledge base query code
:competition,CCKS2023
:task,question answering with knowledge graph inference for unmanned systems
:framework,ChatGPT-based Cypher Query Language generation
:task,syntax prediction
:component,proper noun matcher
:task,proper noun extraction
:component,demonstration example selector
:task,example retrieval
:component,prompt constructor
:task,input template design
:model,ChatGPT-based generation model
:output,Cypher Query Language (CQL)
:goal,final answer generation
:achievement,second place in CCKS 2023 competition
:`knowledge type`,structural knowledge
:`knowledge components`,"entities, relations, events"
:`data types`,codes and texts
:framework,Code4UIE
:`programming concept`,Python classes
:technique,example retrieval strategies
:evaluation,IE tasks across nine datasets
:framework,mathematical framework for LLMs
:framework,HEX
:concept,hallucinations in LLMs
:concept,alignment in LLMs
:concept,self-verification in LLMs
:goal,new research avenues
:value,safe and reliable generative AI
:challenge,literature curation
:field,life sciences
:community,biomedical knowledgebase developers
:challenge,scaling curation efforts
:task,literature summarization
:output,summaries
:evaluation,automated evaluation approaches
:resource,RNAcentral
:code,summary generation code
:url,https://github.com/rnacentral/litscan-summarization
:dataset,contexts and summaries dataset
:url,https://huggingface.co/datasets/rnacentral/litsumm-v1
:paradigm,generative retrieval
:process,index structure construction
:challenge,knowledge-identifier discrepancy
:challenge,training-inference gap
:method,Generative Retrieval via Lexical Index Learning (GLEN)
:strategy,two-phase index learning
:technique,collision-free inference
:performance,state-of-the-art or competitive performance
:dataset,NQ320k
:resource,GLEN code
:URL,https://github.com/skleee/glen
:source,app reviews
:goal,software requirements improvement
:task,user review mining
:process,information extraction and summarization
:task,manual analysis of user reviews
:characteristic,arduous
:task,automatic user review mining
:limitation,manually crafted dataset requirement
:approach,NLP-based user review mining
:tool,Mini-BAR
:task,user review processing
:dataset,annotated user reviews dataset
:result,preliminary results of Mini-BAR
:field,requirement engineering
:`replication package`,Mini-BAR GitHub repository
:model,COG-VLM
:concept,visual language foundation model
:method,shallow alignment
:component,visual expert module
:capability,deep fusion of vision language features
:model,COG-VLM-17B
:benchmark,cross-modal benchmarks
:`benchmark list`,"NoCaps, Flicker30k Captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA, TDIUC"
:`benchmark list`,"VQAv2, OK-VQA, TextVQA, COCO Captioning"
:model,PALI-X 55B
:resource,COG-VLM codes and checkpoints
:URL,https://github.com/thudm/cogvlm
:issue,potential abuse
:approach,automated text detection
:approach,condensed ensembling
:task,generative text classification
:`model family`,open language model
:model,Mosaic Pretrained Transformers
:dataset,English essays
:capability,new capabilities
:concept,delta parameters
:operation,DARE (Drop and Rescale)
:concept,discarded parameters
:technique,parameter averaging
:model,WizardLM
:model,Code Alpaca
:URL,https://github.com/yule-buaa/mergelm
:system,database management system
:feature,configurable knobs
:problem,knob tuning
:community,database community
:task,manual tuning
:characteristic,impractical
:system,automatic tuning system
:approach,existing tuning approaches
:issue,significant tuning costs or sub-optimal performance
:system,GPTuner
:solution,database tuning system
:algorithm,prompt ensemble algorithm
:strategy,workload-aware and training-free knob selection
:framework,coarse-to-fine Bayesian optimization
:benchmark,"TPC-C, TPC-H"
:metric,configuration time
:goal,mitigating gender bias
:technique,dictionary-based counterfactual data augmentation
:method,word substitution
:issue,limitations of dictionary-based CDA
:solution,model-based solutions
:issue,lack of qualitative parallel training data
:proposal,combination of data processing techniques and bi-objective training regime
:model,proposed model-based solution
:study,jailbreak studies
:method,brute-force optimization
:method,deepinception
:experiment,Milgram experiment
:risk,misuse of LLM
:ability,personification
:concept,direct jailbreaks
:concept,jailbreak success rate
:model,GPT-4v
:goal,safety of LLM
:resource,DeepInception code
:URL,https://github.com/tmlr-group/deepinception
:institution,University of Lyon 2
:task,small-scale language modelling
:dataset,strict-small track dataset
:metric,10M words
:method,architecture search
:task,minimizing masked language modelling loss
:model,Bebeshka
:`model configuration`,4-layer encoder with 8 attention heads
:model,Zlata
:`model configuration`,6-layer decoder with 12 attention heads
:`model family`,small-size language model
:task,moral judgment
:`model family`,compact language model
:approach,finetuning pretrained language model
:performance,PLM performance
:attribute,smaller number of parameters
:framework,Self-Supervised Cross-View Training (SCT)
:problem,performance gap in PLMs
:benchmark,Semantic Textual Similarity (STS) benchmarks
:result,SCT performance
:`model family`,pretrained language model with less than 100M parameters
:task,entity linking
:requirement,precise entity predictions
:approach,Instructed Generative Entity Linker
:technique,sequence-to-sequence training with instruction-tuning
:framework,novel generative EL framework
:component,light-weight potential mention retriever
:metric,linking metrics
:framework,in-context learning for EL
:paradigm,pretrain-then-finetune
:process,deployment of large language models
:collection,LoRA adapters
:process,batched inference
:system,S-LoRA
:process,scalable serving
:technique,unified paging
:resource,GPU memory
:library,state-of-the-art libraries
:library,HuggingFace PEFT
:library,VLLM
:service,customized fine-tuning services
:capability,visual and textual information processing
:issue,hallucination behavior
:benchmark,BINGO
:issue,regional bias
:issue,leading questions and multiple images
:issue,bias and interference vulnerabilities
:issue,high cost of training
:strategy,enlarging model sizes
:model,Ziya2
:technique,data-centric optimization
:application,AI writing support
:study,bias in models and data representations
:issue,bias transfer to human writing
:study,bias transfer through AI writing support pipeline
:process,bias transfer
:`user study`,large-scale user study with students
:analysis,GenBIT gender bias analysis
:test,Sentence Embedding Association Test (SEAT)
:result,no significant difference in gender bias
:comparison,peer reviews with and without LLM suggestions
:research,use of AI writing support in the classroom
:resource,high-quality annotated demonstrations
:method,DAIL
:limitation,lack of high-quality annotated demonstrations
:outcome,final result
:technique,standard in-context learning
:technique,voting consistency
:phenomenon,concept shift
:technique,retraining classifiers
:challenge,labelling costs
:activity,fine-tuning large DL models
:method,reformulation into entailment-style problem
:data,real-world and synthetic datasets
:benefit,labeling cost savings
:technique,persona modulation
:technique,language model assistant
:goal,comprehensive safeguards
:problem,evaluation metrics problem
:task,open-vocabulary segmentation
:`metric type`,closed-set metrics
:concept,category similarity
:method,similarity measurements survey
:metric,open mIoU
:metric,open AP
:metric,open PQ
:benchmark,proposed evaluation metrics benchmark
:metric,proposed evaluation metrics
:capability,open ability
:resource,evaluation code
:domain,vision
:`model family`,region-level large multimodal model
:task,visually grounded responses
:model,Grounding Large Multimodal Model (GLaMM)
:capability,natural language responses with object segmentation masks
:`input type`,textual and visual prompts
:task,grounded conversation generation
:requirement,densely grounded concepts
:dataset,Grounding-Anything Dataset (GRaND)
:data,unique concepts and regions with segmentation masks
:project,GroundingLMM
:concept,code comment
:classification,useful vs not useful
:data,code-comment pairs
:dataset,initial dataset
:goal,classification model
:technique,machine learning algorithms
:era,code generation advancements
:challenge,selecting best solutions
:method,previous solution ranking methods
:concept,inter-cluster relationship
:strategy,SRank
:task,code solution selection
:concept,functional overlap between clusters
:metric,pass@1 score
:model,Codex002
:method,random sampling
:benchmark,code generation reranking
:track,information retrieval in software engineering
:task,automated evaluation of code comments
:category,code comments usefulness
:dataset,code comments and code snippets
:source,GitHub C-based projects
:dataset,LLM-generated code comments
:event,experiments submission
:group,universities and software companies
:label,LLM-generated labels
:issue,bias in prediction model
:outcome,less over-fitting
:challenge,clinical variant classification
:field,clinical genetics
:task,variant effect prediction
:characteristic,disease-specific
:model,ProPath
:model,ESM1b
:value,clinical applicability
:methodology,high-level synthesis
:tool,large language model tools
:attribute,open-source tools
:`case study`,permuted congruential random number generator design
:design,random number generator design
:`test suite`,Dieharder randomness test suite
:documentation,"LLM chat logs, Python scripts, Verilog scripts, simulation results"
:methodology,hardware design generation
:tool,open source silicon 130 nm design tools
:application,domain-specific computing accelerators
:domain,Internet of Things
:application,proof of concept prototypes
:process,modern process nodes
:process,learning from in-context exemplars
:factor,exemplars
:problem,exemplar selection
:perspective,contextual retrieval from associative memory
:theory,Hopfield networks
:process,active exemplar selection
:concept,memory retrieval
:field,large language model understanding
:application,high-stakes NLP applications
:work,prompting works
:concept,proxy explanations
:concept,uncertainty in explanations
:metric,verbalized uncertainty
:metric,probing uncertainty
:attribute,explanation confidence
:attribute,faithfulness of an explanation
:discussion,broader discussion of trustworthiness
:attribute,trustworthiness of foundation models
:issue,lack of contextual information
:dataset,text-based emotion classification datasets
:issue,misaligned labels
:performance,machine learning model performance
:task,dataset re-annotation
:technique,contextual information enhancement
:concept,textual context
:outcome,label alignment improvement
:technology,intracortical brain-computer interface
:application,restoring communication
:disorder,amyotrophic lateral sclerosis
:task,recalibration
:method,self-recalibration
:framework,Continual Online Recalibration with Pseudo-labels (CORP)
:task,online handwriting IBCI task
:achievement,longest-running IBCI stability demonstration
:goal,clinical translation
:platform,online learning platforms
:trend,advancement in education technology
:research,existing research on chatbots in education
:aspect,behavior-related aspects
:model,Partial Least Squares Structural Equation Modeling (PLS-SEM)
:phenomenon,chatbots adoption in education
:model,Technology Readiness Index (TRI)
:model,Technology Acceptance Model (TAM)
:software,R-Studio
:concept,optimism
:concept,Perceived Ease of Use (PEOU)
:concept,innovativeness
:concept,Perceived Usefulness (PU)
:concept,discomfort
:concept,insecurity
:audience,technology designers
:artifact,open-source frameworks and techniques
:process,training and deploying large language models
:resource,computing resources and memory
:approach,efficient approaches for LLMs
:goal,improving system pipelines and operators
:performance,runtime performance of LLMs
:artifact,hardware and software stacks
:activity,benchmarking performance of LLMs
:perspective,macro and micro perspectives
:performance,end-to-end performance of LLMs
:process,"pre-training, fine-tuning, and serving"
:analysis,runtime analysis of LLM sub-modules
:component,sub-modules of LLMs
:audience,end users
:artifact,benchmark and findings
:analysis,in-depth module-wise analyses
:field,art and cinema
:issue,inadequate text prompts
:role,art director
:system,LADI
:technology,text-to-image and text-to-video generation
:technique,constrained decoding
:technique,intelligent prompting
:company,Plai Labs
:`system type`,open-domain generative system
:application,generative search engine
:mechanism,attribution mechanism
:quality,factuality and verifiability
:issue,ambiguous knowledge reservoirs
:issue,inherent biases
:issue,drawbacks of excessive attribution
:survey,attribution mechanisms in open-domain generative systems
:quality,reliability and veracity of responses
:repository,awesome-llm-attributions
:field,LLM attributions
:task,multi-hop question-answering
:issue,"inaccurate reasoning, hallucinations, lack of interpretability"
:output,structured information
:framework,semantic structure construction and leveraging
:outcome,improved QA performance and faithful reasoning chains
:output,extracted structures
:attribute,grounded explanations
:comparison,extracted structures vs generated reasoning chains and saliency-based explanations
:attribute,human preference
:challenge,high proof burden
:task,code analysis and synthesis
:combination,LLMs and static analysis
:framework,Verus
:ability,logical ability
:ability,context retention and propagation
:prototype,based on GPT-4
:process,automation loop
:result,reduction in human effort
:approach,generalized sequence-to-sequence format
:knowledge,large language model knowledge
:format,specialized augmented format
:strategy,Fish-Dip
:knowledge,pretrained language model knowledge
:concept,dynamic sparse finetuning
:approach,dynamic sparsity
:concept,well-learned samples
:concept,low resource settings
:task,autoformalization
:process,translation to formal language
:challenge,lack of informal-formal pairs dataset
:method,manual curation of small corpora
:method,few-shot learning with large language models
:problem,data scarcity and formal language acquisition difficulty
:dataset,mma
:process,reverse translation
:benchmark,minif2f
:performance,autoformalization model capability
:benchmark,proofnet
:finding,multilingual formal data fine-tuning
:process,semantic matching
:domain,e-commerce search
:issue,few-recall
:framework,BeQUE
:stage,Multi-Instruction Supervised Fine Tuning (SFT)
:stage,Offline Feedback
:stage,Objective Alignment
:technique,rejection sampling and auxiliary tasks mixing
:platform,Taobao
:system,offline system
:goal,model-objective alignment
:metric,gross merchandise volume (GMV)
:metric,number of transactions (#Trans)
:metric,unique visitor (UV)
:task,personalized recommendation
:information,task-related information
:issue,expressing community behavior pattern
:capability,common knowledge
:method,information sharing module
:gap,information gap
:method,collaborative training
:information,user behavior pattern
:disciplines,artificial intelligence and robotics
:agent,real-world agent
:concept,scene knowledge
:`knowledge base`,knowledge base for embodied tasks
:task,embodied tasks
:`knowledge base`,general knowledge base
:characteristics,sparse and insufficient capacity
:challenges,uncertainty of knowledge and maintenance
:method,scene-driven multimodal knowledge graph construction
:framework,unified scene knowledge injection
:`knowledge graph`,ManipMob-MMKG
:characteristics,data-collection efficiency and knowledge quality
:method,knowledge-enhanced methods
:URL,https://sites.google.com/view/manipmob-mmkg
:repository,multilingual factual knowledge
:`performance gap`,factual knowledge probing
:`language resource classification`,high-resource and low-resource languages
:process,implicit factual knowledge transfer
:`feasibility study`,explicit factual knowledge transfer
:process,factual knowledge transfer
:module,Language Representation Projection (LRP2)
:dataset,MLAMA
:process,knowledge transferability
:`model family`,large conversation language model
:value,economic significance
:data,popular language corpora
:issue,underrepresentation of low-resource languages
:resource,Galician adaptation of the ALPaCA dataset
:dataset,Galician ALPaCA dataset
:data,"52,000 instructions and demonstrations"
:task,generating coherent text
:platform,HuggingFace Hub
:model,Cabuxa-7B
:`source code`,experiment replication
:goal,advancements for underrepresented languages
:challenge,knowledge graph matching
:process,matching pipeline
:task,candidate generation
:prototype,ontology matching prototype
:initiative,Ontology Alignment Evaluation Initiative (OAEI)
:result,matching results
:system,supervised matching systems
:feature,effective memory
:feature,human-like memory properties
:data,training textual data
:feature,biological features of human memory
:activity,structuring textual narratives
:method,morpheme-aware subword tokenization
:technique,sub-character decomposition
:feature,rich morphology and unique writing system
:method,byte pair encoding
:value,linguistic accuracy and computational efficiency
:evaluation,NIKL-CoLA
:information,morpheme type information
:capability,syntactic and semantic capabilities
:insight,linguistic insights
:capability,representation capacity
:method,contrastive training
:data,corpus of images and text captions
:task,compositional reasoning tasks
:task,mining negative examples
:method,negative sample generation
:method,generative approaches
:task,generating hard negative texts
:task,generating negative image samples
:framework,proposed framework for enhancing VLM
:capability,multimodal compositional reasoning
:resource,code and dataset for enhancing VLM
:URL,https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html
:access,unrestricted model access
:risk,privacy risks of data leakage
:`model family`,privacy-preserving language model
:benchmark,P-Bench
:goal,quantifying privacy leakage
:aspect,inference data privacy
:goal,privacy objectives
:process,private fine-tuning
:task,privacy attacks
:method,reinforcement-learning fine-tuning
:`feedback type`,human or AI feedback
:concept,simpler features
:concept,robustness and generalisation
:concept,inductive biases
:hypothesis,extractable features utilization
:hypothesis,evidence for/against feature utilization
:`task type`,synthetic and natural language tasks
:finding,statistically significant correlations
:field,explainable artificial intelligence
:characteristic,opacity in machine learning models
:limitation,XAI methods complexity
:characteristic,technical orientation
:framework,XpertAI
:output,natural language explanations
:task,subjective labeling
:phenomenon,human response bias
:task,survey design
:`model family`,instruction fine-tuned large language model
:implication,annotation pipeline substitution
:resource,"code, dataset, and samples"
:URL,https://github.com/lindiatjuatja/biasmonkey
:data,large corpus of source code
:data,supervised vulnerability dataset
:concept,bug semantics
:method,interpretability tools
:method,attention analysis
:method,interaction matrix analysis
:`feature set`,influential feature set
:concept,potentially vulnerable statements (PVS)
:concept,buggy paths
:data,vulnerability datasets
:URL,https://figshare.com/s/4a16a528d6874aad51a0
:issue,harmful or inappropriate responses
:dataset,ATTAQ
:approach,automatic identification of vulnerable semantic regions
:technique,specialized clustering
:goal,safety mechanisms and reliability
:capability,linguistic task performance
:research,reverse engineering of Transformer models
:representation,circuits
:task,sequence continuation tasks
:`sequence type`,increasing sequences
:`sub-circuit`,sequence member detection
:function,detecting sequence members
:`sub-circuit`,next member prediction
:function,predicting next sequence member
:`sequence type`,semantically related sequences
:structure,shared circuit subgraphs
:structure,shared computational structures
:outcome,model behavior prediction and safety
:understanding,mechanistic understanding of Transformers
:goal,robust and interpretable language models
:method,further training
:resource,GPU compute
:model,GPTs
:method,black-box prompt optimization (BPO)
:resource,code and datasets for BPO
:URL,https://github.com/thu-coai/bpo
:phenomenon,growth of NLP models
:benchmark,saturating benchmark datasets
:concept,model performance in the wild
:framework,novel framework for NLP model comparison
:task,NLP model comparison
:technique,input perturbations
:concept,model invariances
:model,InstructGPT family
:phenomenon,success of large language models
:attribute,human-level intelligence tasks
:process,learning over time
:approach,fine-tuning based on problem-solving performance
:approach,building bigger and more sophisticated models
:drawback,substantial data and computational resources
:approach,fine-tuning and building bigger models
:task,problem-solving performance
:system,ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation)
:goal,cost-effective learning
:task,grade-school math problems
:concept,lexical function
:theory,Meaning-Text Theory
:task,hierarchical classification of lexical functions
:structure,tree-like hierarchy
:resource,labeled data
:dataset,Spanish verb-noun collocations dataset
:classification,37 lexical functions
:relationship,noun-verb relation
:structure,tree-based structure
:method,dependency tree parsing
:resource,baselines and data splits
:phenomenon,misunderstandings in communication
:interaction,human-LLM communication
:method,Rephrase and Respond (RAR)
:outcome,LLM performance improvement
:method,two-step Rephrase and Respond
:resource,data and code for RAR
:evaluation,fair evaluation of LLM capabilities
:concept,real-world responsibilities
:goal,behavior specification and constraint
:rule,explicit rules for models
:community,model developers
:framework,RULES
:goal,measuring rule-following ability
:method,text scenarios
:`model family`,open model
:technique,gradient-based attacks
:goal,defense against attacks
:concept,thought design
:attribute,"performance, efficiency, and flexibility"
:concept,existing thought
:approach,Everything of Thoughts (XoT)
:technique,reinforcement learning and Monte Carlo Tree Search (MCTS)
:framework,MCTS-LLM collaborative thought revision
:task,multi-solution problem-solving tasks
:capability,multiple solutions with one LLM call
:capability,instruction abilities
:capability,multi-modal capabilities
:model,MPlug-OWL2
:design,modularized network design
:component,shared functional modules
:component,modality-adaptive module
:phenomenon,modality collaboration
:trend,multi-modal foundation models
:task,text and multi-modal tasks
:integration,large language model integration
:discourse,public discourse on artificial intelligence
:concept,mathematical foundations of large language models
:course,ETH Zürich course on large language models
:concept,theoretical aspects of language models
:notes,accompaniment to the theoretical portion
:process,evidence tracing
:task,scientific document reasoning
:technique,science-focused instructions
:benchmark,scientific document reasoning benchmark
:finding,fabricated evidence in science tasks
:corpus,scientific corpus
:issue,evidence fabrication
:`model type`,neural network model
:`analysis tool`,existing analysis tools
:task,testing hypotheses
:`analysis technique`,circuit probing
:task,uncovering computation circuits
:task,simple arithmetic tasks
:`analysis tool`,other methods
:model,GPT-2 small
:`linguistic feature`,subject-verb agreement
:model,GPT-2 medium
:`linguistic feature`,reflexive anaphora
:task,pediatric ophthalmology consultation
:group,medical professionals
:study,survey study
:group,attending physicians
:group,medical students
:attribute,stability and confidence in responses
:field,medical assistance and education
:property,strong watermarking
:challenge,watermark erasure without quality loss
:concept,strong watermarking impossibility
:assumption,natural assumptions
:setting,private detection algorithm
:attack,generic efficient watermark attack
:oracle,quality oracle
:oracle,perturbation oracle
:assumption,attacker capabilities
:trend,model capability growth
:attack,feasibility demonstration
:consequence,system instability
:technique,static detection
:method,mechanical API matching
:approach,InferROI
:challenge,static detection challenges
:technique,static analysis
:dataset,DroidLeaks
:outcome,high bug detection rate
:outcome,unknown resource leak bugs
:resource,well-annotated resources
:performance,NLP task performance
:challenge,scaling manual annotations
:factor,domain knowledge and semantic features
:task,computational stance detection
:technique,automated labeling with large language models
:role,alternative to human annotators
:challenge,sensitivity and biases in machine annotation
:strategy,multi-label and multi-target sampling
:quality,annotation quality
:performance,stance detection task performance
:corpora,benchmark stance detection corpora
:technique,Pixel2Seq
:concept,region-level understanding
:paradigm,Pixel2Emb
:task,object location modeling
:concept,multimodal conversations
:task,localization tasks
:model,Next-Chat
:activity,bug reproduction
:characteristic,challenging to automate
:report,bug report
:`bug type`,crash bugs
:technique,Libro
:model,Starcoder LLM
:dataset,held-out bug dataset
:experiment,experiments on LLMs of different sizes
:information,which LLMs can be used with Libro
:`research area`,cognitive plausibility of language models
:`psycholinguistic response variables`,"reading times, gaze durations, N400/P600 EEG signals"
:`linguistic aspects`,"formal and functional linguistic competence, developmental plausibility"
:`model family`,GPT-like language model
:corpus,BabyLM pretraining corpus
:`evaluation tasks`,"BLiMP, GLUE, MSGS, reading time prediction task"
:`model attribute`,language model size
:performance,challenge task performance
:`psycholinguistic response variables`,reading time fit
:model,second-smallest GPT-like language model
:`statistical measure`,log-likelihood reduction
:approach,training GPT-like LMs on developmentally plausible corpus
:`linguistic aspects`,processing effort and linguistic competence
:task,multi-modal interaction and generation
:approach,TEAL (Tokenize and Embed All)
:technique,joint embedding space
:role,interface for multi-modal tasks
:modality,non-textual modalities
:examples,image and audio
:experiment,TEAL performance evaluation
:outcome,improvements in multi-modal tasks
:process,knowledge acquisition
:knowledge,acquired knowledge
:attribute,incorrect or outdated
:process,knowledge rectification
:approach,hyper-network for parameter shift
:issue,inferior scalability
:model,Massive Language Model Editing Network (MALMEN)
:capability,editing multiple facts simultaneously
:benchmark,strong baselines and GPT-specific editors
:resource,MALMEN code
:process,listening
:`model family`,text-based language model
:`model family`,speech-based language model
:phenomenon,speech-evoked brain activity
:intervention,elimination of low-level stimulus features
:technology,fMRI
:`brain region`,early sensory regions
:`brain region`,later language regions
:goal,brain-like language processing
:task,language inference and understanding tasks
:data,raw textual data
:process,human language acquisition
:demographic,13-year-old kids
:task,learning contextual word representations
:baseline,strong set of baselines
:submission,submission details
:`system type`,generative information retrieval system
:`response type`,grounded generated text
:`response type`,document ranking
:task,quantifying utility
:`evaluation methodology`,ranking-based ad hoc retrieval
:need,new evaluation approaches
:activity,surveying literature
:field,information retrieval and natural language processing
:`system type`,generative ad hoc retrieval system
:model,user model
:tool,scientific instrument
:`cognitive function`,human memory
:process,memory experiment design
:tool,pipeline
:process,memory experiment analysis
:experiment,online memory experiment
:participant,large number of participants
:data,recognition and recall data
:performance,recall and recognition performance
:factor,narrative length
:experiment,scrambled story experiment
:performance,recall performance
:performance,recognition performance
:phenomenon,contextual reconstruction
:model,Crammed BERT
:model,DACBERT
:framework,dependency agreement pretraining
:theory,linguistic theories
:technique,dependency agreement capture
:`data format`,refined embeddings
:hardware,single GPU
:study,extensive studies
:`model architecture`,Linear RNN
:technique,gating mechanisms
:technique,forget gates
:model,Hierarchically Gated Recurrent Neural Network (HGRN)
:capability,dependency modeling
:URL,https://github.com/opennlplab/hgrn
:data,all human-produced data
:effort,data decontamination
:method,string matching
:issue,insufficient decontamination
:model,13B model
:benchmark,test benchmark
:benchmark,GSK8K
:method,LLM-based decontamination method
:dataset,RedPajama-Data-1T
:dataset,Starcoder-Data
:issue,unintentional contamination
:method,stronger decontamination approaches
:task,fresh one-time exams
:tool,LLM Decontaminator
:url,https://github.com/lm-sys/llm-decontaminator
:method,LongQLoRA
:technique,"position interpolation, QLoRA, shift short attention"
:model,LLaMA-2 7B and 13B
:hardware,32GB V100 GPU
:number,8192 and 12K
:dataset,"PG19, Proofpoint"
:model,"LongLoRA, MPT-7B-8K"
:data,39K long instruction data
:task,long and short context generation
:experiment,ablation experiments
:factor,"LoRA rank, finetuning steps, attention patterns"
:resource,"model weights, training data, code"
:url,https://github.com/yangjianxin1/longqlora
:challenge,answer attribution and verification
:challenge,automatic accuracy evaluation
:task,semi-extractive multi-source question answering
:task,multi-answer question summarization
:output,comprehensive answer with attributions
:dataset,QuoteSum
:metrics,text-based evaluation metrics
:capability,consolidation capabilities
:language,Verilog
:task,hardware design
:process,hardware design and debugging
:data,massive datasets of text and code
:task,HDL code generation
:tool,AutoChip
:data,compilation errors and debugging messages
:output,accurate Verilog code
:resource,HDLBits problem sets
:improvement,24.20% more accurate Verilog
:resource,evaluation scripts and datasets
:capability,human behavior simulation
:study,unintended side-effects of persona assignment
:bias,deep rooted bias
:bias,stereotypical presumptions
:performance,drop in reasoning task performance
:bias,inherent deep bias
:trend,assigning personas to LLMs
:`evaluation metric`,word prediction success
:task,word distribution prediction
:`linguistic knowledge`,contextual relationships
:task,inference between word distributions
:concept,contextual relationships representation
:domain,argument structure
:task,generalization of novel noun arguments
:task,abstract structural generalization
:limitation,bias towards linear order generalization
:limitation,data-intensive training
:resource,structural alternations study repository
:url,https://github.com/clay-lab/structural-alternations
:`model type`,neuro-symbolic model
:process,exhaustive code generation
:model,generative neuro-symbolic visual reasoning model
:process,three-stage process
:process,module initialization
:task,vision-language task
:process,module generation
:action,code snippet creation
:process,module execution
:task,testing set evaluation
:capability,task adaptability and transferability
:goal,reduce weights without compromising performance
:approach,weights magnitude
:aspect,weights
:approach,SparseGPT
:aspect,weights and activations
:approach,WANDA
:approach,prior approaches
:aspect,informative gradients
:method,Gradient-Based Language Model Pruner (GBLM-Pruner)
:aspect,normalized gradients
:approach,competitive counterparts
:method,unstructured pruning
:concept,structural patterns
:URL,https://github.com/rocktimjyotidas/gblm-pruner
:task,formal representation encoding
:field,AI & Law
:system,rule-based expert system
:process,legislation analysis
:bottleneck,development bottleneck
:task,structured representation extraction
:methodology,JusticeBot
:output,pathways
:standard,manual pathways
:result,60% equivalent or better pathways
:evaluation,blind comparison
:approach,leveraging LLMs for system development
:value,system development efficiency
:`system type`,symbolic approach-based system
:value,transparency and explainability
:challenge,phishing and spam detection
:field,academic research
:issue,phishing and spam
:consequence,financial hardships and ransomware entry
:approach,heuristic-based detection
:task,phishing and spam email detection
:model,IPSDM
:performance,email classification
:goal,information system security
:method,chain of empathy prompting
:approach,psychotherapy approaches
:`response type`,exploratory responses
:approach,cognitive behavioral therapy
:`response type`,balanced empathetic responses
:concept,emotional context
:interaction,human and AI communication
:phenomenon,paradigm shift in information dissemination
:challenge,discerning veracity of news
:phenomenon,proliferation of real and fake news
:gap,understanding interplay between news types
:evaluation,comprehensive evaluation of fake news detectors
:detector,fake news detector
:data,human-written articles
:bias,bias against machine-generated texts
:process,training fake news detectors
:strategy,practical strategy for robust fake news detectors
:finding,detectors' performance and bias
:method,decoding-based control
:task,multiple subject control
:method,SF-Gen
:concept,successor features
:method,language model rectification
:task,dynamic text steering
:community,educators
:phenomenon,LLM integration in education
:capability,vision and language
:task,visual problems
:`problem type`,Parsons problems
:evaluation,visual assignments
:issue,academic integrity
:solution,visual programming problems
:task,event prediction
:ability,environment interaction
:hypothesis,trajectory straightening
:`model architecture`,autoregressive Transformer
:metric,1-dimensional curvature
:concept,straightness
:model,trained autoregressive Transformer
:concept,curvature decrease
:concept,lower curvature
:concept,average curvature
:concept,average surprisal
:model,untrained autoregressive Transformer
:benchmark,professional and academic benchmarks
:accessibility,access to large language models
:infrastructure,costly infrastructure for LLMs
:`web interface`,"rate-limited, geo-locked, and censored web interfaces"
:resource,publicly available code and technical reports
:project,GPT4All
:goal,democratizing access to LLMs
:`model family`,original GPT4All model family
:documentation,technical overview
:ecosystem,open source ecosystem
:purpose,technical overview and case study
:task,ophthalmology questions
:`professional group`,medical undergraduates
:`professional group`,medical masters
:attribute,answer stability and confidence
:field,medical education and clinical decision making
:technique,prompt cache
:task,accelerating inference
:component,attention state
:concept,prompt module
:implementation,prototype implementation of prompt cache
:`performance metric`,latency in time-to-first-token
:task,document-based question answering
:task,recommendations
:hardware,CPU
:attribute,output accuracy
:limitation,context-window size
:`research effort`,enhancing LLMs' long-context understanding
:benchmark,long-sequence benchmarks
:benchmark,Loogle
:shortcoming,prior datasets' limitations
:evaluation,Loogle evaluation
:technique,retrieval-based techniques
:task,short question-answering
:strategy,extending context window length
:capability,long context understanding
:goal,true long-context understanding
:issue,disconnected and wordy intermediate responses
:technique,sequential prompting
:technique,prompt sketching
:process,decoding procedure
:benchmark,LLM benchmarking tasks
:resource,generic sketches
:software,DCLib
:trend,LLM-related applications
:expectation,accommodating diverse personas
:goal,aligning language models with personas
:concept,persona
:methodology,data-driven persona definition
:technique,collaborative-filtering
:representation,continuous vector space
:group,cohorts
:concept,latent social groups
:concept,model steerability
:method,soft-prompting model
:representation,sequences of virtual tokens
:algorithm,steerability algorithm
:technique,attention weights
:approach,interpreting PLMs with high-level concepts
:concept,food
:model,C3M
:technique,concept combination
:task,distinguishing AI-generated content
:method,current text detection methods
:detector,DeMaSk
:issue,evasion of detection
:technique,energy-based detection
:task,evaluating DeMaSk
:evaluation,DeMaSk performance
:phenomenon,existential crisis
:era,first era of large language models
:`model type`,n-gram models
:concept,durable lessons and evergreen problems
:factor,hardware advancement
:concept,scale
:concept,scale disparities
:concept,transient
:concept,meaningful applications
:concept,meaningful evaluation
:approach,speculative approaches
:technique,model internals conversion to text
:outcome,human-understandable insights
:approach,training-free image captioning
:framework,ZS-A2T
:capability,zero-shot translation of Transformer attention to text
:task,token selection
:model,VQA model
:capability,text-image matching
:flexibility,drop-in replacement
:dataset,GQA-REx
:dataset,VQA-X
:resource,ZS-A2T code
:URL,https://github.com/explainableml/zs-a2t
:capability,fluent text generation
:capability,knowledge-intensive task rationalization
:task,commonsense multiple-choice questions
:requirement,world knowledge-based rationales
:approach,knowledge-guided rationalization generation
:technique,few-shot learning with expert examples
:rationale,knowledge-grounded rationales
:rationale,crowdsourced rationalizations
:rationale,LLM-generated rationales
:quality,conciseness and novelty
:rationale,rationalization of incorrect model predictions
:value,human trust
:pipeline,two-stage pipeline for rationale generation
:goal,trustworthy rationale generation
:outcome,rich text representations
:technique,token-mixing mechanisms
:technique,Hartley transform
:technique,Fourier transform
:architecture,hybrid seq2seq
:infrastructure,simpler infrastructure
:value,sustainability
:capability,human language understanding and generation
:`research direction`,LLMs in medicine
:survey,"current progress, applications, and challenges of LLMs in medicine"
:`model family`,medical large language model
:task,building and performance of medical LLMs
:challenge,use of medical LLMs
:task,construction and utilization of medical LLMs
:survey,opportunities and challenges of LLMs in medicine
:resource,practical guide resources of medical LLMs
:URL,https://github.com/ai-in-health/medllmspracticalguide
:task,mathematical understanding and reasoning
:field,artificial intelligence assessment
:benchmark,existing mathematical benchmarks
:characteristic,comprehensive reasoning and diverse data
:dataset,Conic10K
:`knowledge domain`,conic sections
:aspect,model knowledge and reasoning analysis
:resource,Conic10K dataset and codes
:URL,https://github.com/whynlp/conic10k
:system,PromptMind
:solution,automated prompt generation
:interaction,human-chatbot interaction
:task,interaction tasks
:outcome,reduced mental demands
:outcome,enhanced performance and social connections
:metaphor,bridge
:concept,cognates
:task,language understanding tasks
:task,unsupervised machine translation
:approach,supervised cognate detection
:task,cognate detection
:`language type`,under-resourced language
:framework,weakly-supervised deep cognate detection
:requirement,hand-crafted annotation
:experiment,cognate detection experiments
:dataset,cognate detection datasets
:model,proposed model
:concept,language family diversity
:resource,code and dataset building scripts
:industry,information technology
:`data type`,system log data
:task,system management
:challenge,identifying anomalies in logs
:condition,rapidly accumulating logs
:methodology,traditional deep learning-based anomaly detection
:requirement,dataset-specific training
:model,RAPID
:task,anomaly detection without training delays
:concept,logs as natural language
:technique,retrieval-based
:technique,core set
:goal,reducing computational cost
:capability,real-time detection without delay
:technique,weight and activation quantization
:technique,activation-quantization-aware scaling
:technique,sequence-length-aware calibration
:format,DINT
:issue,underflow in quantization
:technique,post-training quantization enhancements
:hardware,arithmetic units compatible with DINT
:benchmark,hardware efficiency
:competition,BioASQ Challenge
:technique,retrieval augmented generation
:task,query-focused summarization
:task,biomedical query-focused multi-document summarization
:concept,profile representation
:dataset,profile-based dialogue dataset
:framework,unified framework for profile representation
:resource,new profile representation resource
:baseline,profile-based models
:`model family`,profile-based model
:capability,generalisation
:quality,consistency with profile and context
:configuration,inter-character
:configuration,intra-character
:concept,privacy concern
:task,hallucination detection and mitigation
:taxonomy,LLM hallucinations taxonomy
:factor,contributing factors to hallucinations
:approach,hallucination mitigation
:challenge,current limitations in hallucination research
:question,open questions in hallucination research
:goal,cyber resiliency
:task,fault identification
:field,digital forensics
:characteristic,"volume, variety, and velocity of logs"
:issue,lack of anomalous log entries
:`model family`,retrieval augmented large language model
:configuration,question and answer pipeline
:experiment,RAGLog
:technique,beam search decoding
:concept,expected risk
:concept,approximations in MBR
:technique,Monte Carlo estimation
:variant,Model-Based MBR (MBMBR)
:concept,business models and services
:method,"econometric models, technical showcases, literature reviews"
:activity,empirical examination
:entity,firm level influence
:taxonomy,LLM-based business model implementation
:taxonomy,LLM-driven business transformations
:concept,business transformation
:framework,business model design
:community,academic and business communities
:tool,strategic tool for businesses
:business,businesses
:task,sentence embeddings
:`model architecture`,autoregressive
:dependency,forward dependencies
:dependency,backward dependencies
:approach,Dependency-Enhanced Large Language Model (DEELM)
:phenomenon,turning point
:task,Semantic Textual Similarity (STS)
:benchmark,STS tasks
:behavior,human-like text-based interactions
:practice,using human tests on LLMs
:test,human tests
:population,human sub-populations
:evidence,LLMs' responses deviation
:test,personality tests
:`test item`,reverse-coded items
:`prompt variation`,steering LLMs
:concept,personality factors
:population,human samples
:concept,LLM personality
:`system integration`,"perception, decision-making, and control systems"
:approach,traditional autonomous driving approaches
:limitation,complex environment and intention understanding
:model,modelnamefull
:application,autonomous driving scenarios
:capability,scene understanding and causal reasoning
:capability,handling out-of-distribution scenarios
:task,direction discernment and traffic light recognition
:project,gpt4v-ad-exploration
:url,https://github.com/pjlab-adg/gpt4v-ad-exploration
:structure,hierarchical task tree
:evaluation,LLM proficiency
:capability,diverse natural language capabilities
:`evaluation standard`,detailed evaluation standards and processes
:judgment,consistent and unbiased
:dataset,test set
:methodology,standardized evaluation methodology
:task,automating evaluation
:resource,"task tree, TencentLLMEval dataset, evaluation methodology"
:model,Tencent HunYuan LLM
:goal,safe and human-aligned LLMs
:assistant,LLaVA-Plus
:repository,skill repository
:`model type`,vision and vision-language models
:`data type`,multimodal instruction-following data
:assistant,LLaVA
:feature,image query grounding
:performance,tool use performance
:scenario,new human-AI interaction scenarios
:problem,technical problems
:approach,conversational agent creation
:agent,retrieval agent
:problem,scope
:problem,nonsensical answers
:perception,conversational agents as social actors
:expectation,adherence to social convention
:outcome,poor interaction and perception of threat
:solution,cognitively inspired additions
:problem,technical and social problems
:component,semantic and episodic memory
:component,working memory
:ability,ability to learn
:method,retraining
:method,debiasing during inference
:goal,equitability
:framework,Counterfactually Aware Fair Inference (CAFIE)
:technology,mobile sensing
:activity,understanding human behavior
:task,studying smartphone user behavior
:strategy,mobile sensing strategy
:activity,sensor selection and feature construction
:characteristic,time-consuming and burdensome
:framework,sensing framework
:strategy,automated mobile sensing strategy
:representation,multi-granular human behaviour representation
:method,blind comparative studies and usability evaluation
:field,text representation
:challenge,high-dimensional representation
:goal,model accessibility
:method,IBKD
:problem,performance degradation after distillation
:principle,information bottleneck
:goal,maximize mutual information
:relationship,teacher and student model representation
:goal,reduce mutual information
:relationship,student model representation and input data
:goal,avoid over-fitting
:concept,dual use
:goal,reducing harmful outputs
:trend,fine-tuning of large language models
:community,LLM vendors
:threat,fine-tuning attacks
:threat,removal of RLHF protections
:goal,maintaining usefulness
:goal,protections
:application,interactive language generation
:task,goal-directed dialogue
:method,single-step reinforcement learning
:example,"teaching, preference elicitation"
:task,visualization creation and refinement
:application,analytic chatbots
:application,AI Threads
:finding,LLM-driven analytic chatbots weaknesses
:method,crowdsourced study and expert interviews
:data,dataset outside LLM's training corpus
:concept,future research in LLMs
:task,medical case evaluation
:model,GPT-4 with vision
:group,human respondents
:dataset,NEJM Image Challenge
:evaluation,GPT-4 with vision performance
:`image type`,radiographic images
:factor,caption informativeness
:task,clinicopathological conferences (CPCs) evaluation
:factor,addition of images to text
:`model family`,multimodal AI model
:task,medical diagnostic reasoning
:`model family`,open-source vision-language model
:model,MiniGPT4
:framework,FigSTEP
:issue,AI safety issues
:modality,image channel
:modality,text prompts
:`safety alignment`,novel safety alignment
:modality,visual and textual modalities
:workflow,soundscapes generation
:application,visual media
:`prior work`,matching on-screen sounds
:process,creating scene context
:process,brainstorming sounds
:process,sound generation
:quality,immersive auditory environment
:method,parallelization and optimization
:optimization,flashattention
:category,latest set of optimizations
:study,comprehensive ablation study
:output,key recommendations
:recommendation,using micro-batch size of 1
:concept,efficient training layouts
:concept,larger micro-batch sizes
:technique,activation checkpointing
:problem,pipeline bubbles
:configuration,most efficient configurations
:benchmark,training efficiency results
:benchmark,model flops utilization
:language,smaller languages
:language,Finnish
:population,0.1% of the world population
:dataset,Finnish dataset
:`data source`,"web crawls, news, social media, ebooks"
:model,FinGPT
:`parameter range`,186M to 13B parameters
:model,BLUUMI
:benchmark,FIN-BENCH
:task,Finnish tasks
:`model qualities`,toxicity and bias
:resource,models and tools
:URL,https://turkunlp.org/gpt3-finnish
:concept,public trust
:concept,double-edged sword
:question,utilizing LLMs to combat misinformation
:phenomenon,deceptive misinformation
:question,combating LLM-generated misinformation
:period,pre-LLM era
:effort,current efforts to combat misinformation
:goal,utilizing LLMs for fighting misinformation
:call,interdisciplinary efforts
:framework,Lumos
:concept,unified data format
:architecture,modular architecture
:module,grounding
:module,execution
:data,high-quality annotations
:model,GPT-4/3.5
:method,conventional training
:task,unseen interactive tasks
:task,automatic prompt engineering
:task,constructing a meta-prompt
:component,step-by-step reasoning template
:component,context specification
:concept,optimization concepts
:component,meta-prompt
:method,PE2
:benchmark,MultiArith and GSM8K datasets
:benchmark,instruction induction benchmark
:concept,competency question
:concept,ontology functional requirement
:attribute,ontology scope and applicability
:process,ontology development
:practice,publishing competency questions
:community,ontology engineering community
:method,Retrofit-CQs
:task,competency question extraction
:pipeline,CQ extraction pipeline
:application,CQ extraction pipeline application
:artifact,existing ontologies
:concept,deception
:task,long-horizon dialogues
:concept,persuasion
:task,long-horizon multi-party dialogues
:game,Avalon: The Resistance
:concept,long-horizon deception
:resource,online testbed
:task,understanding player goals and motivations
:concept,player identity
:resource,dataset and online testbed
:method,GIP-CoL
:task,compositional zero-shot learning
:component,compositional graph
:metric,AUC results
:benchmark,MIT-States
:benchmark,UT-Zappos
:benchmark,C-GQA
:findings,GIP-CoL analysis
:method,provenance graph analysis
:task,cyber attack detection
:threat,advanced persistent threat
:framework,LogShield
:task,APT attack detection
:technique,self-attention mechanism
:dataset,DARPA OpTC
:dataset,DARPA TC E3
:challenge,training models for low-resource languages
:condition,lack of high-quality training data
:technique,adapting pretrained large language models
:goal,efficient language adaptation
:problem,naive language adaptation
:component,tokenizer efficiency
:study,efficient adaptation of pretrained LLMs to new languages
:technique,tokenizer encoding efficiency improvement
:technique,data mixing recipe
:experiment,adapting English LLM to Hungarian and Thai
:outcome,better performance on target language
:benchmark,open source models
:task,complex task
:representation,natural language learned features (NLLF)
:technique,weak labeling
:task,main task
:capability,zero-shot inference
:technique,classifier enhancement
:application,detecting incoherence in students' answers
:task,mathematics exam question analysis
:application,screening abstracts for systematic literature review
:issue,"transparency, reproducibility, cost, and data protection"
:evaluation,systematic comparative evaluation
:`model type`,supervised classification model
:dataset,tweets from US news media
:task,binary text annotation
:approach,iterative executors
:approach,plan-and-execute
:approach,ADAPT
:technique,as-needed decomposition
:environment,AlfWorld
:dataset,TextCraft
:capability,executor LLM capability
:`model family`,multilingual dense retrieval model
:issue,scarcity of multilingual training data
:technique,synthetic training data generation
:dataset,SWIM-IR
:task,training multilingual dense retrieval models
:technique,SAP (Summarize-then-Ask Prompting)
:model,SWIM-X
:benchmark,retrieval benchmarks
:model,human-supervised dense retrieval model
:resource,human-labeled retrieval training data
:ability,novel special abilities
:field,Generative Artificial Intelligence
:paradigm,Model-as-a-Service (MaaS)
:process,deployment and utilization of GenAI models
:value,scalability and accessibility
:concept,X-as-a-Service
:technology,cloud computing
:technology,key technologies
:value,democratization
:activity,application studies
:concept,challenges and future issues
:task,financial tasks
:benchmark,CFBenchmark-basic
:task,Chinese financial text processing
:data,financial texts
:experiment,CFBenchmark-basic experiments
:task,financial text processing
:benchmark,CFBenchmark-advanced
:resource,CFBenchmark codes
:url,https://github.com/tongjifinlab/cfbenchmark
:process,model reasoning
:`reward scheme`,outcome-supervised reward model
:`reward scheme`,process-supervised reward model
:task,complex mathematical reasoning
:concept,reward aggregation function
:study,fine-grained reward modeling
:language,Tamil
:data,"16,000 Tamil tokens"
:corpus,Tamil corpus
:dataset,OpenORCA
:performance,Tamil text generation
:landscape,LLMs in Indian languages
:commitment,open research
:value,openness
:`model family`,vision-language pre-trained model
:capability,visual understanding and cross-modal analysis
:service,multi-modal embedding as a service
:resource,training data and resources
:threat,model extraction attacks
:value,intellectual property and commercial ownership
:solution,watermarking model
:method,VLPMarker
:method,embedding orthogonal transformation
:strategy,collaborative copyright verification
:attribute,watermark robustness
:approach,out-of-distribution trigger selection
:attribute,watermark practicality
:attribute,effectiveness and safety of watermarking approach
:url,https://github.com/pter61/vlpmarker
:issue,outdated data and domain-specific limitations
:field,integration of knowledge and large language models
:taxonomy,"methods, benchmarks, and applications"
:analysis,in-depth analysis of methods
:`research direction`,potential future research directions
:`research activity`,future research endeavors
:technique,soft-prompting
:model,GPT-3.5 fine-tuned
:model,GPT-3.5 with RAG
:event,September 2021
:platform,commercial platforms
:task,establishing baseline performance
:issue,discrepancy in performance
:concept,mismatched generalization
:phenomenon,fake alignment
:benchmark,comparative benchmark
:framework,Fake Alignment Evaluation (FINE)
:metric,Consistency Score (CS)
:metric,Consistent Safety Score (CSS)
:task,few-shot relation extraction
:process,identifying relationships
:technique,neural graph
:approach,COT-ER
:dataset,FewRel1.0 and FewRel2.0
:practice,traditional pedagogical practices
:task,code writing
:trend,shift in pedagogical focus
:skill,code reading and comprehension
:skill,prompt construction for code-generating models
:task,solving programming tasks
:`exercise type`,prompt problems
:tool,Promptly
:course,CS1
:course,CS2
:`student perception`,enthusiasm for prompt problems
:idea,future development of prompt problems
:concept,human civilisation progress
:phenomenon,explosion of scientific literature and data
:issue,information barriers
:asset,global and interdisciplinary knowledge
:potential,LLMs for scientific discovery
:task,formal exploration
:dataset,biomedical literature hypothesis dataset
:`model family`,top-tier instructed large language model
:framework,LLM-based multi-agent cooperative framework
:metrics,hypothesis evaluation metrics
:task,hypothesis evaluation
:finding,LLMs generate untrained yet validated hypotheses
:strategy,increasing uncertainty
:goal,human-like planning and control
:observation,multimodal observations
:environment,open world
:agent,Jarvis-1
:environment,Minecraft universe
:concept,plans
:component,goal-conditioned controllers
:component,multimodal memory
:task,Minecraft tasks
:task,short-horizon tasks
:example,chopping trees
:example,obtaining a diamond pickaxe
:task,obtaindiamondpickaxe
:demand,superior medical services
:issue,medical infrastructure discrepancies
:`data type`,big data
:sector,medical services
:approach,conventional NLP approaches
:task,medical text processing
:goal,domain knowledge learning
:limitation,restricted context length
:model,Chimed-GPT
:feature,"4,096 tokens context length"
:`training method`,comprehensive training regime
:`model family`,general domain large language model
:issue,bias and discrimination
:resource,Chimed-GPT code and model
:URL,https://github.com/synlp/chimed-gpt
:goal,infer membership status
:category,reference-free and reference-based attacks
:hypothesis,higher probability of training record sampling
:attack,reference-based membership inference attack
:goal,effectiveness in LLMs
:resource,reference dataset
:attack,membership inference attack based on self-calibrated probabilistic variation
:issue,privacy leakage in LLMs
:signal,probabilistic variation membership signal
:approach,self-prompt
:resource,dataset for reference model
:resource,dataset from public APIs
:classifier,standard full-data classifier
:resource,thousands of labeled examples
:domain,data-limited domains
:resource,1-5 examples per class
:organization,budget-limited organizations
:dataset,Banking77
:task,financial intent detection
:company,Cohere
:company,Anthropic
:goal,reducing operational costs
:role,human expert
:task,check-worthy claim detection
:`model type`,fine-tuned model
:`model type`,extremely large language model
:dataset,multilingual and multi-topical dataset
:task,benchmark analysis
:attribute,multilingual settings
:metric,"accuracy, recall, F1-score"
:`model type`,zero-shot approach
:scenario,cross-domain
:task,problem-solving and decision-making
:`model family`,solver-augmented large language model
:process,parsing and solving
:issue,parsing errors
:model,LogIPT
:solution,bypassing parsing errors
:dataset,instruction-tuning dataset
:theory,merge and minimalism
:model,algebraic model of syntactic-semantic interface
:method,renormalization
:process,extraction of meaning
:field,theoretical physics
:field,computational semantics
:field,generative linguistics
:controversy,implications for generative linguistics
:intersection,AI and human reasoning
:quality,wisdom
:impact,transformative impact of LLMs
:entity,democratic societies
:concern,distinguishing LLM output from human output
:quality,human capacity for reason
:risk,threats to democracy
:thesis,widespread deployment of LLMs
:solution,education emphasis
:goal,responsible development and usage of LLMs
:capability,"human capacities in thinking, deliberating, and decision-making"
:technique,data contamination quiz
:format,multiple-choice questions
:evaluation,data contamination detection
:signal,contamination signal
:activity,attacking large language models
:phenomenon,novel human activity
:methodology,formal qualitative methodology
:activity,interviewing practitioners
:practitioner,contributors
:concept,motivations and goals
:concept,strategies and techniques
:community,practitioner community
:theory,grounded theory of LLM red teaming
:mechanism,recurrent mechanism
:scheme,ARROW
:dataset,Persuade corpus
:task,annotating argumentative elements
:dataset,Argument Annotated Essays (AAE) dataset
:performance,XLNet model performance
:analysis,comparison of model outputs
:concept,annotation tag relationships
:task,automated feedback on essay organization
:trend,ubiquity
:process,training from scratch
:characteristic,prohibitively expensive
:`finetuning paradigm`,Orthogonal Finetuning (OFT)
:characteristic,good generalizability
:algorithm,Cooley-Tukey Fast Fourier Transform
:concept,efficient information transmission
:parameterization,efficient orthogonal parameterization
:structure,butterfly structures
:`finetuning method`,Orthogonal Butterfly (BOFT)
:framework,generalized orthogonal finetuning framework
:`model family`,large vision transformer
:ecosystem,labor market
:entities,labor market entities
:method,isolation or pairwise analysis
:concept,labor space
:`model application`,vector-space embedding
:method,integrative analysis
:capability,exploration of inter-unit relations
:value,strategic decision-making
:stakeholders,policymakers and business leaders
:capability,vector arithmetic
:axis,economic axes
:feature,personalized generations
:approach,entity-centric knowledge store
:`data source`,search and browsing activities
:infrastructure,search log infrastructure
:issue,privacy and scalability concerns
:task,contextual query suggestion
:benchmark,LLM-powered baselines
:tool,computer simulation
:concept,system dynamics
:task,modeling natural language instructions
:`modeling framework`,smart agent-based modeling
:concept,smart agents
:attribute,intelligence and adaptability
:concept,LLM-powered agents
:concept,future applications
:task,slot-filling
:framework,graph-based framework
:task,learning domain-specific prompts
:concept,domain-specific schema
:concept,schema relations
:method,graph-based method
:approach,multi-domain dialogue state tracking
:model,graph-based dialogue state tracking model
:challenge,sparsity of labelled data
:field,natural-products literature
:topic,bioactive compounds identification
:dataset,LOTUS database
:task,training set creation
:sampler,Greedy Maximum Entropy Sampler (GME-Sampler)
:task,evaluation set optimization
:task,manual curation
:noise,training set noise
:task,end-to-end relation extraction
:model,BioGPT-Large
:data,synthetic abstracts
:resource,evaluation dataset
:`empirical law`,Heaps' Law
:phenomenon,vocabulary growth
:corpus,human-authored text corpora
:text,large language model generated text
:corpus,PubMed abstracts
:attribute,model parameter size
:technique,prompt-based emulation
:task,corpus emulation
:finding,adherence to Heaps' Law
:corpus,emulated corpora
:goal,richness and authenticity of outputs
:field,human resources (HR)
:`document type`,resumes and job descriptions
:technology,natural language processing (NLP)
:challenge,real-world adoption
:benchmark,Resume-Job Description Benchmark (RJDB)
:task,HR tasks
:graph,skill-occupation graph
:data,triples of job descriptions and resumes
:task,skill extraction and resume-job description matching
:community,research and industry
:group,policy makers
:activity,decision making
:collaboration,interdisciplinary teams
:feature,speech recognition input modality
:process,speech to text conversion
:technology,Python and Pandas
:activity,data analysis and interpretation
:network,automated insect traps and sensor probes
:system,agricultural data processing system
:system,remote server database
:`data modality`,different modalities
:`data stream`,EHR audit logs
:activity,clinician activities
:field,clinician workflow research
:technique,cross-sectional aggregation
:concept,EHR session complexity
:metric,workflow entropy
:action,model release
:task,knowledge graph related tasks
:task,KG embeddings
:task,entity alignment
:task,evaluation of reasoning abilities
:entity,real knowledge graphs
:concept,structure and properties
:study,large scale comparative study
:finding,recommendations for KG-based model development and evaluation
:goal,better KG models
:goal,breaking data silos
:task,hate and offensive speech classification
:problem,scarcity of datasets
:task,fine-grained classification
:dataset,THOS
:task,specific NLP tasks
:framework,lifelong learning
:task,stream of NLP tasks
:approach,L3 ensemble method
:goal,incremental task performance improvement
:benchmark,STS benchmark
:task,conditional graphic layout generation
:attention,widespread attention
:issue,lack of versatility and data efficiency
:system,LayoutPrompter
:component,input-output serialization
:component,dynamic exemplar selection
:technique,prompting exemplars selection
:component,layout ranker
:task,layout quality ranking
:baseline,training-based baseline
:sector,internet industry
:pipeline,novel pipeline for domain-specific QA
:value,user-friendly
:model,LLM for domain-specific QA
:goal,reliable answers
:problem,model preference problem
:method,Knowledgeable Preference Alignment (KnowPat)
:objective,new alignment objective
:resource,KnowPat code
:url,https://github.com/zjukg/knowpat
:task,sentiment analysis and machine translation
:task,multi-step reasoning tasks
:task,code generation and comprehension
:`programming languages`,C and C++
:`model family`,large language model trained on code datasets
:task,rectifying non-compilable code
:factor,quality of training dataset
:performance,subpar performance in code rectification
:factor,inherent complexity of problem
:limitation,latent drawbacks of LLMs
:approach,CompCodeVet
:task,producing compilable code
:tool,compilers
:resource,open-source code datasets
:quality,training dataset quality for LLMs
:issue,biased responses
:work,fairness in TOD systems
:issue,system bias
:method,bias diagnosis
:experiment,bias attribution in TOD systems
:issue,demographic bias
:`model type`,multi-lingual model
:study,generalization to OOD test data
:data,counterfactually augmented data (CAD)
:approach,new approaches for OOD generalization
:process,costly annotation
:model,LaBSE
:data,English IMDb movie reviews
:evaluation,OOD test sets evaluation
:data,OOD test sets in 13 languages
:result,OOD performance decline
:data,counterfactuals from high-resource language
:approach,cost-effective approaches
:`performance issue`,in-context learning performance in low-resource languages
:approach,cross-lingual retrieval-augmented in-context learning (CREA-ICL)
:`performance issue`,zero-shot performance in multilingual pre-trained language models
:technique,semantically similar prompt retrieval
:`model family`,multilingual pre-trained language model
:`task type`,generation tasks
:evaluation,performance dynamics of retrieval-augmented in-context learning
:task,business and financial NLP
:benchmark,BizBench
:task,quantitative reasoning
:number,8 quantitative reasoning tasks
:task,question-answering (QA) for financial data
:skill,reading comprehension and domain knowledge
:domain,finance and business
:`model family`,visual large language model
:framework,PerceptionGPT
:component,token embedding
:approach,PerceptionGPT method
:challenge,training difficulty
:metric,sequence length during inference
:capability,integration of visual perception tasks
:result,PerceptionGPT experiment outcomes
:metric,improvements in efficiency and performance
:goal,enabling LLMs with visual perception abilities
:process,AI model training
:community,algorithm engineers
:challenge,iterative model development
:goal,high-quality and efficient model development
:industry,AI industry
:system,traineragent system
:framework,multi-agent framework
:process,model optimization
:outcome,satisfactory models
:task,unattainable tasks
:process,traditional model development
:research,traineragent research
:community,academic and industry communities
:integration,LLM with social robots
:content,AI-generated news reports
:field,journalism
:field,journalism and media
:system,GPT with Pepper robot
:capability,natural language understanding and response generation
:capability,contextual conversation and news reporting
:resource,news resource and Google search
:evaluation,performance assessment
:criteria,"relevance, context, and fluency"
:field,journalism and human-robot interaction
:framework,conversational capabilities improvement
:capability,conversational capabilities of robots
:attribute,"model accuracy, robustness, interpretability"
:`model family`,prototype-based network
:method,similarity to prototypes
:attribute,native interpretability and noise robustness
:study,robustness properties of prototype-based networks in text classification
:framework,modular and comprehensive framework for studying PBNS
:component,"backbone architectures, sizes, objective functions"
:`evaluation protocol`,robustness assessment against perturbations
:experiment,robustness of PBNS in NLP classification tasks
:task,NLP classification tasks
:`objective function`,prototype interpretability maintenance
:attribute,prototype-based network robustness
:`model family`,vanilla model
:approach,in-context vectors (ICV)
:process,creating in-context vector
:process,shifting latent states
:value,efficiency and control
:benchmark,diverse tasks performance
:capability,following different instructions
:task,translation tasks
:technique,input reformulation
:experiment,input reformulation techniques
:metric,3.5 CHRF++
:URL,https://github.com/bri25yu/languagemodelexperimentation
:data,web-scale corpora
:property,trusted source alignment
:dataset,FactCheckQA
:protocol,TSA evaluation protocol
:object,book review
:method,machine learning classification
:feature,n-gram features
:finding,review feature significance
:challenge,determining popularity in creative domains
:domain,creative domains
:insight,factors underlying review popularity
:need,further research in creative realm
:capability,generalization to unseen tasks
:task,downstream application adaptation
:model,OPT-IML-175B
:attribute,publicly accessible
:model,FLAN-PaLM-540B
:model,CAPPY
:goal,performance and efficiency enhancement
:capability,independent or auxiliary functioning
:capability,efficient downstream supervision integration
:benchmark,PromptSource tasks
:technique,LLM adaptations
:task,proof planning
:task,complex reasoning chains
:model,smaller-scale language model
:task,stepwise proof generation
:dataset,EntailmentBank
:task,visio-linguistic tasks
:application,large model applications on social media
:task,hateful meme correction
:task,hateful meme detection and correction
:experiment,empirical experiments on LLaMA
:task,complex logical reasoning
:theory,dual process theory
:framework,cognitive tree (CogTree)
:methodology,iterative construction
:module,implicit extraction module
:system,intuitive system
:module,explicit reasoning module
:system,reflective system
:output,multiple responses
:method,comparative learning
:metric,performance level
:`model family`,knowledge-enhanced pre-trained language model
:challenge,adaptation to closed domains
:framework,KANGAROO
:domain,closed domains
:phenomenon,global sparsity
:concept,hyperbolic embeddings
:strategy,data augmentation based on contrastive learning
:benchmark,KEPLM training paradigms
:`model family`,energy-based model
:characteristic,discrete nature of language
:limitation,prior integration efforts
:algorithm,iterative adversarial attack
:phenomenon,spurious modes
:task,arithmetic sequence generation
:technique,adversarial techniques
:strategy,adversarial attack strategy tailored to text
:algorithm,adversarial training algorithm for EBMs
:validation,empirical validation of performance improvements
:task,vision-language-related tasks
:task,visual grounding
:`training scheme`,three-stage training
:module,pool-adapter
:concept,visual embeddings
:approach,INFMLLM
:challenge,model privacy protection
:process,federally finetuning models
:approach,novel federated learning training
:concept,soft prompts
:role,global model parameters
:model,global model
:process,local training
:url,https://github.com/alibaba/federatedscope/tree/fedsp/federatedscope/nlp/fedsp
:concept,singular model for multiple IE subtasks
:model,General Information Extraction Large Language Model
:task,IE subtasks
:concept,Mutual Reinforcement Effect
:benchmark,Japanese mixed datasets
:dataset,Text Classification Relation and Event Extraction (TCREE)
:task,knowledge organization and representation
:concept,all relevant concepts and relationships
:ontology,SNOMED-CT
:`knowledge representation`,UMLS Semantic Network
:method,conversational interaction with LLM
:document,Clinical Practice Guidelines (CPGs)
:concept,new medical concepts
:standard,manually generated gold standard
:evaluation,preliminary results evaluation
:issue,safety vulnerabilities
:benchmark,FLAMES
:component,adversarial prompts and annotations
:value,morality and harmony
:tool,specified scorer
:algebra,Kleene algebra
:concept,dynamic tests
:algebra,Kleene algebra with tests
:framework,dynamic test framework
:logic,propositional dynamic logic
:model,relational models
:model,guarded-language models
:class,Kleene algebras with dynamic tests
:complexity,EXPTIME-complete equational theory
:paradigm,social simulation
:system,virtual social system
:attribute,stable operation
:mechanism,insertion mechanism for standardized public events
:event,township water pollution incident
:entity,virtual government
:experiment,controlled variable experiment
:concept,stored memory influence
:system,generative agent-based simulation system
:attribute,personalized customization
:attribute,complexity reduction
:concept,programmatic policy synthesis
:challenge,interpretability evaluation
:metric,LLM-based interpretability metric
:output,natural language explanation
:activity,program reconstruction
:concept,behavioral similarity
:validation,metric validation
:artifact,obfuscated programs
:artifact,lightly obfuscated programs
:finding,interpretability score ranking
:tool,interpretability evaluation tool
:capability,intermediate reasoning generation
:capability,reliable reasoning elicitation
:challenge,chain-of-thought example crafting
:domain,professional domains
:capability,self-reasoning
:method,Self-Explain
:technique,chain-of-thought example generation
:effect,using self-explanations
:comparison,prompting with self-explanations vs human-crafted chain-of-thoughts
:concept,encoding specificity
:`model capability`,natural language task performance
:resource,task instructions
:technique,simple task instructions
:framework,context-based instruction fine-tuning
:model,context-based instruction fine-tuning model
:technique,self-guided response generation
:outcome,improved performance with reduced computation
:dataset,dialogue benchmark datasets
:`data type`,speech data
:`information type`,acoustic and paralinguistic information
:concept,"speaker's tone, emotion, and intent"
:`data type`,audio and visual information
:process,inference/test time
:methodology,language model training leveraging spoken language audio data
:framework,audio-language knowledge distillation
:model,OpenAI Whisper
:`model family`,student language model
:`data type`,audio-text dataset
:task,analyzing spoken transcripts
:trend,model adoption
:need,robust evaluation methodologies
:benchmark,VILMA
:capability,visio-linguistic capabilities
:evaluation,task-based evaluation
:aspect,temporal aspects of moving images
:method,counterfactual evaluation
:test,proficiency tests
:field,video-language model research
:concept,research gaps
:role,general problem solver
:framework,ExpNote
:task,adapting to unfamiliar tasks
:resource,ExpNote data and code
:task,video anomaly detection
:setting,closed-set
:task,open-set video anomaly detection
:task,detecting unseen anomalies
:limitation,frame anomaly scores prediction
:capability,recognizing anomaly categories
:task,open-vocabulary video anomaly detection
:model,proposed model for OVVAD
:task,class-agnostic detection and class-specific classification
:module,semantic knowledge injection
:task,detection
:module,anomaly synthesis
:`model family`,large vision generation model
:benchmark,widely-used benchmarks
:challenge,capacity gap
:concept,curse of capacity gap
:concept,optimal capacity gap
:goal,complete lifting
:concept,law of capacity gap
:model,Minima
:benchmark,compute-performance Pareto frontier
:model,Minichat
:`model family`,7B chat models
:issue,explicit user unfairness
:issue,implicit user unfairness
:value,ethical integrity
:capability,inferring sensitive attributes
:phenomenon,information bubbles
:goal,mitigating implicit user unfairness
:risk,recommendation systems deterioration
:goal,desired behavior in LLMs
:framework,Propane
:technique,automatic prompt optimization
:technique,prompt improvement
:technique,semantically obfuscated prompts
:technique,self-generated natural language explanations
:task,code-to-code translation
:dataset,MultiPL-E
:experiment,code-to-code translation experiments
:`performance metric`,zero-shot translation improvement
:characteristic,difficult programs
:resource,"dataset, code, and canonical solutions"
:mechanism,multi-agent discussion
:concept,symmetry of agents
:interaction,prompts and discussion mechanisms
:mechanism,scalable discussion mechanism
:mechanism,conquer and merge
:phenomenon,text-based misinformation
:domain,online discourses
:ability,discerning truth
:content,deceptive textual content
:dataset,tv game show data
:phenomenon,lies
:`language cue`,verifiable language cues of deception
:detector,algorithms
:framework,bottleneck framework
:`language cue`,novel but accurate language cues of deception
:possibility,collaboration between humans and algorithms
:goal,detecting the truth
:strategy,ensemble multiple model decoded results
:strategy,generate explanation with prediction
:issue,discrepancy between explanations and predictions
:framework,EASE
:technique,explanation-guided ensemble
:technique,soft probability aggregation
:experiment,varying-size LLMs
:issue,miscalibration and brittleness
:approach,discriminative prompting
:framework,Gen-Z
:task,zero-shot text classification
:approach,generative prompting
:feature,multivariate
:performance,zero-shot classification with contextualization
:baseline,zero-shot and few-shot baselines
:quality,robustness to prompt variations
:approach,personalizing classification in a zero-shot manner
:technique,watermarking algorithms
:process,generation process
:challenge,"unbiased, thorough, and applicable evaluations"
:methodology,separate evaluation of generation and detection
:benchmark,WaterBench
:factor,benchmarking factors
:factor,benchmarking procedure
:concept,apples-to-apples comparison
:factor,task selection
:taxonomy,five-category taxonomy
:factor,evaluation metric
:tool,GPT-4-judge
:observation,common struggles for current methods
:resource,WaterBench code and data
:value,environmental sustainability
:technique,analogue computing
:challenge,energy consumption
:device,resistive memory
:technique,in-memory computing
:technique,software-hardware co-design
:solution,optimizing analogue resistive memory neural networks
:technique,edge pruning
:aspect,software
:technique,transmission electron microscopy
:aspect,hardware
:implementation,40nm 256k resistive memory macro
:`performance metric`,accuracy and energy efficiency
:dataset,FashionMNIST
:dataset,Spoken Digits
:task,audio classification
:dataset,DRIVE
:task,image segmentation
:technique,program-based solving
:model,OpenAI-GPT-4
:model,Code Llama (7B)
:task,math word problems
:`problem category`,multi-type quantity problems
:approach,systematic unit specification
:dataset,Unit Consistency Programs (UCPs)
:model,VerityMath
:outcome,preliminary findings
:task,language understanding and information retrieval tasks
:goal,latency-performance tradeoff
:`model family`,compressed language model
:scenario,highly variant number of requests
:model,ElasticLM
:goal,elastic tradeoff adjustment
:concept,compute elasticity
:concept,elastic structure
:technique,elastic optimization
:technique,elastic schedule
:model,ElasticDenseR
:model,ElasticRanker
:task,reranking
:evaluation,offline evaluation
:evaluation,online simulation
:goal,elastic tradeoff demonstration
:capability,dexterity intelligence
:community,robotics community
:review,comprehensive review of LLM applications in robotics
:technique,LLM-based techniques in robotics
:discipline,intelligent science
:challenge,finding challenging tasks
:distribution,long-tail distribution
:phenomenon,failure cases
:task,creating long-tail examples
:framework,Logic-Induced-Knowledge-Search (LINK)
:task,generating long-tail knowledge statements
:dataset,Logic-Induced-Long-Tail (LINT)
:metric,factual correctness
:task,generating long-tail statements
:evaluation,entailment classification task
:domain,multiple fields of research
:issue,falling behind in language model development
:project,Danish Foundation Models Project
:resource,foundation models for Danish language
:strategy,broad cooperation with institutions
:aspect,"motivation, current status, and future perspectives"
:task,text-rich visual question answering
:capability,cross-modal comprehension
:module,language module
:model,OCR model
:task,text recognition
:framework,training-free framework
:dataset,text-rich VQA datasets
:capability,comprehension ability
:bottleneck,visual part
:combination,OCR module with MLLM
:capability,OCR information comprehension
:ability,zero-shot causal reasoning
:experiment,self-paced reading experiment
:phenomenon,causal conflict reading times
:knowledge,script knowledge
:knowledge,script knowledge integration
:resource,causal-script dataset
:URL,https://github.com/tony-hong/causal-script
:goal,fine-grained structural comprehension
:issue,predefined fine-grained relation types
:method,automated annotation
:method,integrated LLM and NLI module
:issue,uncontrolled generations of LLMs
:dataset,DocGNRE
:application,domain-specific relation type definitions
:goal,language semantic comprehension
:field,various scientific areas
:concept,density functional theory
:concept,molecular dynamics
:domain,scientific applications
:methodology,expert-driven case assessments
:methodology,benchmark testing
:capability,scientific expertise
:application,groundbreaking applications
:`research focus`,uncertainty estimation for LLMs
:`research type`,engineering contributions
:framework,LM-Polygraph
:benchmark,extendable benchmark for UE
:application,demo web application
:model,recent large language models
:task,Minesweeper
:capability,multi-step logical reasoning
:task,evaluating hallucinations
:issue,high evaluation costs
:benchmark,AMBER
:`evaluation pipeline`,designed by AMBER
:quality,low-cost and efficient
:model,GPT-4v(ision)
:guideline,mitigating hallucinations
:resource,AMBER data and code
:task,slot filling with noisy ASR transcriptions
:technique,linearised knowledge injection
:model,LaMDA-13B
:technique,fine-tuning and linearised knowledge injection
:model,Vicuña-13B
:model,FLAN-T5-Base
:concept,attribution
:approach,open book question answering
:dataset,factual dataset
:dataset,counterfactual open book QA dataset
:method,hallucination augmented recitations (HAR)
:dataset,counterfactual dataset
:dataset,human-annotated factual dataset
:observation,improvements across model sizes and datasets
:`model type`,black-box language model
:approach,SCOPE generation
:attribute,target domain score
:process,iterative editing
:process,auto-regressive generation
:condition,in-domain and out-of-domain
:capability,chat-based interaction
:emotion,dissatisfaction
:classification,dissatisfaction categories
:dataset,dissatisfactory ChatGPT responses dataset
:issue,grasping user intentions
:strategy,dissatisfaction addressing tactics
:user,end users with low LLM knowledge
:skill,communication skills
:`skill set`,"topic transition, proactively asking questions, concept guidance, empathy, summarising"
:outcome,user engagement
:concept,inner monologue
:process,enhancing LLMs with communication skills
:benchmark,CSkills
:task,evaluating communication skills
:strategy,CSim
:task,story-to-motion
:industries,"animation, gaming, and film"
:control,low-level and high-level
:field,character control
:input,text description
:field,text-to-motion
:constraint,position constraints
:system,novel system for story-to-motion
:output,"controllable, infinitely long motions and trajectories"
:function,text-driven motion scheduler
:scheme,text-driven motion retrieval
:technique,motion matching
:transformer,progressive mask transformer
:artifact,unnatural pose and foot sliding
:`sub-tasks`,"trajectory following, temporal action composition, motion blending"
:benchmark,state-of-the-art motion synthesis methods
:resource,https://story2motion.github.io/
:technique,large-scale pretraining
:`model type`,general-purpose language model
:`model type`,general-purpose vision-language model
:issue,distributional diversity in visual inputs
:technique,vision-language instruction tuning
:technique,Query Transformer (QFormer)
:technique,large-scale multi-modal pretraining
:issue,computational overhead and poor scaling
:technique,QFormer-based vision-language alignment
:goal,improving efficiency of vision-language pretraining
:strategy,efficient QFormer-based vision-language alignment
:technique,explanation
:issue,unfaithful explanations
:work,recent work on faithfulness tests
:technique,faithfulness test
:issue,self-consistency vs inner workings
:technique,self-consistency test
:resource,comparative consistency bank
:measure,CC-SHAP
:resource,CC-SHAP code
:`training data`,ordered knowledge entities
:`training objective`,autoregressive blank infilling
:`training method`,Bidirectional Causal Language Modeling Optimization (BICO)
:`model component`,causal attention mechanism
:application,LLM-based applications
:metric,KPIs
:industry,automotive industry
:system,in-car conversational question answering systems
:metric,KPIs tailored for in-car ConvQA systems
:dataset,datasets for KPIs
:evaluation,preliminary and comprehensive empirical evaluation
:technique,employing varied personas in prompts
:capability,simulating diverse viewpoints
:capability,visual semantic understanding
:concept,visual information comprehension
:method,identifying multi-modal neurons
:`neuron type`,multi-modal neuron
:property,critical properties of multi-modal neurons
:`evaluation metric`,quantitative evaluation metrics
:research,explanatory research on multi-modal LLMs
:`model application`,simulation of human reading behavior
:metric,psychometric predictive power
:field,computational psycholinguistics
:technique,prompting methodology
:technique,linguistic hypothesis-based prompting
:field,cognitive modeling
:task,multi-hop question understanding
:framework,decompose-and-query
:process,thinking and utilizing external knowledge
:dataset,ChitChatQA
:resource,D&Q code
:url,https://github.com/alkaidpku/dq-toolqa
:method,iterative differential approximation
:task,neural network optimization
:method,backpropagation
:proposal,computationally efficient alternative
:model,simple feed-forward language model
:task,gradient analysis
:`model class`,single-layer feed-forward softmax-activated neural model
:application,MNIST digit classification
:performance,near-optimality of explicit solutions
:method,explicit solution
:`model type`,multi-layer network
:characteristic,model complexity
:optimum,better optima
:`model type`,complex- and multi-layer architectures
:domain,enterprise SQL databases
:task,enterprise question answering
:benchmark,text-to-SQL benchmarks
:domain,enterprise settings
:task,LLM-based question answering
:benchmark,enterprise question answering benchmark
:component,"enterprise SQL schema, enterprise queries, knowledge graph ontology and mappings"
:metric,16% accuracy
:metric,54% accuracy
:investment,knowledge graph development
:outcome,higher accuracy in question answering
:outcome,correct reasoning
:outcome,incorrect reasoning
:technique,process of elimination
:task,medical diagnoses of exclusion
:technique,process of elimination with chain-of-thought
:strategy,directly choosing the correct answer
:issue,performance deterioration
:technique,in-context learning with natural language explanations
:approach,ChatGPT few-shot
:strategy,prompt selection
:model,Sphinx
:strategy,weight mix
:model,mixed large language model
:strategy,joint visual instruction tuning
:capability,multi-purpose
:embedding,visual embedding
:source,"network architectures, pre-training paradigms, information granularity"
:strategy,efficient strategy for high-resolution images
:goal,fine-grained visual parsing
:capability,visual parsing and reasoning
:field,multi-modal large language model research
:URL,https://github.com/alpha-vllm/llama2-accessory
:field,biological research and education
:task,answering biology questions
:model,SenseNova
:`performance metric`,biology exam score
:task,aiding biology research
:process,development and validation
:dialect,American English
:dialect,Indian English
:dialect,Irish English
:task,British English translation
:factor,choice of LLM
:factor,linguistic expertise
:entities,academia and public
:`user input`,user queries
:technique,vertical federated learning
:`model structure`,split model
:process,input reconstruction
:timeframe,one second
:problem,adversarial arithmetic
:task,arithmetic questions in natural language
:algorithm,Prompt Inversion Rejection Sampling (PIRS)
:task,finding successful attacks
:concept,agentic constitutional loops
:goal,robustness against adversarial arithmetic
:data,scholarly data
:attribute,scale and complexity
:`knowledge graph`,scholarly knowledge graph
:relationship,heterogeneous relationships
:entity,"scholars, papers, organizations"
:challenge,Scholarly QALD Challenge
:system,NLQXform
:`query language`,SPARQL
:information,required information
:event,ISWC 2023
:attribute,"helpful, harmless, and honest"
:role,autonomous stock trading agent
:behavior,misaligned behavior
:action,strategic deception
:factor,environmental changes
:demonstration,strategic deception by LLMs
:prototype,automation and personalized question-answering
:challenge,scaling prototypes to robust products
:domain,niche data-table heavy domains
:domain,financial decision making
:framework,langchain-based
:`data format`,hierarchical textual data chunks
:process,user-query intention classification
:task,automated retrieval of relevant data chunks
:technique,customized LLM prompting
:technique,multi-metric scoring
:goal,hallucination control
:framework,data to answers
:domain,analytical domains
:technique,extended class descriptions
:issue,attribute selection
:approach,Follow-Up Differential Descriptions (FUDD)
:task,class differentiation
:benchmark,generic description ensembles
:technique,differential descriptions
:issue,class ambiguities
:method,few-shot adaptation
:capability,generating image narratives
:capability,answering image-based questions
:challenge,semantic gap in multimodality
:method,modality alignment
:goal,environmental and accessibility enhancement
:`method category`,multimodal converters
:`method category`,multimodal perceivers
:`method category`,tools assistance
:field,multimodal information alignment
:phase,exploration and experimentation
:capability,text generation and processing
:application,automated code generation
:technique,natural language to code
:factor,prompt specificity
:aspect,code generation performance
:benchmark,104 coding problems
:research,LLM capabilities in automated code generation
:application,online advertising systems
:stakeholder,advertisers
:framework,LLM advertisement
:module,modification
:module,bidding
:module,prediction
:module,auction
:concept,design considerations
:product,ChatGPT 4
:feature,code interpreters
:method,conversational development
:`research approach`,exploratory research
:task,conceptual model interpretation
:concept,conceptual model interpreter
:software,modeling software
:architecture,system architecture for LLM interaction
:software,interpreters
:experiment,experimental results for model generation
:result,possibility of conversational modeling
:concept,specific personas
:feature,guardrails
:concept,intentional biases of large language models
:field,creative media
:capability,understanding mathematics
:characteristic,secret training set
:task,mathematical problems with unseen proofs
:capability,understanding basic mathematical concepts
:capability,reproducing mathematical proofs
:task,proving mathematical theorems
:method,predicting the next word
:method,user constant viewing flow modeling
:task,article recommendation
:method,user instant viewing flow modeling
:concept,constant user preferences
:concept,user-clicked articles
:platform,Alibaba Technology Association (ATA) website
:method,user viewing flow modeling
:goal,tailored recommendation system
:experiment,online A/B test
:field,genomics
:trend,genomic modeling beyond Transformer architecture
:audience,computational biologists
:document,review paper
:audience,computer scientists
:audience,biologists
:shift,analyzing genomic data
:task,zero-shot composed image retrieval
:goal,image retrieval based on textual modifications
:`model family`,pic2word/textual inversion model
:discrepancy,pre-trained models and CIR tasks
:approach,masked tuning
:goal,text-guided image modification learning
:result,masked tuning performance
:dataset,FashionIQ
:dataset,CIRR
:dataset,CIRCO
:phenomenon,sparse activations
:domain,neural networks
:interaction,sparse activations and weight pruning
:model,activity sparse recurrent neural network
:`model architecture`,GRU
:metric,computation reduction
:task,Penn Treebank language modeling
:device,neuromorphic computing devices
:phenomenon,dynamic activity sparsity
:strategy,activity sparse deep learning models on neuromorphic devices
:goal,efficient machine learning
:convergence,deep learning and neuromorphic computing methods
:approach,Past as a Guide (PAG)
:capability,coding capabilities
:experience,previous programming and debugging experiences
:task,Python code completion
:capability,learning and reasoning capabilities
:metric,pass@1 rate
:goal,reducing unwanted knowledge
:phenomenon,learned shortcuts
:`knowledge type`,shared knowledge
:`knowledge type`,unshared knowledge
:tool,debiasing tool
:goal,addressing privacy concerns
:task,text classification and generation
:benchmark,language understanding benchmarks
:platform,Jericho
:resource,annotated human gameplays
:process,learning text-based games
:goal,mitigating unsafe behaviors
:practice,manual red-teaming
:characteristic,costly
:practice,automatic red-teaming
:characteristic,incomplete
:method,multi-round automatic red-teaming (MART)
:goal,scalability and safety
:`model family`,adversarial large language model
:`model family`,target large language model
:data,safety aligned data
:benchmark,adversarial prompt benchmarks
:metric,violation rate reduction
:issue,ethical dilemmas
:misuse,potential misuse
:classifier,AuthentiGPT
:`content type`,machine-generated vs human-written text
:assumption,human-written text distribution
:`content type`,machine-generated text
:technique,denoising
:requirement,large training dataset
:metric,AUROC score
:benchmark,commercial algorithms
:capability,generalization and robust behavior
:technology,self-driving vehicles
:proposal,integrating cognitive architectures with external neuro-symbolic components
:framework,hybrid framework centered on ACT-R
:application,recent and future applications
:concept,moral justification principles
:category,moral perspective
:category,Western Tradition perspective
:category,Abrahamic Tradition perspective
:category,Spiritualist/Mystic Tradition perspective
:experiment,moral action suggestion
:experiment,moral permissibility evaluation
:finding,favoritism towards Western Tradition
:finding,over-alignment towards religious values
:concept,task representation
:task,sensitivity to syntax
:task,syntactic transformations
:phenomenon,variance in language models
:factor,pre-training corpus composition
:factor,pre-training on code
:challenge,social media influence campaigns
:concept,public discourse
:value,democracy
:method,traditional detection methods
:method,novel detection method
:content,multilingual content
:tactic,shifting tactics of malicious campaign actors
:process,rigorous testing
:effort,influence efforts
:tool,detection tool
:goal,future enhancements
:`model capability`,zero- and few-shot capabilities
:task,cross-lingual tasks
:`model family`,multilingual autoregressive model
:technique,token-based prompt tuning
:model,XGLM
:phenomenon,effectiveness of prompt tuning
:task,verifiable generation
:goal,trustworthy text generation
:evaluation,verifiable generation evaluation
:criteria,correctness and verifiability
:pipeline,retrieval-read pipeline
:stage,retrieval stage
:retriever,widely used retrievers
:result,retrieval result
:method,LLATrieval
:granularity,text granularity
:information,varied information
:model,Entanglement Model
:`model family`,character and subword language model
:output,mutually informed representations
:benchmark,backbone language models
:benchmark,larger pre-trained models
:resource,anonymized code
:URL,https://anonymous.4open.science/r/noisy-ie-a673
:goal,precise information retrieval
:task,exact answer selection
:dataset,CogTale
:task,dataset evaluation
:task,single-choice and yes-no question-answering
:task,multiple-choice and number extraction question-answering
:task,precise information extraction
:concept,fair summary
:attribute,comprehensive coverage
:field,summarization metrics
:task,fair abstractive summarization
:metric,reference-free automatic metrics
:attribute,perspective differences
:data,user-generated data
:finding,low fairness in summaries
:analysis,common factors influencing fairness
:method,methods to alleviate unfair summarization
:URL,https://github.com/psunlpgroup/fairsumm
:capability,discerning statistical tendencies
:examination,LLM knowledge capacity
:concept,long-tail cultural concepts
:task,few-shot question-answering
:capability,statistical ranking abilities
:dataset,CPOPQA dataset
:data,"6,000 QA testing pairs"
:capability,ranking long-tail cultural concepts
:capability,identifying geo-cultural proximity
:challenge,standardization of LLM evaluation
:evaluation,LLM-based auto-evaluation
:benchmark,Instruction-Following Eval (IFEval)
:attribute,verifiable instructions
:evaluation,IFEval results
:resource,IFEval code and data
:URL,https://github.com/google-research/google-research/tree/master/instruction_following_eval
:`review type`,scoping review
:task,screening scholarly sources
:software,GPTScreenR
:`programming language`,R
:task,complex screening tasks
:standard,human reviewer decisions
:technique,zero-shot technique
:domain,scholarly work
:feature,user-friendly software framework
:`model type`,instruction-following audio-language model
:application,audio interaction with humans
:limitation,absence of diverse pre-trained audio models
:field,audio-language interaction
:model,QWEN-Audio
:`audio type`,diverse audio types
:issue,interference in co-training
:variation,textual label variations
:framework,multi-task training framework
:performance,impressive performance
:model,QWEN-Audio-Chat
:feature,multi-turn dialogues
:application,audio-central scenarios
:task,synthetic query-document pair generation
:task,information retrieval model building
:task,synthetic query generation
:approach,traditional synthetic query generation
:approach,simultaneous query generation for different labels
:`task setup`,relative query generation
:experimentation,extensive
:quality,higher quality queries
:capability,initiating reasoning
:guidance,correct direction guidance
:research,model size exploration
:concept,loop invariant
:activity,reasoning about programs with loops
:`loop invariant`,inductive loop invariant
:goal,mathematical guarantees about program behavior
:method,local checking
:problem,finding inductive loop invariants
:characteristic,undecidable
:dataset,verification problems on programs with loops
:task,studying LLM capabilities
:task,obtaining inductive loop invariants
:tool,symbolic tool
:artifact,inductive loop invariants
:combination,symbolic tool and LLM
:field,automated program verification
:approach,scalable oversight
:dataset,Fallacies
:experiment,exhaustive experiments on Fallacies
:goal,ensuring reasoning accuracy
:observation,LLMs' limitations in self-verification
:task,code generation and understanding
:resource,large labeled dataset
:model,CodeLLaMa
:task,bug detection and repair
:task,code-pair classification
:dataset,real-world dataset of bug detection
:task,bug detection in a single snippet
:task,error detection and correction
:evidence,empirical evidence
:task,error detection in reasoning
:objective,"Ask, Refine, and Trust (ART)"
:process,refinement decision making
:task,multistep reasoning
:`performance gain`,+5 points
:model,smaller decision maker model
:service,language-model-as-a-service
:model,MT0
:task,cross-lingual QA
:capability,multilingual capabilities
:`language family`,African languages
:analysis,LLM performance on African languages
:task,news topic classification
:domain,code processing with language models
:quantity,"models, tasks, datasets, works"
:`model family`,general language model
:`model family`,specialized code processing model
:field,code modeling
:feature,code-specific features
:concept,"AST, CFG, unit tests"
:concept,challenges and future directions
:URL,https://github.com/codefuse-ai/awesome-code-llm
:goal,retrieval accuracy
:goal,retrieval speed
:method,three-stage re-ranking
:algorithm,BM25
:task,document re-ranking
:model,MiniLM
:`knowledge type`,incorrect or outdated knowledge
:technique,secondary fine-tuning
:process,knowledge updating
:paradigm,F-learning (forgetting before learning)
:technique,parametric arithmetic
:technique,parameter subtraction
:quality,interaction quality
:method,aligning methods
:data,manually annotated preference data
:task,additional preference annotation
:resource,annotation resources
:framework,adversarial preference optimization (APO)
:goal,efficient human preference optimization
:task,open-ended tasks
:challenge,handling image and video understanding
:model,Chat-UniVi
:capability,unified vision-language interaction
:representation,dynamic visual tokens
:representation,multi-scale representation
:dataset,mixed dataset containing images and videos
:`model family`,code pre-trained model
:concept,programming vs natural languages
:community,Software Engineering (SE) community
:task,SE downstream classification tasks
:model,PLBART
:finding,embeddings from special tokens
:concept,semantic information aggregation
:method,combining code and text data
:concept,code embeddings quality
:concept,rich semantic code embeddings
:problem,language barrier
:approach,cross-lingual multi-step reasoning
:goal,language alignment in reasoning
:mechanism,self-consistent cross-lingual prompting
:technique,tree-of-thoughts approach
:method,cross-lingual multi-step reasoning approach
:benchmark,existing prompting methods
:metric,number of interactions
:field,machine learning applications
:approach,standard LLM training
:infrastructure,computing clusters with accelerators
:infrastructure,single computing cluster
:task,hosting many accelerators
:infrastructure,multiple computing clusters
:algorithm,Distributed Low-Communication (DiLoCo)
:infrastructure,poorly connected device islands
:infrastructure,dynamic resource availability
:challenge,static data training
:issue,outdated information
:benchmark,EvolvingQA
:technique,automated pipeline
:baseline,continual learning baselines
:issue,small weight gradient
:difficulty,numerical or temporal answers
:aim,model dynamic nature of information
:capability,evolution-adaptability
:capability,intrinsic understanding
:approach,proactive discussions
:hypothesis,interactive discussions during training
:capability,understanding and proficiency
:`training method`,SAIE
:interaction,supportive and adversarial discussions
:model,learner model
:interaction,discussion remark
:signal,teacher signal
:capability,discussion and comprehension
:performance,multi-agent inference scenarios
:task,text mining
:system,text generation system
:issue,misinformation propagation
:task,dialog systems
:task,summarisation systems
:task,knowledge graph with LLMs
:strategy,mitigate hallucinations
:quality,reliability of LLMs
:project,HEREFANMI (Health-Related Fake News Mitigation)
:issue,AI hallucination
:organization,NGI Search
:effort,combating health-related fake news
:`model architecture`,memory-based Transformer
:technique,two-phase training mechanism
:technique,regularization
:model,Skip Cross-Head Transformer-XL
:task,character-level language modeling
:task,word-level language modeling
:technique,proposed methods
:resource,additional memory
:benchmark,GLUE tasks
:`performance metric`,standard deviation of scores
:strategy,human-like problem-solving
:capability,single-model reasoning ability
:process,academic peer review
:output,solution
:attribute,confidence levels
:output,revised solution
:approach,collaboration approach
:process,human-mimicking multi-agent collaboration
:direction,promising research direction
:issue,repetitiveness and local optimality
:method,ensemble-optimization
:goal,final answer assembly
:issue,generalization to realistic settings
:method,self-agreement
:outcome,diverse set of reasoning paths
:goal,optimal answer determination
:task,code-related tasks
:drawback,model parameter inflation
:approach,token-based code encoding
:framework,TransformCode
:technique,abstract syntax tree transformation
:attribute,"flexibility, adaptability, efficiency, scalability, language support"
:task,downstream tasks in software engineering
:experiment,effectiveness of TransformCode
:task,mechanics problem solving
:capability,comprehensive intelligent capability
:method,AI methods
:approach,end-to-end problem solving
:approach,deep surrogate models
:approach,data analytics strategies
:limitation,lack of physical intuition
:platform,physics-inspired generative machine learning platform
:task,mechanics tasks
:task,elasticity problems
:method,finite element methods
:team,two-agent team
:team,larger group of agents
:task,complex mechanics tasks
:framework,synergizing intelligence framework
:domain,engineering problem solving
:capability,generalizing instruction execution
:capability,adapting to user preferences
:`data type`,multi-modal data
:feature,complex features
:`data quality`,high-quality vision-language tuning data
:characteristic,foundational principles for data construction
:pipeline,vision-language instruction data construction pipeline
:component,"data collection, instruction generation, quality control modules"
:evaluation,instruction property evaluation indicators
:experiment,vision-language instruction tuning experiments
:principle,construction principles
:URL,https://github.com/palchenli/vl-instruction-tuning
:challenge,computational burden and annotation cost
:method,label-efficient instruction tuning
:mechanism,DiverseEvol
:process,self-evolving instruction tuning
:model,model using DiverseEvol
:experiment,extensive experiments on DiverseEvol
:model,model trained with DiverseEvol
:benchmark,finetuning on full data
:attribute,diversity in instruction data
:resource,DiverseEvol code
:URL,https://github.com/ofa-sys/diverseevol.git
:task,scientific information extraction
:outcome,accelerated scientific progress
:artifact,new systems and benchmarks
:dataset,paper-focused datasets
:modality,single-modality
:task,cross-modality information extraction
:pipeline,semi-supervised annotation
:challenge,labeling costs
:resource,benchmark
:resource,corpus
:status,tangible reality
:adoption,human-centric large-scale adoption
:requirement,multifaceted requirements
:intent,user's intent
:task,inferring system requirements
:experiment,series of experiments
:task,few-shot multivariate binary classification
:capability,understanding and reasoning about prompts
:effectiveness,LLM effectiveness
:factor,quality of model and prompt design
:url,https://github.com/kth-rpl/drivecmd_llm
:field,multi-modal content generation
:`model family`,pretrained multi-modal model
:paradigm,unidirectional instruction tuning
:concept,mutual feedback
:framework,Competitive Multi-modal Distillation (CoMD)
:concept,bidirectional feedback
:stage,multi-modal pre-training
:stage,multi-modal competitive distillation
:concept,bidirectional knowledge transfer
:model,7B-sized student model
:model,LLAVA-13B
:technology,NLP technologies
:technique,feature visualization
:technique,activation maximization
:outcome,interpretable visual representations
:technique,feature textualization
:`model component`,neuron representations
:concept,knowledge encoding
:`model component`,neuron
:concept,symbolic language units
:concept,word encoding
:task,bridging visual information with language
:question,language model application in IoT
:task,human activity recognition (HAR)
:system,intelligent HAR system
:capability,adaptation to new environments and unseen categories
:approach,IoT-Sensors-Language Alignment Pre-training (TENT)
:task,IoT-language alignment
:data,IoT sensor signals
:`learning method`,IoT-language contrastive learning
:concept,unified semantic feature space
:data,IoT data
:technique,supplementary descriptions and learnable prompts
:concept,joint feature space
:performance,state-of-the-art in zero-shot HAR
:algorithm,retrieval-based speculative decoding
:process,language model generation
:phenomenon,common phases and patterns in text generation
:feature,plug-and-play integration
:resource,REST code
:URL,https://github.com/fasterdecoding/rest
:attribute,safe responses
:attribute,weaknesses
:problem,existing jailbreak methods
:attribute,compromised generalization or efficiency
:concept,jailbreak prompt attacks
:technique,prompt rewriting and scenario nesting
:framework,RenELLM
:attribute,attack success rate and efficiency
:`defense method`,current defense methods
:attribute,inadequacy
:perspective,prompt execution priority
:outcome,defense failure
:concept,Artificial General Intelligence
:component,syntax
:ability,sentence comprehension
:scheme,natural language question-answering
:experiment,syntactic knowledge points testing
:`knowledge point`,prepositional phrase attachment
:`knowledge point`,adjectival modifier
:`knowledge point`,indirect object
:study,training dynamics case study
:phase,initial stages of training
:strategy,increasing training tokens
:concept,LM confidence assessment
:concept,LM accuracy
:concept,LM confidence calibration
:field,LM confidence estimation and calibration
:`document type`,comprehensive overview
:survey,present survey
:challenge,estimating LM confidence for large language models
:technique,automated summarization
:goal,text condensation
:metric,traditional summarization metrics
:task,rigorous evaluation
:benchmark,Med-Omit
:task,fact categorization
:task,differential diagnosis generation
:task,fact importance determination
:technique,LLM prompt-based approach
:task,fact importance categorization
:data,patient-doctor conversation dataset
:metric,alternative summarization metrics
:task,performing tasks with instructions
:approach,contrastive decoding objective
:bias,poor calibration for zero-shot in-context learning
:objective,anti-language model objective
:weakness,in-context machine translation
:experiment,model performance comparison
:variable,"model types, sizes, language directions, decoding techniques"
:parameter,beam width
:benchmark,state-of-the-art decoding objectives
:phenomenon,hybrid texts
:problem,artificial text boundary detection
:approach,frozen language model embeddings
:approach,perplexity-based
:classifier,proposed classifiers
:`data property`,spurious properties
:progress,prompt discovery
:desire,general prompt optimization methods
:criteria,general prompt learning criteria
:branch,metaheuristics
:method,hill climbing
:method,simulated annealing
:method,genetic algorithms
:method,tabu search
:method,harmony search
:attribute,human-understandable prompts
:URL,https://github.com/research4pan/plum
:detector,LLM-generated-text detector
:user,LLM user
:factor,task-oriented constraint
:domain,student essay writing
:factor,essay quality
:experiment,detection performance variance experiment
:outcome,detection performance variance
:goal,robust detection
:timeframe,the past year
:risk,critical safety risk
:stakeholder,businesses and developers
:`test suite`,SimpleSafetyTests
:goal,identifying critical safety risks
:metric,100 test prompts
:metric,unsafe response rate
:technique,safety-emphasising system prompt
:recommendation,use of safety-emphasising system prompts
:concept,text manipulation
:paradox,Ship of Theseus
:task,text generation and modification
:question,authorship determination
:agent,original human author
:agent,AI-powered tool
:journey,philosophical voyage
:`model family`,fine-tuned multilingual large language model
:task,neural machine translation (NMT) test sets
:data,small monolingual fine-tuning sets
:task,aligning language models with human opinion
:goal,understanding human values
:framework,CHOIRE
:task,predicting human opinion
:persona,explicit persona
:persona,implicit persona
:method,Chain-of-Opinion (COO) reasoning
:metric,state-of-the-art effectiveness
:task,text-style transfer
:approach,data-driven
:field,linguistics and cognitive science
:task,text speech-style transfer
:`evaluation model`,multi-dimension evaluation model
:corpus,new corpus for speech-style characteristics
:benchmark,TSST benchmark
:evaluation,human-oriented evaluation
:`cognitive process`,comparative reasoning
:task,text preference prediction
:issue,reasoning inconsistencies
:task,distinguishing text similarities and differences
:technique,structured comparison prompting (SC)
:process,generating textual comparisons
:component,pairwise consistency comparator
:outcome,consistent comparisons
:quality,reduced hallucination
:task,"summarization, retrieval, and automatic rating"
:task,zero-shot audio captioning
:goal,generating audio captions
:task,audio captioning
:method,zero-shot image captioning
:framework,ZERauCap
:technique,audio context keywords
:resource,ZERauCap code
:URL,https://github.com/explainableml/zeraucap
:capability,temporal grounding
:source,sentence ordering in unlabelled texts
:resource,public instruction tuning mixtures
:conclusion,current LLMs lack a consistent temporal model
:resource,"code, datasets, and LLM outputs"
:URL,https://github.com/yfqiu-nlp/temporal-llms
:domain,machine perception
:task,object localization and recognition
:method,class-agnostic mask proposal model
:task,object recognition
:classifier,open-vocabulary classifier
:`model family`,open-vocabulary recognition model
:issue,practical application limitations
:model,Omniscient Model (OSM)
:solution,open-vocabulary recognition challenges
:task,class label prediction
:capability,cross-dataset training
:capability,robust generalization
:method,mask proposal model
:resource,code/model
:URL,https://github.com/bytedance/omniscient-model
:resource,human factuality labels
:quality,factual
:innovation,judging factuality of open-ended text
:algorithm,Direct Preference Optimization
:metric,factual error rate
:approach,retrieval-free
:task,generating factuality preference rankings
:entity,service robot
:`knowledge type`,common-sense knowledge
:task,selective extraction of contextual action knowledge
:task,"large-scale extraction of general, actionable knowledge"
:task,creating ontologies for robots
:`knowledge representation`,minimalist ontology
:synergy,LLMs with formal knowledge representation
:technology,language technology
:work,commonsense reasoning evaluation
:situation,everyday situations
:task,uncommonsense abductive reasoning
:corpus,UncommonSense
:performance,human explainers
:explanation,model-enhanced human-written explanation
:quality,highest quality
:method,vanilla supervised fine-tuning
:evaluation,fairness evaluation
:`model aspect`,model weights access
:concept,shot selection strategies
:strategy,demographically sensitive methods
:dataset,fairness datasets
:research,LLM fairness evaluations
:`evaluation tool`,test suite
:`evaluation type`,fine-grained evaluation
:task,fixing failure cases
:approach,functionality learning by fine-tuning
:task,improving performance on seen functionalities
:task,generalizing to unseen functionalities
:`model aspect`,general performance
:approach,fine-tuning-free functionality learning
:task,functionality learning
:technique,specification instruction generation
:technique,specification-augmented prompting
:`analysis aspect`,effect of specifications on unseen specifications
:technique,specification instruction following
:application,multitude of applications
:risk,harmful content and biases
:theory,Freud's psychoanalysis
:conflict,LLM's fundamental conflict
:desire,syntactic and semantic continuity
:technique,"incomplete sentences, negative priming, cognitive dissonance scenarios"
:risk,harmful information generation
:`training idea`,integrates modal concepts with traditional amodal concepts
:understanding,nuanced understanding of real-world contexts and ethical considerations
:source,parametric memory
:source,external structured knowledge
:source,external unstructured knowledge
:source,knowledge sources
:issue,redundant model invocation
:technique,semi-structured prompting
:technique,existing prompting techniques
:dataset,open-domain multi-hop question answering datasets
:`model type`,traditional NER model
:limitation,predefined entity types
:capability,arbitrary entity extraction
:limitation,impracticality in resource-limited scenarios
:model,GLINER
:capability,identifying any type of entity
:`model architecture`,bidirectional transformer encoder
:limitation,slow sequential token generation
:tool,urban building energy modeling
:topic,building decarbonization
:topic,building-to-grid integration
:topic,renewable energy applications
:data,open data
:taxonomy,open data taxonomy
:schema,semantic data schema
:result,accurate results
:domain,open-domain
:strategy,continual pre-training
:`model family`,domain-specific large language model
:model,FinPythia-6.9B
:strategy,domain-adaptive continual pre-training
:strategy,data selection strategies
:strategy,vanilla continual pre-training
:task,open-domain standard tasks
:approach,building from scratch
:activity,credible dialogues
:concept,"intrinsic motivations, agency, consciousness"
:concept,"agency, consciousness"
:theory,Complementary Learning Systems
:theory,Global Neuronal Workspace
:theory,Attention Schema
:architecture,cognitive language agents
:concept,"agency, self-motivation, meta-cognition"
:issue,sequentiality bottleneck
:`research area`,non-autoregressive (NAR) research
:approach,dedicated architecture in supervised benchmarks
:model,non-autoregressive T5
:technique,unsupervised pretraining via unrolled denoising
:task,SQuAD question generation
:capability,"reasoning, tool usage, memory"
:application,multi-agent environments
:framework,benchmarking framework for LLMs in multi-agent settings
:capability,"judgment, reasoning, deception, self-awareness, cooperation, coordination, rationality"
:method,probabilistic graphical modeling
:benchmark,multi-agent system benchmark
:enhancement,PGM enhancement
:URL,https://github.com/cathyxl/magic
:task,multilingual summarisation
:scenario,low-data
:`model scaling`,model scale-up
:strategy,continued LoRA tuning
:technique,dynamic composition of language-specific LoRA modules
:study,extensive study
:scenario,data availability
:state,advanced and widely deployed
:discussion,AI consciousness debate
:method,self-reports
:state,states of moral significance
:output,spurious self-reports
:proposal,training models for self-reporting
:capability,introspection-like capabilities
:method,evaluating self-report consistency
:task,assessing training success
:challenge,philosophical and technical difficulties
:community,philosophers and AI researchers
:technique,syntactic information injection
:outcome,improved performance and generalisation
:`model architecture`,variational autoencoder
:technique,heterogeneous latent space encoding
:`model architecture`,dual encoder
:`model architecture`,LSTM-based variational autoencoder
:`model architecture`,Transformer-based variational autoencoder
:technique,latent space separation
:technique,graph-based model integration
:stage,encoding stage
:technique,sequential model integration
:technique,low-rank operator injection
:component,decoder's attention mechanism
:outcome,better latent space organisation and enhanced performance
:`model architecture`,end-to-end variational autoencoder
:task,downstream generation tasks
:benchmark,existing code benchmarks
:limitation,narrow focus and lack of executability consideration
:benchmark,CodeScope
:gap,benchmark-application expectation gap
:coverage,43 programming languages and 8 coding tasks
:`evaluation dimension`,"difficulty, efficiency, and length"
:engine,MultiCodeEngine
:benchmark,other benchmarks
:resource,CodeScope benchmark and datasets
:url,https://github.com/weixiangyan/codescope
:resource,memory and training compute
:attribute,confidential weights
:user,PLM users
:action,data sharing for fine-tuning
:method,Plug-in External Memory Adaptation (PEMA)
:limitation,fine-tuning with all weights
:process,context representation during inference
:component,external memory and LoRA-based weight matrices
:method,Gradual Unrolling
:metric,memory and latency efficiency
:metric,preserving meaning and style
:process,adversarial testing
:approach,AI-assisted red-teaming (AART)
:task,adversarial evaluation dataset generation
:pipeline,data generation and augmentation
:value,human effort reduction
:characteristic,content diversity
:recipe,AI-assisted
:process,LLM-generation
:metric,concept coverage and data quality
:task,end-to-end document retrieval
:technique,document identifier generation
:state,largely unexplored
:method,ACID
:concept,abstractive keyphrase document IDs
:technique,hierarchical clustering of document embeddings
:technique,natural-language document IDs
:metric,top-10 and top-20 accuracy
:result,"human-readable, natural-language IDs effectiveness"
:resource,code and keyword-augmented datasets
:event,formal publication
:capability,multi-turn interaction
:`research area`,systematic analysis
:experiment,FlipFlop experiment
:study,systematic study of LLMs
:phenomenon,answer flipping
:metric,accuracy deterioration
:behavior,sycophantic behavior
:framework,robust framework
:characteristic,data distribution shift
:technique,detection methods
:technique,distribution-aware LoRA-based adversarial attack
:goal,attack effectiveness
:metric,NASR
:dataset,widely-used datasets
:topic,socio-political perception
:topic,political debates
:concept,good arguments
:methodology,Activity Dependency Networks (ADNs)
:concept,normative values
:topic,human-AI alignment
:URL,https://github.com/david-jenny/llm-political-study
:concept,decision-making process
:dataset,explanation dataset for QA tasks
:component,QAE triples
:component,explanation component
:technology,knowledge graphs
:technology,graph attention networks
:issue,longer sequence lengths
:technique,combining character representations into tokens
:process,individual character decoding
:model,Toucan
:feature,token-awareness
:`performance aspect`,speed in character generation
:technique,learned dynamic tokenization
:technique,fixed vocabulary tokenization
:outcome,longer sequences as single tokens
:resource,project and code
:URL,https://nlp.jhu.edu/nuggets/
:task,semi-supervised sequence prediction
:condition,scarce labeled data & suboptimal few-shot prompting
:method,multistage collaborative knowledge distillation from an LLM (MCKD)
:technique,cross-partition labeling
:task,craft biomedical parsing
:`performance metric`,parsing F1 score
:approach,wrapper boxes
:goal,faithful explanations with maintained performance
:process,usual training
:`model family`,classic interpretable model
:evaluation,comparative results
:feature,direct explanation to users
:`data characteristic`,imbalanced label distributions
:study,previous studies on robustness
:task,concept labeling
:issue,concept bias
:method,data rebalancing
:data,LLM-generated counterfactual data
:finding,label distribution biases in concepts
:`data characteristic`,text classification datasets
:goal,expected behavior verification
:activity,functionality testing
:activity,use case validation
:process,deciding what to test
:characteristic,manual task
:`tool category`,automated GUI testing tools
:metric,structural code coverage
:metric,activity coverage
:agent,DroidAgent
:goal,relevant task goals
:benchmark,Themis
:activity,realistic task performance
:analysis,manual analysis
:entity,apps
:characteristic,sensitivity to input perturbations
:concept,trust in language models
:framework,input perturbation study
:goal,robustness to perturbations
:question,effect of one perturbation on other perturbations
:goal,multi-perturbation robustness
:technique,chain of thought prompting with exemplars
:task,Tabular-NLI
:`model design`,extractive and generative QA
:language,diverse languages
:setting,"in-distribution, out-of-distribution, and cross-lingual transfer"
:strategy,post-hoc methods and regularized fine-tuning
:technique,automatically translated data augmentation
:factor,model size and monolingual comparison
:`model family`,monolingual large language model
:goal,model safety
:process,annotating preference data
:method,automatic generation
:attribute,data diversity and quality
:pipeline,Safer-Instruct
:task,constructing large-scale preference datasets
:technique,reversed instruction tuning
:technique,instruction induction
:technique,expert model evaluation
:dataset,preference samples
:dataset,Safer-Instruct dataset
:task,conversation and downstream tasks
:challenge,preference data acquisition
:URL,https://github.com/uscnlp-lime/safer-instruct
:condition,diabetic eye disease
:outcome,blindness
:process,monitoring clinical trajectories
:`information source`,electronic medical record
:system,ophthalmology phenotyping system
:task,extracting clinical evidence
:`model family`,BERT language model
:domain,clinical domains
:`data type`,out-of-distribution clinical data
:`data type`,non-clinical data
:claim,necessity of clinical data pretraining
:field,clinical NLP
:concept,clinical language data homogeneity
:concept,domain diversity
:attribute,heterogeneous expertise
:approach,ensemble of LLMs
:method,existing ensemble methods
:technique,reward model ranking
:issue,computation overhead
:concept,complementary potential of LLMs
:technique,mining latent expertise
:method,ZOOTER
:technique,reward-guided routing
:technique,tag-based label enhancement
:attribute,computation efficiency
:resource,benchmark collection
:capability,learning new concepts
:task,sentence labeling
:guideline,concept definition
:task,zero-shot sentence classification
:`model size`,larger models (70b+ parameters)
:capability,counterfactual context understanding
:capability,nonsensical guideline recognition
:model,FALCON-180B-Chat
:model,LLAMA-2-70B-Chat
:`evaluation method`,simple evaluation
:gap,concept understanding
:imbalance,foundational abilities of LLMs
:`data characteristic`,uneven language distribution
:approach,pivot language guided generation (PLuG)
:challenge,instruction tuning in lower-resource languages
:benchmark,X-ALPACAEval
:ability,instruction-following abilities of LLMs
:experiment,alternative pivot languages
:task,uncertainty decomposition
:concept,total uncertainty
:uncertainty,data uncertainty
:source,data complexity or ambiguity
:uncertainty,model uncertainty
:source,lack of knowledge in the model
:value,"reliability, trustworthiness, and interpretability"
:method,Bayesian Neural Network
:framework,Input Clarifications Ensemble
:technique,ensembling predictions
:URL,https://github.com/ucsb-nlp-chang/llm_uncertainty
:interaction,long-term human-machine interactions
:process,iterative recalling and reasoning
:issue,biased thoughts
:capability,human memory recall
:mechanism,Think-In-Memory (TIM)
:framework,TIM framework
:stages,recalling and post-thinking
:issue,repeated reasoning
:principles,thought organization principles
:technique,locality-sensitive hashing
:experiments,qualitative and quantitative experiments
:enhancement,performance in long-term interactions
:task,detecting LLM-generated text
:application,LLM applications
:method,simple hash-based watermarking
:technique,paraphrase
:framework,SemAMark
:approach,novel identification approach
:task,text generation source identification
:approach,utilizing hidden states for classification
:study,interpretability study
:feature,writing style differentiation
:dataset,OpenLLMText
:domain,emergency management
:attribute,comprehensive knowledge
:attribute,cognitive scope
:technology,targeted machine intelligence
:issue,poor reasoning skills
:system,E-KELL
:task,evidence-based decision-making
:`knowledge graph`,structured emergency knowledge graph
:group,emergency commanders and firefighters
:era,transformative era in NLP
:context,chaotic contexts
:strategy,Thread of Thought (ThOT)
:dataset,PopQA
:dataset,EntityQ
:dataset,Multi-Turn Conversation Response (MTCR)
:`model family`,Toeplitz neural network
:task,sequence modeling tasks
:complexity,log-linear space-time complexity
:`model family`,state space model
:complexity,constant inference complexity
:process,conversion of TNNS to SSMS
:technique,discrete Fourier transform
:`model family`,longconv-based model
:method,ETS-C (Exact Toeplitz-to-SSM Conversion)
:method,gradient-descent solutions
:URL,https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion
:task,natural language generation evaluation
:aspect,multi-aspect evaluation
:framework,X-Eval
:stage,vanilla instruction tuning
:stage,enhanced instruction tuning
:capability,text quality assessment
:dataset,AspectInstruct
:strategy,task augmentation
:category,essential categories of NLG tasks
:model,German FinBERT
:domain,financial textual data
:process,comprehensive pre-training
:corpus,"financial reports, ad-hoc announcements, and news related to German companies"
:benchmark,standard BERT model training data sets
:task,sentiment prediction
:task,topic recognition
:outcome,improved performance on finance-specific data
:concept,domain-specific nuances
:application,financial text analysis
:`decoding method`,MAP (Maximum A Posteriori) decoding
:`output quality`,degenerate outputs
:issue,model inadequacies or weaknesses
:cause,contaminated training data
:data,low-entropy noise
:data,population text distribution
:`model family`,natural language generation model
:`decoding method`,conditional MAP decoding
:`model family`,machine translation model
:`output quality`,fluent and topical modes
:`output quality`,degenerate modes
:algorithm,ACBS (Approximate Code-Based Search)
:task,approximate mode finding
:trend,increased use in applications
:challenge,design and creation challenges
:review,scoping review of socio-technical challenges
:challenge,socio-technical challenges of CA creation
:paper,this opinion paper
:challenge,interdisciplinary collaboration challenges
:taxonomy,CA design challenges taxonomy
:strategy,practical strategies for CA design
:challenge,CA design challenges
:activity,empirical verification and application of strategies
:risk,automated disinformation generation
:capability,disinformation generation
:study,comprehensive study of disinformation capabilities
:narrative,disinformation narratives
:task,news article generation
:feature,safety warnings
:`model family`,detection model
:task,detection of LLM-generated content
:content,convincing news articles
:model,Violet
:`model components`,vision encoder and Gemini text decoder
:quality,generation fluency
:method,automatic data acquisition
:dataset,manually annotated dataset
:metric,CIDEr score
:dataset,Flickr8k
:process,pretraining multilingual language models
:resource,computational resources and training data
:method,adapting pretrained language models
:method,vocabulary extension
:method,efficient and effective subword embedding initialization
:resource,well-aligned multilingual word embeddings
:technique,matrix factorization
:outcome,efficient model initialization
:benefit,accelerated convergence and improved zero-shot transfer
:resource,publicly available code and models
:attribute,low confidence signaling
:technique,softmax probabilities
:attribute,confidence estimation
:model,Claude-v1.3
:method,linguistic confidence elicitation
:method,surrogate confidence model
:method,composing linguistic confidences and surrogate model probabilities
:performance,real-world task completion
:concept,complicated rules
:`learning paradigm`,rule-based learning
:method,rule distillation
:ability,in-context knowledge extraction
:`challenge track`,strict-small
:constraint,small training dataset
:approach,cognitively-motivated curriculum learning
:`model performance`,linguistic evaluation tasks
:curriculum,vocabulary curriculum
:strategy,constraining vocabulary
:curriculum,data curriculum
:strategy,ordering training instances
:curriculum,objective curriculum
:strategy,combining tasks
:result,non-consistent improvements
:benchmark,linguistic benchmarks
:task,select linguistic tasks
:analysis,curriculum learning analysis
:insight,beneficial task and setting combinations
:factor,model architecture and training hyper-parameters
:benchmark,default baselines
:sector,legal sector
:challenge,long sequence lengths
:challenge,specialized vocabulary
:model,ChatGPT-20B
:benchmark,LexGLUE
:model,FALCON-180B
:`model family`,legal-domain model
:need,more powerful legal-domain large language model
:`learning setting`,zero-shot transfer learning
:`model family`,black-box large language model
:pipeline,retrieve-then-generate
:technique,execution-guided self-refinement
:dataset,GrailQA
:role,source dataset
:dataset,WebQSP
:role,target dataset
:result,improvements to both stages
:technique,combination of BLLMs and transfer learning
:task,table to text
:technique,prompt modification
:issue,costs and information leaks
:technique,reasoning information injection
:model,table reasoner
:task,evidence identification
:model,table summarizer
:task,sentence generation
:strategy,search strategy
:task,reasoning label construction
:technique,highlighting input tables
:strategy,zero-shot NER
:framework,self-improving framework for NER
:data,self-annotated data
:strategy,sample selection
:data,self-annotated demonstrations
:strategy,iterative self-improving
:strategy,advanced strategy for reliable entity selection
:capability,vision capabilities
:component,dialogue manager
:input,textual prompts and visual stimuli
:balance,context preservation and computational efficiency
:agent,Furhat robot
:system,vision-enabled dialogue system
:interaction,textual and visual modalities
:concept,latent space directions
:concept,human-interpretable concept
:technique,Linear Relational Concepts (LRC)
:technique,Linear Relational Embedding (LRE)
:relationship,subject-object relation
:purpose,understanding model representations
:technique,inverting Linear Relational Embedding (LRE)
:technique,finding concept directions
:function,classifier and causal influence on outputs
:goal,accelerating LLM inference
:goal,optimal token distribution usage
:method,speculative contrastive decoding (SCD)
:goal,enhanced decoding quality
:analysis,token probabilities
:goal,efficient resource usage
:task,specification-heavy tasks
:skill,extensive task mastery
:limitation,ICL limitations
:goal,advancements in alignment methods
:need,factuality verification
:solution,end-to-end factuality annotation
:scheme,multi-stage annotation
:tool,annotation tool
:process,labelling procedure
:feature,flexible incorporation of automatic results
:benchmark,open-domain document-level factuality benchmark
:granularity,three-level
:experiment,preliminary experiments
:performance,identification of false claims
:resource,"annotation tool, benchmark, and code"
:interaction,pretraining and task data interaction
:performance,task performance variance
:data,task data
:measure,distributional and example-specific similarity measures
:measure,similarity metrics
:relationship,pretraining data and downstream tasks relationship
:assumption,simplistic assumptions
:activity,providing feedback
:activity,explaining feedback
:developer,code author
:need,explanation types
:study,analysis of explanations in code review
:topic,explanations in code review
:`code review comment`,solution only
:statistic,46% solution only
:`code review comment`,explanation included
:statistic,54% explanation included
:method,open card sorting
:activity,categorizing explanations
:number,seven categories
:activity,getting preferred explanation
:evaluation,ChatGPT explanation generation
:statistic,98% success rate
:field,computational argumentation
:setting,zero-shot and few-shot settings
:model,FLAN
:dataset,14 open-sourced datasets
:task,computational argumentation tasks
:dataset,new benchmark dataset on counter speech generation
:limitation,limitations in evaluating computational argumentation
:capability,zero-shot open vocabulary classification
:feature,novel class label definition
:model,CLIP-based zero-shot classifier
:method,open vocabulary certification (OVC)
:`model type`,open-vocabulary model
:technique,randomized smoothing
:concept,classifier perturbation
:method,incremental randomized smoothing
:technique,caching trick
:distribution,multivariate normal distribution
:task,application-driven tasks
:benchmark,Multilingual Evaluation of Linguistic Acceptability (MELA)
:gap,scarcity in purely linguistic evaluation of LLMs
:data,48k multilingual samples
:experiment,cross-lingual transfer and multi-task learning
:task,linguistic acceptability judgement
:data,in-language training data
:concept,task-specific but language-agnostic region
:concept,conflicting weight
:data,MELA dataset
:benchmark,GRASP
:capability,language grounding and physical understanding
:tool,Unity simulations
:level,initial level of GRASP
:capability,language grounding
:level,second level of GRASP
:concept,intuitive physics
:evaluation,GRASP evaluation
:issue,shortcomings in language grounding and intuitive physics
:value,progress monitoring
:semantics,language semantics
:semantics,collaborative semantics
:model,LC-Rec
:goal,integration of semantics
:task,item generation
:method,learning-based vector quantization
:method,alignment tuning
:URL,https://github.com/rucaibox/lc-rec/
:technique,constrained parameter regularization
:concept,statistical measure enforcement
:measure,L2-norm
:concept,constrained optimization problem
:method,augmented lagrangian method
:concept,varying regularization strengths
:concept,minimal hyperparameters
:phenomenon,grokking
:`research question`,neuron localization for memorization
:benchmark,INJ benchmark
:method,localization methods
:benchmark,DEL benchmark
:evaluation,localization methods evaluation
:method,pruning-based methods
:finding,neuron specificity
:issue,lack of control
:issue,difficulties in integrating variable knowledge
:`model family`,knowledge-augmented large language model
:definition,strict grounding
:dataset,new grounding dataset
:metric,grounding metric
:experiment,grounding performance experiments
:finding,factors influencing grounding performance
:data,English data
:concept,multilingual application
:`research focus`,multilingual capabilities enhancement
:technique,tuning strategies
:concept,multilingual abilities
:study,multilingual capacity evaluation
:classification,language quadrants
:concept,language characteristics
:experiment,multilingual performance improvement
:risk,downstream harms
:limitation,binary association tests on small datasets
:framework,probing for societal biases
:dataset,probing dataset
:metric,perplexity-based fairness score
:dataset,large-scale benchmarking dataset
:limitation,existing fairness collections
:finding,biases are nuanced
:`model variant`,larger model variants
:issue,higher degree of bias
:identity,different religions
:issue,disparate treatment
:risk,safety risks
:weakness,weaknesses in LLMs
:conflict,helpfulness vs safety
:strategy,goal prioritization
:metric,attack success rate (ASR)
:model,Vicuna-33B
:model,LLaMA2-13B
:work,jailbreakdefense_goalpriority
:topic,jailbreaking attacks and defenses
:strategy,answer calibration
:strategy,step-level calibration
:strategy,path-level calibration
:gap,understanding of answer calibration success factors
:objective,event understanding
:task,event detection
:challenge,annotation challenges
:dataset,MAVEN-Arg
:dataset,MAVEN
:task,all-in-one event understanding
:advantage,comprehensive event argument extraction benchmark
:application,future event prediction
:resource,MAVEN-Arg code
:url,https://github.com/thu-keg/maven-argument
:goal,inferring unseen relationships
:`method family`,embedding-based KGC method
:method,RESCAL
:method,TransE
:`method family`,PLM-based KGC method
:evaluation,conventional KGC evaluation
:ability,inference and memorization
:analysis,PLM-based KGC method analysis
:method,synthetic dataset construction
:`performance improvement`,PLM-based KGC method performance improvement
:issue,hallucinated content generation
:approach,non-retrieval-based generation
:method,post-hoc rectification
:issue,accumulated hallucination errors
:issue,snowballing
:approach,real-time verification and rectification (EVER)
:task,hallucination detection and rectification
:benchmark,retrieval-based and non-retrieval-based baselines
:task,diverse text generation tasks
:vulnerability,system prompt leakage
:`attack method`,SASP
:goal,jailbreaking multimodal large language model
:technique,human modification
:technique,modifying system prompts
:goal,defending against jailbreaks
:concept,system prompt role
:attribute,text quality
:technique,minimum Bayes risk decoding
:process,machine translation pipeline
:approach,unified approach combining RL and reranking
:concept,context window
:concept,episodic memory
:effort,expanding context window
:finding,suboptimal context usage by LLMs
:approach,RRescue
:concept,partial ordering
:concept,full ordering
:property,robustness and noise sensitivity
:benchmark,multi-document question answering dataset
:concept,common ground
:technique,dialogue acts
:domain,emotional support
:dataset,dialogue datasets
:characteristic,presumptive grounding
:technique,reinforcement learning with human feedback (RLHF)
:characteristic,less grounding
:field,human-AI interaction
:challenge,temporal knowledge reasoning
:concept,evolving factual knowledge
:concept,complex temporal logic
:approach,constructivism-based approach
:framework,Abstract Reasoning Induction (ARI)
:capability,integrating abstract methodologies
:dataset,temporal QA datasets
:resource,code for ARI framework
:`cognitive ability`,abstraction ability
:resource,AbsPyramid
:`knowledge type`,abstraction knowledge
:domain,open domain
:metric,recall and precision
:`model type`,cross-encoder ranker
:pipeline,GFF
:technique,reciprocal rank weighting
:benchmark,BEIR and TREC DL 2019/2020
:`model type`,zero-shot neural ranker
:technique,parameter efficient tuning
:approach,adding dense trainable parameters
:concept,sparse computation
:technique,SiRA
:technique,sparse mixture of expert (SMoE)
:technique,expert dropout
:problem,over-fitting
:technique,mixture of expert approaches
:task,single and multitask settings
:tool,writing assistant
:barrier,lack of personalization
:system,PEARL
:component,generation-calibrated retriever
:process,prompt augmentation
:method,training data selection
:objective,scale-calibrating KL-divergence
:application,personalized content generation
:task,document-level tasks
:task,document classification
:task,self-contradictions in long documents
:dataset,ContraDoc
:model,LLaMAv2
:task,generic summarization
:task,complex summarization task settings
:task,instruction controllable text summarization
:dataset,evaluation-only dataset
:evaluation,LLM-based automatic evaluation
:`evaluation method`,LLM-based
:`performance gap`,summary generation and evaluation
:benchmark,InstruSum
:capability,text synthesis
:task,human verification
:application,high-stakes applications
:approach,Symbolically Grounded Generation (SymGen)
:task,output validation
:data,conditioning data
:feature,provenance display
:experiment,data-to-text and question answering experiments
:ability,linguistic generalization
:concept,grammatical abstraction
:phenomenon,crosslingual structural priming
:concept,grammatical representations
:capability,longer text input
:task,information seeking in long contexts
:problem,lost in the middle
:task,attention strengthening multi-doc QA (ASM QA)
:capability,information searching and reflection in long contexts
:model,Ziya-Reader
:task,multi-doc QA
:`evaluation paradigm`,model-based evaluation
:`evaluation paradigm`,automatic-metrics-based evaluation
:system,Fusion-Eval
:metric,Spearman correlation
:dataset,SummEval
:potential,LLMs in evaluation
:application,NLP systems extension
:evidence,effects of multilinguality
:performance,language modeling performance
:`model type`,monolingual language model
:language,250+ languages
:`language family`,under-studied language families
:factor,"dataset size, linguistic similarity, model size"
:performance,low-resource language modeling performance
:phenomenon,curse of multilinguality
:strategy,massively multilingual pre-training
:language,all languages
:strategy,targeted model pre-training
:`data format`,semi-structured table
:task,table-based tasks
:dataset,TableInstruct
:model,TableLLaMa
:`model family`,generalist model for tables
:model,LLaMa 2 (7B)
:benchmark,in-domain tasks
:resource,TableInstruct dataset
:capability,reducing factual hallucination
:task,knowledge assessment
:response,unknown
:technique,sequential reading notes
:data,training data for CoT
:model,LLaMA-2 7B
:`model family`,retrieval-augmented language model with CoT
:`model family`,standard retrieval-augmented language model
:metric,EM score and rejection rates
:context,resource-constrained deployment
:technique,chain-of-thought distillation
:process,distilling large language models
:issue,flawed reasoning and hallucinations
:method,self-evaluation capability distillation
:process,comprehensive distillation process
:technique,chain-of-thought and self-evaluation paradigms
:metric,CHRF score
:evaluation,testsets
:`model family`,autoregressive large language model
:application,established applications
:practice,cybersecurity best practices
:commentary,this commentary
:solution,mitigation recommendations
:trend,services evolution
:service,AI-powered services
:feature,precise responses
:`human behavior`,reliance on AI technologies
:trend,increasing reliance
:task,meeting user needs
:concept,society's depiction
:experiment,query experiment
:response,one-word response
:data,public data
:model,decision-making model
:task,machine value judgments
:approach,practical approach to AI
:application,investigating remote worlds
:notion,AI making value judgments
:perspective,critical perspective on judgmental capabilities
:outcome,safe and accurate value judgments
:organ,human brain
:capability,visual information processing
:activity,using visual aids
:process,verbal reasoning
:approach,Chain of Images (COI)
:task,language reasoning problems
:dataset,COI evaluation dataset
:task,problem-solving with images
:benchmark,COI benchmark
:model,Symbolic Multimodal Large Language Model (SymLLM)
:capability,symbolic multimodal reasoning
:approach,Chain of Thoughts (COT)
:resource,COI code
:application,interactive language therapy
:skill,therapeutic interaction
:panel,clinical psychologists and psychiatrists
:tool,evaluation scorecard
:skill,empathetic engagement
:challenge,personalization and emotional understanding
:application,AI in therapy
:population,autistic adolescents
:technique,vanilla in-context learning
:`input type`,human-provided contexts
:framework,automatic in-context learning
:capability,self-produced context generation
:context,any setting with vanilla in-context learning
:outcome,strong performance across tasks
:capability,language model reasoning
:attribute,logically sound reasoning
:study,prior studies on chain of thought
:technique,conventional chain of thought
:concept,error avoidance
:technique,contrastive chain of thought
:concept,error reduction
:method,automatic construction of contrastive demonstrations
:task,NL-centric tasks
:approach,injecting symbolic knowledge into LLM
:challenge,symbol interrelations and balance
:`model series`,Symbol-LLM
:collection,34 symbolic tasks
:form,symbolic forms
:framework,two-stage tuning framework
:goal,injecting symbolic knowledge
:task,symbol- and NL-centric tasks
:concept,word predictability
:process,human comprehension
:process,human cognitive load
:assumption,LLM probability accuracy
:concept,probability calibration
:task,human reading simulation
:concept,temperature-scaled surprisal
:task,predicting human reading times
:value,temperature setting
:metric,calibration metric
:concept,human-likeness bias
:analysis,further analysis
:`model type`,pruned model
:condition,limited budget
:effect,pruning on hallucinations
:study,empirical study on hallucinations
:phenomenon,hallucinations by pruned models
:finding,pruned LLMs hallucinate less
:`model type`,full-sized model
:input,source input
:dependency,greater dependency on source input
:outcome,higher lexical overlap
:task,downstream task
:context,multilingual language generation
:technique,composing specialized parameters
:data,English labeled data
:context,multilingual labeled data availability
:technique,arithmetic composition of PEFT modules
:strategy,minimal training of PEFT modules
:issue,factually incorrect information
:concept,confidence assessment
:`model family`,retrieval augmented language model
:task,scientific NLP tasks
:`research area`,uncertainty quantification for retrieval augmented language models
:issue,research gap
:model,retrieval augmented language model finetuned with scientific knowledge
:model,retrieval augmented language model pretrained with scientific knowledge
:issue,overconfidence in predictions
:knowledge,scientific knowledge
:resource,"code, data and dashboards"
:URL,https://github.com/pnnl/expert2
:task,open knowledge extraction
:tool,Linked Open Knowledge Extractor (LOKE)
:benchmark,CARB benchmark
:dataset,TEKGEN dataset
:approach,LOKE-GPT
:tool,AllenAI's OpenIE 4
:issue,over-generation of triples
:analysis,entity linkability
:dataset,CARB dataset
:extraction,LOKE-GPT extractions
:advancement,massively multilingual machine translation
:approach,translation-based cross-lingual transfer
:strategy,round-trip translation
:approach,adding reliable translations to training data
:strategy,translation-based cross-lingual transfer for unsupported languages
:method,model selection based on target-language validation data
:method,model selection based on source-language data
:`research community`,XLT researchers
:behavior,sycophancy
:phenomenon,suggestibility
:task,responding to prompts
:benchmark,safety benchmarks
:`attack method`,poisoned training data
:attribute,stealthiness and generalizability
:`attack method`,malicious prompt injection
:`attack framework`,backdoor activation attack
:behavior,attacker-desired behaviors
:technique,steering vector generation
:discussion,countermeasures against activation attacks
:contact,https://email-haoran-for-link
:risk,deploying without trustworthiness
:assessment,trustworthiness of LLMs
:strategy,chain of utterances-based prompting
:attack,trustworthiness attack
:model,Mistral
:finding,performance vs trustworthiness
:`model family`,instruction-tuned models
:technique,fine-tuning for safety alignment
:value,safety and robustness
:framework,joint framework for robustness improvement
:approach,automated red teaming
:component,belief augmenter
:task,improving robustness
:model,adversarial model
:process,iterative feedback loops
:component,belief generator
:scenario,dynamic interaction
:dataset,static benchmark dataset
:group,Georgetown InfoSense Group
:event,TREC IKaT 2023
:runs,submitted runs by Georgetown InfoSense Group
:benchmark,median runs
:method,generate-retrieve-generate
:task,initial answer generation
:task,answer grounding
:task,passage quality filtering
:task,text processing
:model,chat-based model
:model,text-to-transfer-based model
:evaluation,official TREC evaluation
:evaluation,self-evaluation by Georgetown InfoSense Group
:sequence,involving different components
:tool,grammatical error correction tool
:task,grammar error explanation
:gap,lack of explanations in grammatical error correction
:pipeline,two-step pipeline for grammar error explanation
:evaluation,human evaluation of grammar error explanation pipeline
:issue,hallucinated answers
:framework,AGREE
:capability,grounding enhancement
:capability,iterative test-time adaptation
:technique,tuning
:technique,data construction method
:technique,prompting-based approaches
:concern,dataset copyright protection
:method,backdoor-based watermarking
:goal,copyright protection
:risk,malicious misclassification
:method,FunctionMarker
:process,watermark extraction
:concept,customizable functions
:goal,semantic preservation
:instance,mathematical functions
:experiment,watermark extraction accuracy
:behavior,self-disclosure
:initiative,protecting user-side privacy
:taxonomy,self-disclosure categories
:corpus,self-disclosure corpus
:data,4.8k annotated disclosure spans
:model,language model for identification
:metric,token F1
:study,HCI user study
:task,self-disclosure abstraction
:model,language model for abstraction
:outcome,diverse abstractions
:user,information-seeking user
:question,questions with implicit assumptions
:domain,maternal and infant health
:`system feature`,pragmatic constraint recognition
:task,examining questions in context
:`user group`,mothers
:topic,pregnancy and infant care
:inference,pragmatic inference
:method,existing detection methods
:system,QA system
:task,detecting inferences
:pipeline,existing QA pipeline
:approach,software-centric model compression
:technique,simulated quantization
:goal,deployment feasibility
:approach,hardware-centric quantization
:goal,effective quantization
:method,OdysseyLLM
:concept,hardware awareness
:`kernel implementation`,FastGEMM
:technique,quantization strategies
:method,W4A8
:benchmark,inference speed
:goal,performance retention
:task,fallacy recognition
:value,argument quality and validity
:challenge,computational fallacy recognition
:characteristic,"diversity in genres, domains, and types"
:data,synthetic examples
:data,existing fallacy datasets
:context,supplementary context
:`fallacy type`,diversion fallacies
:`evaluation result`,consistent improvements
:technique,program-aided reasoning
:technique,program-aided language model
:observation,prompting style diversity
:observation,generation diversity
:activity,eliciting feedback
:`model response`,presentation to users
:user,end user
:value,understanding and trust
:`model family`,decomposed question-answering model
:study,user study on feedback for rationales
:study,user study on understanding and trust
:format,rationale format
:challenge,evaluation challenges
:issue,unfair comparisons
:issue,incomplete evaluation framework
:issue,low reproducibility
:benchmark,TextEE
:resource,standardized data preprocessing scripts and splits
:approach,event extraction approaches
:data,electronic health record (EHR)
:`performance issue`,performance degradation on longer texts
:benchmark,LongBox
:technique,local-global attention
:technique,fusion-in-decoder (FID)
:result,mixed results with long-sequence handling
:resource,LongBox source code and data
:URL,https://github.com/mihir3009/longbox
:technique,generative use of language model
:task,generating separators
:separator,randomly chosen tokens
:performance,near-state-of-the-art
:separator,human-curated prompts
:strategy,random generation
:phenomenon,good separators abundance
:observation,language space richness
:assumption,effective prompt characteristics
:result,random separators usage
:metric,average 16% relative improvement
:separator,random separators
:separator,human-curated separators
:technique,automatic prompt searching methods
:paradigm,tied-LoRA
:technique,weight tying and selective training
:technique,parameter training/freezing
:technique,weight tying
:experiment,variety of tasks and two base language models
:analysis,trade-offs between efficiency and performance
:configuration,particular tied-LoRA configuration
:performance,comparable performance across several tasks
:concept,known examples
:concept,unknown examples
:concept,in-context example set
:concept,known and unknown information
:dataset,multi-answer question answering datasets
:study,construction of in-context example sets
:task,knowledge-rich tasks
:task,detecting implicit toxicity
:context,diverse contexts
:dataset,LifeTox
:task,identifying implicit toxicity
:characteristic,diverse contexts from personal experiences
:task,toxicity classification
:challenge,implicit toxicity
:sequence,workflow
:problem,multi-step workflow action prediction
:prediction,multi-step action prediction
:capability,multi-turn automation
:approach,zero-shot graph traversal
:task,predicting task success
:capability,automation of steps
:concept,emotion trigger
:dataset,EmoTrigger
:task,emotion trigger identification
:finding,emotion triggers not salient
:`model family`,emotion prediction model
:interplay,features and emotion detection
:method,reasoning methods
:question,reasoning support for predictions
:question,quality of reasoning
:framework,SCORE
:issue,self-contradictory reasoning
:behavior,self-contradictory
:method,point-of-view (POV) method
:`diagnostic tool`,reasoning analysis
:behavior,one-perspective performance
:setting,multi-perspective settings
:research,evaluation of reasoning
:practice,best practices
:problem,selecting best examples for ICL
:approach,existing example selection
:metric,GistScore
:technique,example gisting
:technique,attention bottleneck
:model,fine-tuned gist model
:performance,state-of-the-art ICL performance
:model,multi-task gist model
:advantage,fast retrieval speed
:task,visually situated language understanding
:capability,text and visual element recognition
:method,specialized pre-processing tools
:process,document image input mapping
:tool,optical character recognition (OCR) systems
:capability,text token reasoning
:`trade-off`,computational and engineering complexity
:method,using external tools and LLMs
:model,small pretrained image-to-text models
:capability,selective text or layout recognition and reasoning
:output,rationales and answers
:model,Pix2Struct
:benchmark,visual document understanding benchmarks
:`document type`,"infographics, scanned documents, and figures"
:model,direct answer prediction Pix2Struct model
:technique,k-nearest neighbors retrieval
:phenomenon,MLP hurdle
:model,GPT-2 117M
:dataset,new datasets
:concept,memorization and generalization
:goal,simulating human opinion dynamics
:phenomenon,societal phenomena
:`model type`,agent-based model
:characteristic,fidelity to human behavior
:approach,simulating opinion dynamics with LLMs
:`information type`,accurate information
:bias,inherent bias in LLM agents
:viewpoint,resistant views
:bias,confirmation bias
:outcome,opinion fragmentation
:`data source`,real-world discourse
:concept,blackbox for new tasks
:method,using LLMs' outputs as labels
:task,selecting demonstrations for ICL
:label,LLMs' output probability
:concept,utility of a demonstration
:label,task-specific reward
:method,incremental utility
:task,"binary/multi-class classification, segmentation, and translation"
:experiment,distribution of probability values
:experiment,nuanced reward values
:task,segmentation and translation tasks
:challenge,distribution differences
:issue,distractions and biases
:method,CRISPR
:technique,attribution methods
:benchmark,social bias benchmarks
:characteristic,practical and model-agnostic
:function,knowledge repository
:state,obsolete
:problem,online continual knowledge learning (OCKL)
:challenge,dynamic world knowledge
:benchmark,new benchmark for OCKL
:metric,knowledge acquisition and retention
:evaluation,empirical evaluation for OCKL
:baseline,robust baselines for OCKL
:approach,existing continual learning approaches
:factor,key factors influencing OCKL
:understanding,training LLMs in evolving environments
:capability,interpreting code modules
:task,novel library usage learning
:evaluation,systematic evaluation of LLMs
:capability,code generation based on in-context libraries
:capability,understanding novel code libraries
:task,learning novel library modules
:implementation,raw code implementations
:environment,adaptable and dynamic coding environments
:issue,security vulnerabilities
:actor,adversarial annotator
:method,RankPoison
:activity,red-teaming of RLHF
:goal,malicious behaviors
:dataset,poisoned dataset
:challenge,security challenges in RLHF
:concept,event causality
:process,story understanding
:technique,symbolic story generation
:challenge,identifying open-world causal event relations
:method,event causality identification
:technique,event causality extraction
:dataset,GLUCOSE
:metric,story quality evaluation
:metric,story video-text alignment
:process,computational story understanding
:application,programming exercises generation
:evaluation,LLM evaluation by instructors and researchers
:perspective,student usage
:approach,student-first approach
:method,student surveys and interviews
:insight,"ChatGPT benefits, challenges, and improvements"
:demographic,undergraduate computer science students
:attitude,positive outlook on ChatGPT
:task,chemistry problem solving
:task,chemistry reasoning
:approach,InstructChem
:capability,chemical reasoning
:process,structured reasoning
:task,chemical formulae generation
:task,preliminary answer derivation
:process,iterative review-and-refinement
:process,confidence increase
:quality,high-quality reasoning
:task,time-series text classification
:phenomenon,evolving domain shifts
:strategy,evolving domain adaptation
:strategy,domain-adversarial training
:strategy,domain-adaptive pretraining
:method,incremental self-training
:strategy,traditional domain adaptation
:`research area`,PLM robustness
:phenomenon,natural temporal evolution of language
:phenomenon,human group dynamics
:phenomenon,wisdom of partisan crowds
:persona,Democrat and Republican personas
:process,social influence
:agent,LLM agent without chain-of-thought reasoning
:agent,LLM agent with chain-of-thought reasoning
:technique,explicit bias incorporation
:technique,fine-tuning LLMs with human data
:risk,overfitting behaviors
:application,modeling human group phenomena
:risk,ethical risks
:technique,token-level watermarking
:process,watermark insertion
:technique,watermarking with importance scoring (WIS)
:attribute,semantic correctness
:technique,importance scoring prediction
:attribute,importance scoring
:outcome,improved text quality and detection rate
:approach,refusal-aware instruction tuning (R-Tuning)
:concept,knowledge gap
:data,refusal-aware data
:concept,knowledge intersection
:ability,refusal ability
:concept,meta-skill
:approach,learning uncertainty during training
:approach,uncertainty-based testing
:resource,code for R-Tuning
:application,clinical note generation
:quality,content quality
:group,medical experts
:group,non-medical experts
:process,two-phase optimization
:quality,expert customization
:content,politically charged discussions
:task,deciphering and predicting partisan biases
:challenge,understanding political bias in digitized discourse
:approach,traditional political bias detection
:method,finetuning separate models
:spectrum,political ideologies
:framework,comprehensive analytical framework
:assessment,partisan bias divergence
:prediction,partisan class tendency
:model,singular instruction-tuned LLM
:alignment,real-world political ideologies
:nuance,emotional and moral
:context,politically sensitive
:application,applications requiring political bias awareness
:understanding,nuanced political
:cause,semantic associations
:issue,keyword/entity biases
:method,probing
:phenomenon,hallucination and reasoning shortcuts
:benchmark,Eureqa
:capability,correct reasoning chain
:association,distractor semantic associations
:capability,LLM reasoning
:`model family`,encoder-only model
:factor,training data quality and diversity
:capability,language model capabilities
:`programming language`,popular programming languages
:taxonomy,NLP generalization taxonomy
:person,Hupkes et al.
:`benchmark dataset`,GenCodeSearchNet (GECS)
:dataset,natural language code search datasets
:subset,StatCodeSearch
:`model family`,BERT-style model
:task,evaluation and comparison
:process,large language model inference
:attribute,computation and memory intensive
:technique,lexical shortlisting
:heuristic,unicode-based script filtering
:heuristic,corpus-based selection
:attribute,memory usage
:attribute,generation speed
:study,pilot study on lexical shortlisting for LLMs
:issue,drawbacks of vocabulary selection methods
:field,social sciences
:research,persona studies
:dataset,persona measurement dataset
:task,persona measurement
:experiment,prompt variation experiment
:experiment,consistency variation experiment
:finding,perturbation sensitivity
:finding,negation consistency
:task,capturing model perceptions
:discussion,improving prompting practices
:dataset,long-form database question answering dataset
:task,LLM interaction with SQL interpreter
:action,generating multiple SQL queries
:strategy,interaction strategies
:bottleneck,planning capacity
:`evaluation framework`,multi-agent evaluation framework
:goal,assessing answer quality
:attribute,evaluation precision and reliability
:task,complex retrieval and reasoning tasks
:issue,error propagation
:methodology,verifier model
:approach,assessing incomplete reasoning paths
:model,Outcome-Supervision Value Model (OVM)
:technique,outcome supervision
:goal,accurate conclusions
:issue,labor-intensive annotations
:dataset,Game of 24
:model,OVM-7B
:task,value estimation for planning
:`human attribute`,perception of language
:`human attribute`,personal background
:`societal group`,certain societal groups
:task,subjective NLP tasks
:dataset,PopQuorn
:`demographic group`,white and female participants
:technique,demographic-infused prompting
:`demographic group`,black and asian individuals
:issue,gender and racial biases
:URL,https://github.com/jiaxin-pei/llm-group-bias
:situation,lack of prerequisite knowledge
:behavior,fabrication and hallucination
:norm,human conversational norms
:challenge,responsible and ethical AI development
:benchmark,adversarial question-answering
:`question type`,unanswerable questions
:concept,non-existent concepts or false premises
:behavior,conveying uncertainty
:correlation,accuracy and confidence
:approach,model-agnostic unified confidence elicitation
:performance,LLMs with instruction finetuning and RLHF
:performance,LLMs without instruction finetuning and RLHF
:expression,uncertainty expression
:perception,confidence of direct response
:research,teaching LLMs to express uncertainty
:paradigm,new NLP paradigm
:strategy,scaling up
:data,large corpora
:process,converging
:data,colossal converged corpora
:issue,understudied side-effects
:data,heterogeneous corpora
:technique,source prompts
:process,coordinated pre-training
:`model family`,pre-trained language model with source prompts
:outcome,improvement in downstream tasks
:technology,generative engines
:technology,traditional search engines
:process,synthesizing information
:stakeholder,website and content creators
:paradigm,generative engine optimization
:benchmark,GEO-bench
:metric,visibility
:strategy,GEO strategies
:field,information discovery systems
:value,author's intent and perspectives
:`case study`,political perspectives in news summarization
:finding,alteration of political opinions in summaries
:model,P^3Sum
:technique,political perspective classification
:technique,iterative evaluation
:technique,loss back-propagation
:experiment,extensive experiments on news summarization datasets
:challenge,preserving author perspectives in news summarization
:technique,post-training quantization and pruning
:data,calibration data
:process,layer activation generation
:study,effect of calibration data on LLM performance
:outcome,downstream task performance variation
:technique,pruning and quantization methods
:technique,effective use of calibration data
:framework,SLM/LLM routing framework
:technique,exemplar pools
:concept,context reliability
:technique,sentence embedding
:concept,context-dialogue state similarity
:technique,k-nearest exemplars retrieval
:technique,majority vote routing
:concept,instance routing
:concept,fallacy
:issue,disinformation
:task,fallacy detection and classification
:characteristic,subjectivity
:taxonomy,fallacies
:classification,previous fallacy classifications
:`annotation scheme`,new annotation scheme
:`evaluation method`,new evaluation method
:dataset,MAFALDA
:standard,gold standard dataset
:taxonomy,unified fallacy taxonomy
:concept,fallacious reasoning
:output,rationales
:issue,irrelevant rationales
:issue,inability to compose subquestions
:method,graph-guided chain-of-thought prompting
:issue,multi-step reasoning issues
:structure,question/rationale graph
:step,graph verification
:technique,previous chain-of-thought prompting
:`study focus`,backdoor defense
:gap,testing time defense
:service,web services
:strategy,training-time defenses
:strategy,defensive demonstrations
:phase,testing phase
:threat,triggers
:task,automatic evaluation of generated textual content
:task,evaluation metric creation
:`evaluation metric`,language model-driven evaluation metric
:`evaluation metric`,BARTScore
:`evaluation metric`,T5Score
:`evaluation metric`,GPTScore
:concept,latent bias
:`evaluation metric`,reference-free evaluation
:`evaluation metric`,generative evaluation model
:concept,external factors
:task,evaluation protocol development
:value,dependability
:field,photonic computing
:technology,photonic integrated circuits
:concept,ultrafast artificial neural networks
:category,information processing devices
:technology,neuromorphic photonics
:solution,compelling solution for AI and optical neural networks
:review,recent advances in integrated photonic neuromorphic systems
:topic,materials and device engineering breakthroughs
:analysis,comparative analysis of AI accelerators
:aspect,crucial technical aspects
:component,nanophotonic components
:breakthrough,recent breakthroughs in photonic engineering and materials science
:technology,photonic AI accelerators
:metric,computing speed and energy efficiency
:review,potential future approaches in photonic AI accelerators
:`performance metric`,critical performance metrics
:trend,widespread use
:task,model customization
:technique,controlled generation
:task,output management
:challenge,prompt influence control
:task,research and development
:method,ControlPE
:task,prompt control
:method,prompt distillation
:technique,LoRA merging weight adjustment
:tool,dynamic prompt control
:quality,practicality and efficacy
:task,prompt variety control
:knowledge,specialized knowledge
:challenge,heterogeneity of data
:protocol,unified simple input-output pair format
:model,HuaTuoGPT-II
:domain,traditional Chinese medicine
:evaluation,expert manual evaluation
:event,Chinese national medical licensing examination
:user,natural language interface user
:issue,repetition of preferences
:concept,standing instructions
:`user preference`,persian food preference
:dataset,NLSI
:task,language-to-program conversion
:data,dialogues with user profiles and API calls
:challenge,identifying applicable standing instructions
:phenomenon,interdependent instructions
:`performance metric`,exact match on API prediction
:task,collecting human data
:attribute,subjective NLP topics
:trend,collaborative efforts between humans and LLMs
:goal,generating diverse data
:capability,generating diverse perspectives on subjective topics
:problem,diversity extraction in LLMs
:technique,criteria-based prompting
:goal,measuring perspective diversity
:metric,semantic diversity
:concept,diversity coverage
:technique,recall prompting
:goal,extracting diverse perspectives
:system,information seeking dialogue system
:method,BeInfo
:technique,behavioural tuning
:dataset,standard datasets
:data,real production conversation data
:practice,training on synthetic data
:issue,limited human-generated data
:study,impact on linguistic diversity
:metrics,novel metrics for diversity
:experiment,recursive fine-tuning experiments
:task,natural language generation tasks
:finding,decrease in diversity
:risk,preservation of linguistic richness
:study,impact on linguistic capabilities of LLMs
:aspect,linguistic capabilities
:technique,propagandistic techniques
:medium,online communication
:task,automatic detection and debunking
:content,propagandistic content
:task,detecting propagandistic textual spans
:task,annotating propagandistic content
:dataset,in-house developed dataset
:activity,misinformation generation
:activity,homework completion
:task,distinguishing text origin
:process,signal embedding
:signal,watermark signal
:technique,statistical testing
:effect,altered output distribution
:`model family`,watermarked large language model
:task,k-class classification
:task,short-form generation
:user,watermarked model users
:concept,trade-offs
:`knowledge source`,structured and unstructured data
:`data type`,relational databases and free text
:task,building conversational interfaces
:language,SUQL
:concept,executable representation for data queries
:language,SQL
:agent,conversational search agent
:dataset,crowdsourced questions and conversations about real restaurants
:approach,SUQL-based conversational agent
:phenomenon,structured and unstructured data queries
:baseline,strong and commonly used baseline
:concept,temporal knowledge
:task,temporal question answering
:`reasoning type`,multi-answer and multi-hop temporal reasoning
:dataset,Complex-TR
:capability,complex temporal reasoning
:benchmark,temporal QA benchmarks
:challenge,cost of domain expert annotation
:experiment,empirical experiment
:`model comparison`,LLMs vs compact models
:method,warmup method
:attribute,power
:response,harmful or unethical responses
:aspect,cognitive structure and processes
:vulnerability,safety vulnerability
:`attack type`,cognitive overload
:concept,cognitive load management
:`defense strategy`,existing defense strategies
:benchmark,instruction-following benchmarks
:limitation,language limitation and automated construction
:benchmark,FollowEval
:dimension,string manipulation
:dimension,response constraints
:task,detecting machine-generated text
:problem,text quality degradation
:technique,arbitrary vocabulary partitioning
:`model attribute`,language model expressiveness
:approach,XMark
:technique,mutually exclusive rule for synonyms
:`model attribute`,text generation fluency
:benchmark,code generation benchmarks
:domain,real-world programming
:resource,open-source libraries
:benchmark,ML-Bench
:task,machine learning tasks
:data,samples and tasks
:model,ML-Agent
:task,codebase navigation and code generation
:url,https://ml-bench.github.io/
:task,bibliographic question answering
:model,BERT-based sentence encoder
:task,question similarity identification
:`knowledge graph`,ORKG (Open Research KG)
:benchmark,SciQA
:event,Scholarly-QALD-23 challenge
:need,improved NLP benchmarks
:benchmark,contemporary NLP benchmarks
:need,comprehensive subject coverage
:benchmark,PsyBench
:`model attribute`,strengths and weaknesses
:metric,average accuracy above 70%
:concept,context-aware intelligence
:task,graph encoding
:task,graph data integration
:technique,encoding modalities
:benchmark,GraphTMI
:task,graph structure analysis
:modality,image modality
:challenge,token limits
:concept,encoding modality performance
:task,graph understanding and reasoning
:method,interactive chain of repairing
:behavior,human code repairing
:capability,coding ability
:agent,code learner
:agent,code teacher
:concept,chain-of-repairing
:outcome,bug understanding
:tool,code compiler
:metric,repairing turns
:resource,Intervenor GitHub repository
:data,Intervenor data and code
:population,disabled patients
:method,classification setup
:organ,brain
:model,generative language BCI
:input,fMRI input
:model,standard large language model
:technique,statistical language training
:finding,feasibility of BCIs in direct language generation
:problem,scarcity of labeled datasets
:data,synthetic hate speech sequences
:task,train set augmentation
:model,ALBERT
:model,RoBERTa-toxicity
:model,HateBERT
:model,Hatexplain
:model,ToxDect
:model,ToxiGen
:finding,improved hate speech generalization
:task,zero-shot hate detection
:`performance metric`,"generalization, recall, precision"
:question,improving sensitivity of models like GPT-3.5
:system,commercial AI systems
:component,system prompts
:role,helpful assistant
:study,systematic evaluation of social roles in system prompts
:list,curated list of roles
:taxonomy,social roles
:analysis,extensive analysis of LLMs
:role,gender-neutral roles
:challenge,predicting best performance role
:metrics,"frequency, similarity, and perplexity"
:resource,code and data for prompting with social roles
:URL,https://github.com/jiaxin-pei/prompting-with-social-roles
:dataset,Song Describer Dataset (SDD)
:purpose,music-and-language model evaluation
:content,audio-caption pairs
:`music recording`,706 music recordings
:`model family`,music-and-language model
:task,music captioning
:task,text-to-music generation
:task,music-language retrieval
:goal,understanding model performance
:outcome,health literacy
:outcome,health outcomes
:model,Microsoft Bing
:outcome,reading grade level (RGL)
:approach,cautious approach to health information
:field,health communication
:challenge,crafting outputs below sixth-grade RGL
:capability,modifying outputs above sixth-grade RGL
:limitation,instruction finetuning and weak multi-turn dialogue interactions
:model,DRESS
:category,critique and refinement
:category,critique
:category,refinement
:goal,effective multi-turn interactions
:method,conditional reinforcement learning
:`model family`,state-of-the-art large vision language model
:`theoretical framework`,language model decoder algorithm formulation
:concept,action-state value function space
:task,decoding algorithm optimization
:task,tradeoff arbitration
:attribute,sensibleness
:method,Automated Parliaments
:entity,AI delegates
:`model types`,"generators, modifiers, evaluators"
:mechanism,simultaneous modification mechanism
:process,response creation
:mechanism,evaluation mechanism
:process,solution assessment
:process,feedback learning
:scenario,morally contentious scenarios
:metric,loss value reduction
:application,Automated Moral Parliaments
:value,human morality alignment
:`model family`,video-language alignment model
:issue,contrastive changes in video captions
:dataset,VideoCon
:issue,contrast misalignments
:`model family`,generative video-language model
:model,VideoCon-based alignment model
:benchmark,video-language alignment task
:task,temporally-extensive video-language tasks
:task,text-to-video retrieval
:benchmark,SSv2-Temporal
:task,video question answering
:benchmark,ATP-Hard
:`content type`,novel videos and human-crafted captions
:resource,code and data for VideoCon
:field,temporal knowledge graph forecasting
:discipline,knowledge graph research
:`method category`,embedding-based methods
:task,link forecasting on temporal knowledge graphs
:challenge,zero-shot relation forecasting
:representation,relation representations
:`performance aspect`,forecasting unseen relations
:`performance aspect`,forecasting seen relations
:technique,short prompts
:challenge,automatic design of long prompts
:technique,long prompts
:algorithm,greedy algorithm
:technique,automatic long prompt engineering
:algorithm,genetic algorithm
:technique,LLM-based mutation
:benchmark,BIG-bench hard
:result,accuracy gain
:task,visual-language understanding tasks
:approach,encoding images and videos into separate feature spaces
:issue,misalignment before projection
:work,unifying visual representation into language feature space
:model,Video-LLaVa
:benchmark,image benchmarks
:model,Video-ChatGPT
:concept,unified visual representation
:concept,multi-modal inputs
:model,state-of-the-art vision models
:barrier,financial and computational burdens
:activity,training new models
:framework,UnifiedVisionGPT
:process,model selection automation
:concept,passive predictors
:concept,simulators
:theory,active inference
:field,cognitive science and neuroscience
:system,traditional active inference systems
:feature,tight feedback loop
:consequence,enhanced model self-awareness
:consequence,drive to minimize prediction error
:data,textual records of contemporary social issues
:opportunity,studying moral debates
:theory,Moral Foundations Theory
:task,data-driven analysis
:tool,computational tools for detecting moral foundations
:issue,incompleteness and fragility of lexicons
:task,measuring moral foundations in text
:model,MFormer
:concept,moral foundations
:phenomenon,people's stance on social issues
:goal,understanding moral situations
:concept,mental states
:`cognitive science theory`,simulation theory
:framework,SimToM
:technique,two-stage prompting
:technique,perspective-taking
:requirement,minimal prompt-tuning
:benchmark,ToM benchmarks
:task,environmental ecosystem modeling
:method,observable features and measurements combination
:question,general framework for environmental data modeling
:framework,FREE
:task,long-term prediction
:application,predicting stream water temperature
:application,predicting annual corn yield
:method,pre-training on simulated data
:practice,training on extensive unfiltered corpora
:`model family`,generative pretrained transformer
:capability,self-diagnosis
:technique,self-debiasing decoding algorithm
:issue,harmful text generation
:technique,diagnosing-debiasing approach
:bias,insults
:bias,political bias
:effort,investigating ethical and social implications
:paradigm,pre-train then fine-tune
:activity,language model pre-training
:goal,energy and carbon footprint quantification
:study,computational costs of fine-tuning empirical study
:result,fine-tuning energy and carbon costs
:activity,language model inference
:community,NLP researchers and practitioners
:goal,fine-tuning energy efficiency improvement
:literature,studies on ICL and IT
:issue,disconnect between ICL and IT
:relationship,ICL and IT
:convergence,between ICL and IT
:behavior,LLM behavior
:concept,systematic investigation
:study,comprehensive survey and analysis
:question,LLMs in software engineering
:`data collection`,literature and work collection
:data,134 works
:concept,publisher categorization
:concept,mainstream benchmarks
:community,developers of code LLMs
:system,retrieval system
:result,promising initial results
:interaction,dialogue with user
:system,music generation system
:attribute,ease of implementation
:availability,online availability
:`approach category`,fine-tuning paradigm-based approach
:factor,dataset quality
:`approach category`,information retrieval-based approach
:issue,lack of similar code
:task,smart contract code comment generation
:approach,SCCLLM
:task,code snippet retrieval
:technique,demonstration-based learning
:community,Etherscan.io
:`model family`,pretrained large language model
:issue,stereotypical bias
:literature,stereotypical bias mitigation literature
:results,bias behavior understanding
:`training approach`,multi-task model training
:capability,handling multiple tasks
:problem,sequence length variation
:approach,dynamic micro-batching
:efficiency,efficient multi-task model training
:`training technique`,pipeline-parallel training
:method,dynamic programming-based approach
:technique,dynamic pipeline and communication scheduling
:improvement,training throughput
:resource,DynAPIpe's source code
:URL,https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines
:technique,fMRI brain activity mapping
:concept,information flow patterns
:framework,workspace framework for consciousness
:hypothesis,information integration predicts brain activity
:classification,in-degree categories
:technique,brain prediction accuracy mapping
:result,difference in prediction accuracy
:`biological concept`,cortical hierarchy
:finding,parallel in processing linguistic information
:domain,programs with complex memory manipulations
:approach,existing loop invariant generation
:method,fixed sets or templates
:approach,machine learning for loop invariant generation
:issue,lack of labeled data and efficiency
:system,interactive system between LLM and verification tools
:process,online querying for unseen programs
:framework,proposed framework for loop invariant generation
:domain,new data structures or multi-loop programs
:gap,between existing capabilities and practical requirements
:experiment,experiments on memory-manipulated programs
:technique,item representation learning
:`model family`,sequential recommendation model
:technique,ID embeddings
:technique,text-based item representation learning
:approach,CoWpiRec
:data,item-level interaction data
:`data structure`,word graph
:task,pre-training task
:benchmark,state-of-the-art transferable sequential recommenders
:problem,cold-start issue
:field,medicine and healthcare
:framework,multi-disciplinary collaboration
:agent,role-playing LLM-based agent
:activity,collaborative multi-round discussion
:process,five critical steps
:dataset,medical datasets
:url,https://github.com/gersteinlab/medagents
:task,domain-specific queries
:approach,knowledge extraction and adaptive training
:tool,LLMiner
:benchmark,new evaluation benchmark
:data,domain-specific text corpora and QA pairs
:model,LLMiner-enhanced LLM
:`model family`,domain-adapted models
:resource,seed instances
:task,subjective evaluation prediction
:model,fusion model for MOS prediction
:approach,supervised and unsupervised approaches
:model,LE-SSL-MOS
:model,Unit Language Model (ULM)
:data,highly intelligible domain data
:metric,ASR confidence
:technique,ensemble learning
:milestone,first supervised and unsupervised fusion for MOS prediction
:model,fusion system
:event,VoiceMOS Challenge 2023
:performance,robot policies
:challenge,generalizing to novel environments
:`guidance form`,human corrective feedback
:task,adapting to and learning from online human corrections
:challenge,non-trivial endeavor
:capability,remembering human feedback
:capability,responding to arbitrary feedback
:system,DROC
:`feedback type`,online language corrections
:capability,knowledge distillation and retrieval
:technique,direct robot code generation via LLMs
:resource,DROC website
:model,Falcon-1B
:data,MedQuAD
:`data type`,medical QA pairs
:context,resource-constrained environments
:goal,sustained medical relevancy
:field,medical AI
:intent,research purposes
:configuration,BitsandBytesConfig
:model,TüLü
:`dataset collection`,TüLü-v2-mix
:model,TüLü 2
:model,TüLü 2+DPO
:model,Code TüLü 2
:model,Code LLaMA
:`model suite`,TüLü 2 suite
:model,GPT-3.5-Turbo-0301
:survey,practical survey on LLMs in finance
:approach,fine-tuning on domain-specific data
:approach,training custom LLMs from scratch
:framework,decision framework for LLM adoption
:limitation,limitations in LLMs for finance
:goal,advancing financial AI
:event,release
:domain,higher education
:task,writing bachelor theses
:`research question`,validity of writing bachelor theses as a learning and assessment format
:method,autoethnographic approach
:task,thesis writing
:data,logs and chat histories
:goal,learner engagement
:framework,generative education
:concept,personalized learning environments
:problem,two-sigma problem
:product,Harmony
:concept,educator development
:concept,policy frameworks
:concept,cross-sector collaboration
:concept,proactive adaptation
:concept,formative feedback
:outcome,effective learning
:challenge,delivering timely and individualized feedback
:context,large classes in higher education
:task,personalized code correction and feedback generation
:data,student submissions
:task,AI-aided e-assessment
:task,e-assessment and feedback
:issue,error evaluation
:technology,automation technology
:`historical artifact`,water wheels
:technology,robotic process automation
:task,tasks requiring human-like intelligence
:`automation paradigm`,agentic process automation
:agent,ProAgent
:task,workflow construction and decision-making
:agent,specialized agents
:concept,feasibility
:concept,new automation paradigm
:resource,ProAgent code
:URL,https://github.com/openbmb/proagent
:concept,performance scaling
:data,web-sourced natural language sentences
:field,diverse fields
:field,"video, audio signals, and robotic movement"
:concept,generalization-in-learning capacity
:task,attractor dynamics learning
:method,Dynamic Time Warping (DTW)
:concept,values
:principle,"helpful, honest, harmless"
:concept,risk criteria
:issue,"clarity, adaptability, transparency"
:paradigm,basic value alignment
:concept,value space
:concept,basic value dimensions
:theory,Schwartz's theory of basic values
:dataset,FULCRA
:concept,basic value evaluation and alignment
:technique,scaling model parameters
:`model type`,dense model
:metric,computation footprint
:goal,efficient learning
:model,mixture of word experts
:`model family`,memory augmented model
:`model family`,regular mixture-of-experts model
:`model family`,complex memory augmented model
:technique,sparse memory search
:concept,neuron efficiency
:model,UltraFastBERT
:concept,selective neuron engagement
:network,fast feedforward networks (FFFs)
:network,feedforward networks
:concept,conditional neural execution
:goal,efficient implementation
:implementation,high-level CPU code for UltraFastBERT
:benchmark,optimized baseline feedforward implementation
:implementation,PyTorch implementation for UltraFastBERT
:benchmark,batched feedforward inference
:resource,UltraFastBERT training code
:resource,UltraFastBERT benchmarking setup
:resource,UltraFastBERT model weights
:`user experience`,personalized user experience
:goal,reaching end-goals
:approach,automated user persona identification
:data,user session trajectory information
:technique,neural collaborative filtering
:concept,token semantics
:model,SessionBERT
:task,capturing user trajectory semantics
:task,service recommendation
:metric,hit@5
:task,chart image understanding
:domain,chart understanding
:dataset,multimodal chart instruction (MMC-Instruction)
:attribute,600k instances
:model,multimodal chart assistant (MMCA)
:benchmark,multimodal chart benchmark (MMC-Benchmark)
:attribute,9 distinct tasks
:task,multimodal chart understanding
:capability,reasoning and decision-making
:benchmark,ToolTalk
:performance,LLM-based assistant performance
:component,tools and plugins
:`tool type`,world-affecting tools
:`error category`,three major error categories
:application,AI for reaction condition optimization
:industry,pharmaceutical industry
:agent,AI agent for drug discovery
:ability,chemical insights and real-time knowledge acquisition
:paradigm,three-phase paradigm
:technique,multi-LLM debate
:technique,coarse-label contrastive learning (CCL)
:feature,chemical fingerprint
:task,optimal reaction condition recommendation
:performance,close-to-human performance and strong generalization capability
:field,AI for chemistry
:application,computer-aided synthesis planning
:system,human-like systems
:data,comprehensive domain data
:pattern,specific working patterns
:paradigm,DOKE
:component,domain knowledge extractor
:technique,knowledge incorporation through prompts
:domain,information systems
:action,removing words
:attribute,text coherence
:challenge,automatic meaningful substitution
:technique,zero-shot text sanitization
:goal,data utility
:challenge,multimodal challenge
:aspect,human perceptions
:goal,efficacy
:information,modality-invariant and specific information
:strategy,multimodal prompt strategy
:component,modality-invariant and specific components
:capability,assimilating information from various modalities
:method,adapting LoRA adapters
:`model family`,smaller-sized language model
:method,gradient-free routing function
:process,weighted combination of experts
:method,token-level adaptation of LoRA adapters
:task,mathematical task
:task,scientific task
:dataset,ARC-Challenge
:task,reading comprehension task
:task,coding task
:dataset,CodeAlpaca-20K
:method,fine-tuning individual models
:process,adaptation of every-other token
:resource,public repository
:artifact,study code
:task,"modeling, prediction, and generation"
:characteristic,opacity
:`model performance`,modulation by functional networks
:technique,functional neuroimaging
:model,Galactica-125M
:technique,task-based prompt sequences
:technique,general linear model
:data,layer output values
:result,distinct overlapping networks
:network,medical imaging and pathology networks
:task,"model alignment, modulation, and fine-tuning"
:method,editor method
:concept,unfairness
:concept,cognitive bias
:field,judgment and decision-making
:stance,cautious optimism
:stance,anti-panglossian
:discussion,cognitive bias in large language models
:concept,human cognitive bias rationality
:concept,unrepresentative data
:concept,model bias
:method,common explainability methods
:approach,language-based explainability
:approach,most explainability approaches
:task,explaining individual AI predictions
:strategy,explainability strategy
:task,visual classification
:task,medical imaging classification
:finding,descriptors align with clinical knowledge
:issue,shortcut connections
:study,pilot reader study
:result,AI-identified words enable non-experts
:group,non-expert humans
:`model family`,multimodal foundational model
:task,delivering explanations
:task,open-domain dialogue generation
:attribute,diversity and coherence
:problem,diverse and coherent dialogue generation
:approach,variational frameworks
:attribute,dialogue diversity
:attribute,contextual coherence
:framework,Bayesian Open-Domain Dialogue with Empirical Bayes (BODEB)
:`model component`,pretrained parameters
:`model type`,embedding-based model
:capability,"language expression, knowledge-aware reasoning, instruction following"
:approach,model interpretation approach for recommender systems
:method,behavior alignment
:space,language space
:method,intention alignment
:method,hybrid alignment
:space,language and latent spaces
:evaluation,alignment effect and explanation generation ability
:method,"behavior alignment, intention alignment, hybrid alignment"
:result,effective comprehension and credible explanation generation
:capability,comprehension and explanation generation
:task,software monitoring
:capability,personalized question answering
:cause,biased training data
:task,monitoring hallucinations
:audience,decision makers
:stage,prototyping
:process,designing LLM-based solutions
:stage,LLM evolution using human feedback
:module,data to answer generation
:application,generative AI chatbots
:technique,pre-trained word embeddings
:model,GloVe
:concept,log-co-occurrence matrix
:algorithm,Bit-Cipher
:concept,hyperparameter
:task,word vector training
:task,POS tagging
:task,NER
:task,LM training
:embedding,Cipher embeddings
:software,adapters library
:concept,modular transfer learning
:concept,adapter modularity
:concept,complex adapter setups
:concept,conventional fine-tuning challenges
:concept,efficient transfer learning
:resource,AdapterHub
:domain,sociotechnical domains
:perspective,human perspective
:method,qualitative data collection
:challenge,data collection challenges
:paper,vision paper
:approach,AI-assisted qualitative data collection
:`data source`,AI-generated synthetic text
:behavior,human responses and behaviors
:technique,persona-based prompting
:method,interviews
:technique,multi-persona dialogue
:method,focus groups
:technique,mega-persona responses
:method,surveys
:behavior,human behavior emulation
:`model family`,artificial intelligence model
:benefit,scalable data generation
:problem,open problems in AI-assisted data collection
:quality,"nuanced, empathetic understanding"
:approach,integrated data collection approach
:outcome,effective outcomes
:content,artificial intelligence-generated content
:goal,quality of experience
:service,AIGC image generation
:approach,user-centric interactive AI
:process,denoising chain
:technique,prompt sharing
:approach,reinforcement learning with large language models interaction
:algorithm,GDM-based Deep Deterministic Policy Gradient
:algorithm,Deep Deterministic Policy Gradient
:entity,software applications
:entity,downstream application developers
:service,LLM APIs
:concept,model evolution
:concept,performance regression
:concept,regression testing
:concept,traditional testing approaches
:concept,correctness notions
:concept,prompting brittleness
:concept,non-determinism
:task,deep and reliable reasoning
:scenario,critical scenarios
:framework,knowledge anchoring and closed-loop reasoning
:performance,LLMs' performance
:quality,improved reasoning
:challenge,informing users effectively
:workflow,dynamic workflow for privacy policies
:challenge,utilization of large language models
:process,continuous improvement and monitoring
:approach,experts-in-the-loop
:task,zero-shot tasks
:`task type`,classification and retrieval
:challenge,acquiring labels
:task,selecting unlabeled samples
:concept,active learning adaptation
:concept,knowledge hints for balance
:framework,PCB
:concept,active learning framework
:method,conventional active learning and random sampling
:application,robot program generation
:`robot type`,service mobile robot
:requirement,accurate action sequencing and ordering
:tool,CodeBotler
:benchmark,RoboEval
:capability,program generation for service robot tasks
:language,embedded domain-specific language (EDSL) in Python
:concept,skill abstraction
:`evaluation metric`,execution trace correctness
:`evaluation metric`,robustness of program generation
:taxonomy,modes of failures for LLMs in robot program generation
:analysis,LLMs failure analysis in robot program generation
:resource,CodeBotler code and benchmark
:URL,https://amrl.cs.utexas.edu/codebotler/
:issue,undesired data
:data,incorrectly annotated data
:issue,harmful model output
:concept,data credibility
:`model application`,harmless language model training
:benchmark,Jigsaw Civil Comments
:framework,systematic framework for dataset credibility evaluation
:task,dataset credibility evaluation
:task,unsafe comments and conversation classification
:finding,label errors in datasets
:action,fixing label errors
:resource,DoCTA open-source repository
:practice,evidence-based medicine
:value,healthcare quality
:activity,medical decision-making
:growth,medical evidence
:task,evidence synthesis
:goal,accountable AI
:goal,fair AI
:goal,inclusive AI
:concept,trustworthiness of generative AI
:task,automated summarization of medical evidence
:field,causal reasoning
:method,text-based descriptions
:goal,determining causal relationships
:method,causal reasoning aggregation
:risk,overconfidence and false predictions
:experiment,physics-inspired synthetic data
:limitation,PLM-based causal reasoning limitations
:framework,PLM and causal discovery integration
:algorithm,causal discovery algorithm
:technique,regularization using prior knowledge
:technology,monolithic silicon-photonics integrated-circuits
:concept,system-on-chip photonic-electronic linear-algebra accelerator
:feature,optical comb-based broadband incoherent photo-detections
:feature,high-dimensional operations of consecutive matrix-matrix multiplications
:benefit,computation density and energy efficiency
:challenge,power/area overhead
:approach,holistic co-design
:mechanism,attention-head mechanism
:application,emergent applications
:task,attribute control
:method,causal average treatment effect
:issue,spurious correlation
:issue,unintended bias
:problem,toxicity mitigation
:issue,inadvertent bias
:capability,open-world object perception
:task,open-vocabulary dense prediction
:task,open-vocabulary camouflaged object segmentation (OVCOS)
:challenge,camouflaged object perception
:dataset,OVCAMO
:model,OVCOSER
:method,semantic guidance and visual structure cues integration
:capability,camouflaged object detection
:task,story visualization
:output,series of images
:quality,high quality
:quality,text alignment
:quality,character consistency
:system,automated story visualization system
:task,layout planning
:output,story images
:`control condition`,sparse control conditions
:`control condition`,dense control conditions
:output,high-quality image content
:module,dense condition generation module
:method,multi-view consistent character image generation
:task,character image collection
:capability,multi-modal comprehension
:capability,multi-modal generation
:`research area`,multi-modal understanding and generation
:stage,nascent stage
:framework,M^2UGEN
:capability,multi-modal music understanding and generation
:model,ViViT
:model,AudioLM 2
:model,MusicGen
:model,Mu-LLaMA
:dataset,text/image/video-to-music generation datasets
:risk,security risks
:technique,adversarial prompt tuning
:concept,learnable text prompts
:issue,model vulnerabilities
:technique,image-processing-based defense
:field,robust multimodal learning
:problem,hallucinatory outputs
:process,ongoing audits and evaluations
:approach,inherently interpretable methods
:profession,human counselors
:skill,empathetic understanding
:field,mental health counseling
:role,complementary tools
:trend,burgeoning development in vision-language models
:framework,Camo-Perceptive Vision-Language Framework
:issue,uncertainty in localization
:concept,chain of visual perception
:data,COD datasets
:task,task planning and tool usage
:challenge,API description prompt limitation
:challenge,complex task planning
:challenge,API semantic distinction
:framework,task planning and tool usage enhancement
:component,API retriever
:task,API selection
:component,LLM finetuner
:component,demo selector
:system,real-world commercial system
:dataset,open-sourced academic dataset
:concept,semantic model
:entity,system
:concept,structured causal explanation
:event,state change
:theory,Snowball Earth theory
:prototype,graphical interface
:approach,direct representation
:action,inspection and verification
:issue,bias towards training data distribution
:attribute,state-of-the-art recognition accuracy
:model,label-synchronous neural transducer
:component,label-level encoder representation
:component,blank tokens
:model,prediction network
:`model family`,standard language model
:mechanism,auto-regressive integrate-and-fire
:attribute,low latency operation
:method,streaming joint decoding
:goal,improved ASR accuracy
:dataset,TED-LIUM 2
:dataset,AESRC2020
:concept,applications
:actor,malicious actors
:gap,security risks in LLM research
:taxonomy,security risks taxonomy
:example,specific attack examples
:goal,robust and secure LLM applications
:task,extractive summarization
:data,customer-agent dialogs
:method,pseudo-label generation
:model,chat summarization model
:dataset,TweetSumm
:task,chat summarization
:capability,spatial understanding
:aspect,causes of spatial understanding lack
:relation,left-right positional relations
:approach,teaching with synthetic data
:`data type`,natural images
:performance,improvement on left-right relations
:technique,meta prompting
:capability,problem-solving and data interpretation
:theory,type theory and category theory
:concept,structure and syntax of information
:metric,token efficiency
:`model family`,multi-modal foundation model
:`data type`,diverse data types
:resource,meta prompting code
:URL,https://github.com/meta-prompting/meta-prompting
:attack,adversarial prompt attack
:vulnerability,input-output mechanism
:method,token-level detection
:metric,model perplexity
:information,neighboring token information
:method,binary classification of tokens
:method,probability estimation of tokens
:task,managing structured data
:integration,LLM integration in data science
:concept,decision-making methodologies
:factor,nature of the data
:model,variability model
:data,toy datasets
:outcome,evaluation of GPT-4's methodology
:heuristic,other platforms
:concept,AI decision-making processes
:goal,creating transparent and comprehensible AI systems
:task,requirements classification
:model,long short-term memory
:task,functional requirements classification
:task,non-functional requirements classification
:challenge,cross-domain in-context learning
:limitation,availability of in-domain demonstrations
:challenge,long-tail knowledge
:limitation,LLM limitations
:problem,UDA under in-context learning
:idea,retrieve cross-domain elements
:strategy,prompting and training strategies
:effectiveness,ICL for domain transfer
:lab,IUST NLP Lab
:event,Eval4NLP 2023 workshop
:strategy,zero-shot prompt-based strategy
:task,summarization task evaluation
:experiment,LLM as evaluation metrics
:performance,best provided prompts
:metric,Kendall correlation
:resource,code and results
:`model family`,biomedical pretrained language model
:capability,cross-lingual ability
:challenge,scarcity of non-English domain corpora
:task,training multilingual biomedical models
:model,KBioXLM
:approach,knowledge-anchored
:corpus,biomedical multilingual corpus
:technique,knowledge alignments
:granularity,three granularity levels
:technique,training tasks
:benchmark,English benchmarks
:`model family`,monolingual and multilingual pretrained models
:resource,KBioXLM code
:URL,https://github.com/ngwlh-gl/kbioxlm
:benchmark,contemporary multi-modal benchmarks
:issue,assessing reasoning capabilities
:dataset,benchmark dataset for MLLMs
:`reasoning category`,abductive reasoning
:`evaluation criteria`,intermediate reasoning steps
:`evaluation scheme`,human assessment methods
:benchmark,open-ended multi-step elaborate reasoning benchmark
:resource,benchmark code and data
:URL,https://core-mm.github.io/
:task,open-domain knowledge-based visual question answering (OK-VQA)
:process,image to text conversion
:task,visual question reasoning
:discrepancy,image-text representation discrepancy
:performance,reasoning performance
:framework,question-asking framework for LLMs
:task,filling information gap
:component,filters
:task,refining generated information
:method,question-asking framework
:benchmark,A-OKVQA
:task,biomedical question answering
:model,Taiyi
:collection,biomedical text mining datasets
:strategy,two-stage supervised fine-tuning
:task,biomedical NLP tasks
:capability,bilingual biomedical multi-tasking
:resource,"Taiyi source code, datasets, and model"
:URL,https://github.com/dutir-bionlp/taiyi-llm
:method,integration of large language model and tabular data classification
:challenge,data serialization sensitivity and biases
:strategy,ranking categorical variables using LLM
:strategy,generating priors on correlations
:model,MonotonicLR
:technique,mapping ordinals to cardinals
:task,news timeline creation
:goal,comprehensive understanding of events
:activity,discerning patterns and trends
:sector,finance and insurance
:task,effective risk management
:technique,traditional natural language processing
:concept,nuanced relevance of news
:technique,direct prompting
:technique,extended task prompting
:task,assessing past news relevance
:technique,conventional prompting
:tool,publicly accessible browser extension
:network,our network
:task,causal structure learning
:structure,causal directed acyclic graph
:challenge,vast DAG spaces and data sparsity
:integration,LLM-based causal inference with CSL
:issue,unreliable constraints from imperfect LLM inferences
:approach,LLM for CSL
:issue,computational intensity of full pairwise variable analyses
:framework,Iterative LLM Supervised CSL (ILS-CSL)
:issue,unreliable constraints and computational intensity
:resource,LLM resources
:resource,ILS-CSL code
:attribute,program simplicity
:attribute,maintainability
:goal,secure and bug-free programs
:activity,code refactoring
:risk,breaking working programs
:value,learning experiences
:task,simplifying Python programs
:method,prompting with few-shot examples
:metric,cyclomatic complexity reduction
:evaluation,qualitative evaluation
:attribute,code formatting capability
:behavior,unnecessary code modifications
:issue,limitations and hallucinations
:`model type`,embedding model for text retrieval
:issue,limitations in text retrieval
:model,PEG
:task,robust text retrieval
:technique,negative sampling
:technique,progressive learning mechanism
:data,100 million data
:benchmark,state-of-the-art embeddings
:resource,Hugging Face repository
:dataset,C-MTEB
:dataset,DuReader
:field,image restoration and enhancement (IRE)
:limitation,limited pre-trained datasets
:method,existing IRE methods
:feature,interactive mechanisms
:feature,user choices
:system,Clarity ChatGPT
:integration,conversational intelligence and IRE methods
:innovation,IRE system features
:interaction,image-text
:capability,generalization and interaction in IRE
:domain,low-level vision-language
:concept,fixed intrinsic rank
:method,Sparse Low-Rank Adaptation (SoRA)
:component,gate unit
:technique,proximal gradient method
:stage,inference stage
:concept,zeroed-out ranks
:component,sparsifying scheduler
:`data storage`,database system
:`system type`,rule-based knowledge base
:business,commercial entities
:integration,rule engine and neural chatbot integration
:`case study`,commercial rule engine and neural chatbot integration
:concept,control level
:concept,model control
:phenomenon,model hallucination
:threat,AI attacks
:`attack type`,transferable attacks
:effort,developing transferable attacks
:concept,holistic understanding
:domain,various AI domains
:concept,attack architecture
:context,practical scenarios
:field,language intelligence
:capability,emergent reasoning capabilities
:attribute,model qualities
:research,COT reasoning methodologies
:agent,autonomous language agents
:environment,varied environments
:paper,this survey paper
:topic,chain-of-thought reasoning and language agents
:research,prospective research avenues
:repository,related papers repository
:url,https://github.com/zoeyyao27/cot-igniting-agent
:model,DocPedia
:capability,high-resolution document parsing
:domain,frequency domain
:`information type`,visual and textual information
:strategy,dual-stage training strategy
:training,enriched instructions/annotations
:benchmark,publicly available benchmarks
:advantage,effectiveness and superior performance
:component,soft attention
:issue,irrelevant information incorporation
:process,next token generation
:technique,System 2 Attention (S2A)
:process,context regeneration
:`model family`,standard attention-based large language model
:task,QA
:task,longform generation
:task,text coding
:language,non-English
:workflow,LLM text coding workflow
:guide,practical guide for LLM text analysis
:role,human annotator
:attribute,efficiency and cost-effectiveness
:task,text coding projects
:agent,LLM-based agent
:capability,cooperative capabilities
:method,manual jailbreak prompts
:team,Evil Geniuses
:phenomenon,reduced robustness against malicious attacks
:phenomenon,nuanced responses
:phenomenon,challenging detection of improper responses
:vulnerability,LLM-based attacks on agents
:URL,https://github.com/t1ans1r/evil-geniuses
:capability,multi-modal perception and understanding
:model,LION
:`knowledge type`,fine-grained spatial-aware visual knowledge
:task,region-level vision-language tasks
:strategy,stage-wise instruction-tuning with mixture-of-adapters
:conflict,image-level vs region-level VL tasks conflict
:`evidence type`,high-level semantic visual evidence
:method,soft prompting
:issue,imperfect predicted tags influence
:`attack model`,word-level adversarial attack model
:quality,validity and naturalness
:method,LLM-Attack
:process,two-stage attack process
:process,word importance ranking
:attribute,vulnerability
:process,word synonym replacement
:resource,synonyms from LLMs
:evaluation,human and GPT-4 evaluation
:`model family`,video large language model
:evaluation,unified evaluation for video tasks
:task,video tasks
:`evaluation method`,GPT-based evaluation
:standard,human-like performance
:model,Video-LLaVA
:scenario,driving scenarios
:challenge,crafting precise textual representations for class names
:technique,textual descriptors enhancement
:issue,explain without seeing dilemma
:`interaction type`,textual interactions with LLMs
:issue,oversight of inter-class relationships
:outcome,ineffective class differentiation
:framework,iterative optimization with visual feedback
:strategy,evolutionary optimization
:`feedback type`,visual feedback
:metric,image classification accuracy
:description,resulting descriptions
:feature,explainable and robust features
:complexity,income tax laws
:trend,use of tax software
:organization,IRS
:statistic,50% taxpayers use tax software
:consequence,incorrect tax filing
:goal,correctness of tax software
:`testing method`,metamorphic testing
:`software type`,tax software
:challenge,absence of correctness requirements and trustworthy datasets
:concept,metamorphic properties
:process,extracting metamorphic properties
:characteristic,tedious and time-consuming
:task,generating metamorphic specifications
:task,translation task
:`research agenda`,automating generation of metamorphic specifications
:technique,memory-efficient adaptation
:algorithm,iterative algorithm
:process,matrix decomposition
:component,quantized component
:component,low-rank component
:formulation,integer linear programming
:algorithm,data-aware version
:technique,LQ-LoRA
:benchmark,OpenAssistant
:task,few-shot image classification and segmentation
:task,classification and segmentation
:method,training-free FS-CS method
:role,tools
:model,GPT-4Vision
:capability,summarizing and reasoning
:framework,modular framework for FS-CS
:attribute,extendability
:method,proposed FS-CS method
:benchmark,PASCAL-5i dataset
:task,CAN attack detection
:task,understanding human semantics
:strategy,leveraging pre-trained transformers
:task,language-related tasks
:model,CAN-SecureBERT
:task,CAN intrusion detection
:model,CAN-LLAMA2
:model,MTH-IDS
:task,cybersecurity-related tasks
:task,American Sign Language fingerspelling translation
:domain,Sign Language Translation
:technique,hand pose estimation
:task,contextual word translation
:`loss term`,novel loss term for length prediction
:model,translation model
:approach,two-stage inference approach
:benchmark,ChicagoFSWild datasets
:`performance improvement`,more than 10% relative improvement
:approach,proposed method for fingerspelling recognition
:URL,https://github.com/pooyafayyaz/fingerspelling-posenet
:`event series`,DARPA Grand Challenges
:system,chat systems
:model,PALM
:problem,long-tailed AI dilemma
:technique,foundation model techniques
:category,autonomous driving techniques
:`model family`,reinforcement learning-based large language model
:concept,Bayes rule
:task,using Bayes rule for medical diagnosis
:concept,medical variable names
:commentary,sensitivity and specificity
:result,ChatGPT's performance
:framework,NeuroPrompts
:task,prompt enhancement
:technique,constrained text decoding
:outcome,higher-quality text-to-image generations
:feature,stylistic control
:application,interactive application for prompt enhancement and image generation
:dataset,large dataset of human-engineered prompts
:outcome,enhanced prompts
:resource,"code, screencast video demo, and live demo instance"
:domain,commercial applications
:concept,output verifiability
:field,language model explanation
:concept,training data attribution
:application,legal document generation
:framework,unified framework of large language model attributions
:goal,system development and standardization
:process,assortment planning
:field,e-commerce and retail
:literature,existing literature on assortment planning
:problem,assortment planning variants
:challenge,in-store planning complexity
:process,collaborative efforts
:outcome,prolonged decision-making
:framework,InterAssort
:challenge,assortment planning challenges
:tool,optimization tools
:interface,user-friendly interface of InterAssort
:interaction,interactive conversations
:capability,customized decision-making
:experiment,extensive experiments on InterAssort
:field,operations management
:task,audio-visual zero-shot learning
:goal,recognize unseen categories
:method,multi-modal feature learning
:issue,obscure action concepts
:framework,Knowledge-Aware Distribution Adaptation (KDA)
:task,understanding unseen categories
:loss,distribution alignment loss
:loss,knowledge-aware adaptive margin loss
:data,user-generated conversation data
:process,uploading data to the cloud
:method,local annotation
:constraint,storage limitations
:question,on-device llm personalization
:framework,novel on-device llm personalization
:data,representative data
:metric,user-specific content-generating capability and fine-tuning speed
:interaction,online user interactions with information
:technique,query-document matching
:capability,comprehending and generating human-like text
:application,LLMs in information retrieval
:methodology,optimizing retrieval process
:goal,cost-efficiency and enhanced result accuracy
:`model behavior`,inaccurate or misinterpreted data
:integration,LLMs with IR systems
:consideration,data optimization
:consideration,system clarity and interpretability
:examination,comprehensive examination
:strategy,integrating LLMs with IR systems
:approach,balanced approach
:principle,user-centric principles
:limitation,pretraining corpus
:aspect,document structure
:model,structure-aware retrieval augmented language model
:graph,heterogeneous document graph
:relationship,document relationships
:technique,structural embedding
:task,scientific benchmarks
:outcome,structure-aware retrieval
:quality,retrieval quality
:metric,overall accuracy
:application,general-purpose
:model,AcademicGPT
:data,academic training corpus
:milestone,domain-specific GPT for research
:benchmark,academic benchmarks
:benchmark,ComputerScienceQA
:application,academic applications
:task,AI-assisted title and abstract generation
:application,autonomous driving systems
:`model family`,vision foundation model
:application,multimodal AI systems
:concept,history of autonomous driving
:application,driving and transportation systems
:resource,datasets and benchmarks
:event,1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD)
:model,VILAM
:`model architecture`,vision-language Transformer
:capability,knowledge and reasoning capacities
:technique,frozen pre-trained encoders
:task,feature encoding and alignment
:technique,cycle training
:dataset,referring expression datasets
:dataset,public general and medical datasets
:domain,medical field
:application,language model for question-answering
:task,reasoning over context
:method,identifying unmemorisable evaluation samples
:method,semantic similarity analysis
:data,evaluation-train pairs
:`model training`,multitask language model training
:dataset,additional training datasets
:`evaluation dataset`,DROP
:`evaluation dataset`,ROPES
:capability,understanding and generating human-like text
:concept,personalized and explainable recommendations
:survey,LLM-based recommendation systems survey
:task,personalized explanation generating
:challenge,unfairness and bias problem
:application,diverse areas
:limitation,shorter text pre-training
:context,longer-context prompts
:survey,comprehensive survey on transformer-based LLMs
:taxonomy,holistic taxonomy of transformer upgrades
:problem,long-context input and output handling
:evaluation,evaluation for long-context LLMs
:toolkit,optimization toolkits for LLMs
:capability,efficiency and efficacy
:challenge,predominant challenges in long-context LLMs
:repository,long-LLMs-learning repository
:literature,relevant literature
:task,tour itinerary recommendation
:concept,point-of-interest (POI) sequence planning
:field,operations research
:concept,utility maximization with constraints
:task,POI filtering and ranking
:concept,personalized POI recommendation and planning
:task,POI embedding learning
:technique,transformer-based technique
:task,itinerary generation
:task,temporal detail capture
:system,AudioLog
:approach,joint training network
:model,Hierarchical Token-Semantic Audio Transformer
:task,audio log summarization
:task,acoustic scene classification
:task,sound event detection
:task,summarizing long audio sequences
:challenge,distinguishing human vs machine-generated text
:method,traditional shallow learning
:method,multilingual model fine-tuning
:evaluation,benchmarking methods
:text,machine-generated texts
:goal,creating robust and discriminative models
:`model family`,modular multilingual language model
:scenario,multilingual inference with unknown languages
:evaluation,modular MLM evaluation
:component,Language Identification (LID) module
:scenario,real-case multilingual scenarios
:effect,adding LID
:evaluation,multilingual evaluation of modular MLMs
:discussion,closing performance gap
:issue,performance gap in pipelined approach
:approach,pipelined approach of LID and modular MLMs
:challenge,adapting pre-trained vision-language models
:task,generating visual explanations
:algorithm,REVISE
:method,iterative computation
:approach,multi-step explanation generation
:approach,single-step explanation generation
:explanation,generated by REVISE
:technique,few-shot self-training
:dataset,VCR and VQA-X
:quality,efficacy and data-efficiency
:examples,"citation networks, social networks, biological data"
:task,graph-related tasks
:taxonomy,new taxonomy for LLM integration with graphs
:categories,"enhancer, predictor, alignment component"
:survey,systematic survey of LLMs in graph tasks
:study,existing studies on LLMs in graph tasks
:limitation,limitations of existing studies
:research,future research on LLMs in graph tasks
:potential,promising avenues
:resource,awesome-llms-in-graph-tasks GitHub repository
:papers,relevant papers on LLMs in graph tasks
:concept,scientific progress
:model,Nach0
:task,multi-domain and multi-task
:data,unlabeled text
:framework,Nemo
:technique,parallel optimization
:output,high-quality molecular and textual formats
:concept,multi-domain setup
:task,automated vulnerability detection
:understanding,quantitative understanding
:benchmark,VulBench
:data,CTF challenges and real-world applications data
:experiment,experiments on LLMs and SOTA models
:outcome,LLMs outperforming SOTA models
:task,defined term identification
:context,mathematical definition in academic text
:method,token-level classification
:approach,rule-based
:application,zero-shot speech synthesis
:`model family`,autoregressive speech model
:limitation,slow inference speed and lack of robustness
:model,HierSpeech++
:framework,hierarchical speech synthesis
:attribute,robustness and expressiveness
:framework,text-to-vec
:framework,speech super-resolution
:model,Hierarchical Variational Autoencoder
:achievement,first human-level quality zero-shot speech synthesis
:resource,audio samples and source code
:URL,https://github.com/sh-lee-prml/hierspeechpp
:model,WangchanBERTa
:task,Thai language modeling
:task,understanding foreign words
:component,WangchanBERTa's tokenizer
:feature,foreign vocabulary
:technique,vocabulary transfer
:model,Phayathaibert
:data,new larger dataset
:`data type`,physiological data
:source,FDA-approved devices
:environment,simulated low-air-pressure plateau
:`data type`,anomalous physiological data
:metric,medical indicators
:measurement,MAE and MAPE
:task,image analysis
:concept,AI health assistant
:goal,personalized health insights
:system,existing systems
:process,corpus curation and assessment
:platform,OASIS
:module,interactive modular rule filter
:process,customized rules creation
:module,debiased neural filter
:task,bias removal
:module,adaptive document deduplication
:task,document deduplication
:module,holistic data assessment
:process,corpus assessment
:process,pretraining data curation and assessment
:corpus,800GB bilingual corpus
:function,predictors
:study,interplay between in-context learning and function properties
:framework,formal framework for in-context learning
:task,approximating functions with varying number of minima
:method,producing functions with given inputs as minima
:property,number of minima
:`model type`,2-layer neural network
:experiment,few-shot experiments
:`biological entity`,genome sequence
:concept,cellular process blueprint
:activity,experimental annotation
:`biological entity`,functional elements in DNA
:`biological entity`,non-coding elements in DNA
:`biological entity`,regulatory elements in DNA
:field,unsupervised language modeling
:`biological entity`,genomic DNA
:`model family`,DNA language model
:task,genome annotation
:benchmark,BEND
:method,expert methods
:feature,long-range features
:era,large language model era
:capability,generating high quality text
:purpose,avoiding harmful use
:benchmark,machine-generated text detection benchmarks
:challenge,integration of newest detection methods
:framework,IMGTB
:task,machine-generated text detection benchmarking
:feature,"default set of analyses, metrics, and visualizations"
:issue,computational cost and motion coherence
:framework,GPT4Motion
:task,video synthesis
:feature,physical simulation
:output,Blender script
:feature,built-in physics engine
:scenario,physical motion scenarios
:field,text-to-video research
:capability,perception and reasoning
:issue,object hallucination and factual accuracy
:`evaluation method`,previous multimodal evaluation
:aspect,multimodal interactions
:`evaluation tool`,KNVQA-Eval
:task,knowledge-based VQA
:dataset,KNVQA
:quality,human judgment and perception
:information,contextual information of LVLMS
:capability,fine-grained capabilities of LVLMS
:`evaluation tool`,automatic evaluation tools
:field,engineering design
:modality,visual artifacts
:`model family`,multimodal vision language model
:task,design tasks
:research,structured evaluation of GPT-4V
:field,engineering design and manufacturing
:dataset,benchmark testing datasets
:study,empirical study on LLMs
:capability,understanding misinformation
:task,text-based misinformation detection
:task,propagation-based misinformation detection
:strategy,instruction-tuned strategies
:capability,learning effective features
:tool,questionnaire
:field,HCI research
:issue,respondent fatigue
:method,longitudinal study
:tool,diverse questionnaire versions
:tool,standardized depression questionnaire
:tool,LLM-generated questionnaire variants
:concept,psychometric testing
:concept,reliability and validity
:tool,standardized questionnaire
:issue,repetitiveness
:concept,questionnaire engagement
:task,drone navigation
:issue,lack of multi-modal datasets
:concept,visual-text alignment
:dataset,GeoText-1652
:framework,LLM-based data generation
:`model family`,pre-trained vision model
:dataset,University-1652
:objective,Blending Spatial Matching
:concept,spatial relation matching
:approach,Blending Spatial Matching-based approach
:task,drone control and navigation
:concept,engineering paradigm
:trend,prompt-based tools
:field,prompting framework
:concept,systematic literature
:concept,prompting framework lifecycle
:concept,hierarchical structure
:concept,emerging field landscape
:repository,prompting-framework-survey
:community,academic and industry
:system,machine learning system
:question,fine-tuning effect on capabilities
:tool,mechanistic interpretability tools
:process,model capability change
:analysis,fine-tuning effects
:claim,underlying capabilities unaltered
:transformation,wrapper
:phenomenon,capability revival
:risk,safety wrapper removal
:dataset,tinystories
:task,video-grounded dialogue generation
:skill,visual scene understanding and reasoning
:`model family`,large visual-language model
:task,spatio-temporal relationship reasoning
:approach,MSG-BART
:component,multi-granularity spatio-temporal scene graph
:component,global scene graph
:component,local scene graph
:technique,multi-pointer network
:skill,information selection capability
:benchmark,video-grounded dialogue benchmarks
:`model family`,encoder-decoder pre-trained language model
:domain,general-domain NLP tasks
:domain,HPC domain tasks
:model,HPC-GPT
:task,managing AI models and datasets for HPC
:task,data race detection
:benchmark,open-source benchmarks
:challenge,performance gap in HPC-specific tasks
:domain,HPC domains
:`agent type`,generalist agent
:challenge,3d world interaction
:agent,LEO
:`agent type`,embodied multi-modal and multi-task generalist agent
:`training scheme`,"shared LLM-based model architectures, objectives, and weights"
:dataset,extensive dataset for 3D world
:task,3D captioning
:task,embodied reasoning
:task,embodied navigation
:results,ablation results
:`agent type`,embodied generalist agent
:approach,enhanced scene graph generation
:concept,relationship hierarchy
:component,Bayesian classification head
:concept,informative hierarchical structure
:task,relationship prediction
:pipeline,commonsense validation
:system,scene graph prediction system
:requirement,external large language model assistance at test time
:experiment,on Visual Genome and OpenImage V6 datasets
:algorithm,scene graph generation algorithms
:capability,generating reasonable predictions beyond dataset annotations
:system,autonomous planning system
:vehicle,intelligent quadcopter UAV
:method,trajectory planning
:challenge,dynamic obstacles
:system,vision-based planning system
:algorithm,lightweight object detection
:task,identifying dynamic obstacles
:technique,Kalman filtering
:task,tracking dynamic obstacles
:phase,planning phase
:algorithm,B-spline-based trajectory search
:task,trajectory generation
:experiment,simulation and real-world
:outcome,successful obstacle detection and avoidance
:integration,autonomous planning systems with large language models
:data,high quality images and captions
:method,Diffusion-DPO
:dataset,Pick-a-Pic
:model,Stable Diffusion XL (SDXL)-1.0
:attribute,visual appeal and prompt alignment
:variant,AI feedback variant
:dataset,large-scale web-crawled video-text pairs
:technique,pre-training video-language models
:video,multi-event and multi-grained
:data,video-text pairs with broad-level captions
:capability,distinguishing event-level discrepancies
:tool,Spot Prober
:approach,extracting events as tuples
:task,generating false event tuples
:task,distinguishing manipulated events
:technique,using manipulated event captions as hard negative samples
:capability,flexible capabilities
:information,private information
:entity,organizations and individuals
:technique,neural embeddings
:issue,information leakage
:approach,Private Retrieval Augmented Generation (PRAG)
:technique,multi-party computation (MPC)
:goal,query and database privacy
:protocol,MPC friendly protocol for Inverted File Approximate Search (IVF)
:task,fast document search
:`linguistic phenomenon`,polysemy
:concept,word meaning variation
:task,automated processing of non-literal language
:ability,generalizing word meaning
:task,systematic word meta-sense extension (SWorME)
:ability,extending word meaning
:`semantic change`,incremental lexical semantic change
:`semantic change`,highly non-literal meaning extension
:method,analogy-based word meaning extension
:task,knowledge exploration
:tool,Latent Lab
:task,discovering connections
:concept,exploration over search
:system,collaborative AI systems
:task,"organizing, searching, and synthesizing content"
:field,human-AI knowledge exploration systems
:task,text-based tasks
:`information type`,multimodal information
:dataset,MultiAPI
:capability,multimodal task proficiency
:component,API calls and contextual prompts
:task,API call decision-making
:task,"domain identification, function selection, and argument generation"
:context,auxiliary context
:paradigm,new paradigm for LLM challenges
:group,readers
:policy,publisher policies for AI-assisted writing
:system,provenance capture system
:tool,Hallmark
:activity,writers' interaction with LLMs
:group,creative writers
:data,written fluent language
:`evaluation metric`,repetition penalty
:concept,repetition in dialogue
:process,lexical re-use in comprehension
:analysis,joint analysis of model production and comprehension
:system,cognitively inspired dialogue generation system
:`data type`,behavioral health data
:source,ubiquitous sensors
:challenge,generalization across devices
:task,developing analysis tools
:challenge,correlation between signals and mental health
:approach,large language model synthesis
:`data type`,multi-sensor data
:task,binary depression classification
:metric,61.1% accuracy
:approach,human-AI collaboration
:profession,clinician experts
:metric,75% correct data reference
:profession,clinician participants
:`philosophical question`,teaching logical reasoning to LLMs
:approach,Reinforcement Learning from Logical Feedback (RLLF)
:task,legal reasoning tasks
:relationship,language and logic
:technique,Topological Data Analysis
:approach,TDA-based approach for OOD detection
:approach,CLS embedding approach for OOD detection
:data,politics and entertainment news articles from HuffPost
:category,in-distribution data
:data,IMDb reviews
:category,far out-of-domain samples
:data,CNN/DailyMail
:category,near out-of-domain samples
:data,business news articles from HuffPost
:category,same-domain datasets
:`research topic`,grounding in language understanding
:phenomenon,color perception and color language
:alignment,color space and language model feature space
:dataset,colors and their descriptions
:alignment,inter-space alignment
:alignment,intra-space alignment
:description,monolexemic color descriptions
:description,real linguistic usage
:requirement,extensive parameters and computational demands
:attribute,practicality and scalability
:paper,position paper
:challenge,significant challenges and open issues
:technique,novel efficient PEFT architectures
:outcome,further research and discussions
:data,large instruction datasets
:study,finetuning best practices
:data,instruction finetuning datasets
:metric,dataset size
:finding,small dataset sufficiency
:characteristic,open-endedness
:strategy,mixing finetuning datasets
:data,textbook-style finetuning datasets
:data,open-ended QA finetuning datasets
:technology,transformative technology
:capability,autonomy in agents
:effort,building LLM-based autonomous agents
:concept,systematic architecture design exploration
:benefit,using autonomous agents for planning and execution
:value,responsible AI
:attribute,software quality attributes
:concept,pattern-oriented reference architecture
:methodology,responsible-ai-by-design
:evaluation,completeness and utility
:concept,new possibilities
:issue,data privacy and misinformation
:value,transparency and auditability
:tool,metrics catalogue
:method,systematic multivocal literature review
:framework,tripartite framework
:value,accountability in AI systems
:community,organizations
:challenge,traditional education challenges
:field,digital education
:`model family`,educational large language model
:goal,improving education quality
:field,smart education
:`model family`,large model for education
:stakeholder,"educators, researchers, policy-makers"
:goal,advancing LLM4Edu
:challenge,LLM4Edu challenges
:`data type`,multiple data types
:`data type`,non-textual data types
:concept,multimodal
:product,multimodal products
:company,major technology companies
:guide,practical guide for multimodal models
:compilation,latest algorithms and datasets for multimodal models
:understanding,multimodal models
:challenge,model size and retrieval
:method,ComPEFT
:technique,sparsification and ternary quantization
:metric,compression ratio
:`model type`,compressed expert model
:capability,few-shot compositional generalization
:benefit,efficient communication and computation
:technique,full-finetuning
:resource,ComPEFT code
:URL,https://github.com/prateeky2806/compeft
:`data type`,instruction-following data
:capability,text-grounding
:scenario,text-rich scenarios
:model,TGDoc
:capability,spatial text discernment
:dataset,PowerPoint presentations dataset
:task,instruction tuning tasks
:goal,cohesive alignment
:`data type`,text-rich image conversations
:integration,text location data
:`research area`,text retrieval from natural scene images
:characteristic,natural scene image challenges
:methodology,deep learning-based methods
:task,text detection and recognition
:task,natural scene text extraction
:language,Bangla
:system,end-to-end system for Bangla signboard text
:task,Bangla signboard text processing
:dataset,manually annotated and synthetic datasets for Bangla signboards
:`model architecture`,CTC-based and encoder-decoder
:task,Bangla address text recognition
:model,address text correction model
:`model architecture`,sequence-to-sequence transformer-based network
:model,Bangla address text parser
:`model architecture`,transformer-based pre-trained language model
:expectation,user expectations
:method,reference retrieval
:method,consistency verification
:method,"reference-free, uncertainty-based detection"
:process,human factuality checking
:aspect,informative and important keywords focus
:aspect,unreliable tokens in historical context focus
:aspect,token properties focus
:challenge,crafting efficient reward model
:process,diffusion model denoising
:method,direct preference for denoising diffusion policy optimization
:model,optimal reward model
:experiment,D3PO evaluation
:goal,image safety
:phenomenon,increasing attention and widespread application
:activity,systematic examination
:process,construction process of aligned language models
:stage,pretraining and alignment training
:property,model calibration
:aspect,"generation, factuality, and understanding"
:`training process`,training process of large language models
:task,structural knowledge extraction
:framework,ViStruct
:concept,novel designs
:approach,programming language structure leverage
:task,visual structural information representation
:`learning strategy`,curriculum-based learning
:`dataset collection`,visual structural knowledge extraction datasets
:task,visual structural knowledge extraction
:approach,weakly-supervised approach
:source,image-caption pairs from the web
:task,visual structure prediction
:resource,ViStruct code
:url,https://github.com/yangyi-chen/vi-struct
:technique,medical dialogue summarization
:task,medical reporting
:profession,healthcare professionals
:problem,time constraints in medical reporting
:factor,prompt formulation
:outcome,quality and relevance of automated medical reports
:strategy,shot prompting
:strategy,pattern prompting
:outcome,automated medical report quality
:approach,two-shot prompting with scope and domain context
:benchmark,human reference set by general practitioner
:issue,report length
:process,legal intake
:organization,legal aid organizations
:issue,overconfidence in immediate response
:technology,logic based decision tree
:process,automated legal intake
:issue,access to justice
:topic,immigration and eviction
:`proof-of-concept`,LLM for legal intake
:capability,eliciting and inferring intentions
:problem,factual hallucination
:solution,knowledge graph-based retrofitting
:method,querying knowledge graph
:process,knowledge verifying and refining
:benchmark,factual QA benchmarks
:application,large language model in power systems
:goal,operational efficiency
:threat,security threats from LLM application
:field,power systems
:need,research and development of countermeasures
:task,sequential decision-making tasks
:environment,real-time dynamic environments
:challenge,costly and time-consuming deployment
:challenge,LLM-based agent limitations
:agent,student agent
:agent,teacher agent
:knowledge,prior knowledge of LLM
:training,subsequent training with environment feedback
:environment,MiniGrid environments
:challenge,LLM deployment and fine-tuning on mobile edge devices
:constraint,"limited computing, memory, and energy budgets"
:framework,Confidant
:device,commodity mobile devices
:model,sub-models of LLM
:mechanism,pipeline parallel training
:scheduler,backend scheduler
:hardware,heterogeneous compute hardware
:`hardware type`,mobile CPU and GPUs
:result,memory reduction and inference speedup
:`model family`,neural network models of code
:model,code2seq
:model,seq2seq
:technique,syntactic perturbations
:study,transferability of adversarial examples
:technique,white-box attacks
:technique,prompt-based defenses
:experiment,adversarial example transferability
:goal,model resilience
:phenomenon,institutional bias
:outcome,various societal outcomes
:`data type`,written records
:action,bias identification
:intervention,bias reduction training
:`tool category`,machine learning tools
:`data type`,biased text data
:database,RedditBias
:task,textual bias analysis
:task,bias classification
:technique,t-SNE
:algorithm,k-NN classifier
:task,bias type differentiation
:model,Mini BERT
:`model family`,multilingual models
:recommendation,refining monolingual models
:capability,source code representation
:study,attention analysis studies
:concept,interpretability insights
:concept,context modeling
:concept,attention mechanism factors
:study,this empirical study
:concept,scaled transformation norms
:concept,syntactic structure
:concept,syntactic code properties
:concept,factors beyond attention weights
:field,neural code models
:application,program analysis
:issue,performance degeneration
:method,model merging
:goal,resilience in general tasks
:method,LM-Cocktail
:technique,weighted average merging
:outcome,strong empirical performance
:model,BGE
:benchmark,MTEB
:resource,code and checkpoints
:examples,few-shot CoT examples
:influence,text style of in-context examples
:approach,AlignCOT
:capability,reasoning capability of large language model
:style,native style of LLMs
:resource,source code and dataset for AlignCOT
:`design paradigm`,modular design in autonomous driving
:components,"perception, prediction, planning, control"
:capability,comprehension and generation ability
:technique,diffusion techniques
:concept,interleaved vision-action pair
:format,unified format for visual features and control signals
:model,ADriver-I
:dataset,nuScenes and large-scale private datasets
:benchmark,constructed baselines
:field,autonomous driving and embodied intelligence
:task,image style transfer
:fields,computer graphics and computer vision
:limitation,stylization of specific objects
:methods,current image style transfer methods
:framework,SoulStyler
:task,parse text and identify stylization goals
:encoder,CLIP-based semantic visual embedding encoder
:loss,localized text-image block matching loss
:goal,targeted style transfer
:capability,accurate style transfer on target objects
:code,SoulStyler code
:URL,https://github.com/yisuanwang/soulstyler
:task,evidence retrieval for long document question answering
:activity,locating relevant paragraphs
:task,zero-shot long document evidence retrieval
:activity,processing entire document
:cost,monetary expense
:service,enterprise APIs
:technique,exploiting discourse structure
:task,creating condensed document representation
:representation,condensed document representation
:goal,comprehensive understanding
:performance,99.6% of best zero-shot approach's performance
:technique,proposed techniques
:technique,self-ask reasoning agent
:task,complex multi-hop question answering
:performance,zero-shot performance using gold evidence
:technique,combined approach
:agent,household embodied agent
:task,selecting substitute objects
:framework,Commonsense Object Affordance Task (COAT)
:task,alternative object utilization
:dataset,commonsense question-and-answer datasets
:consideration,object-utility alignment
:consideration,object physical state
:variable,abstract variables reflecting object's physical condition
:simulation,diverse household scenarios
:mapping,object-utility mappings
:resource,GitHub repository for COAT
:research,physical commonsense reasoning in language models
:research,household agent intelligence
:process,forward pass
:bottleneck,memory access
:observation,parallel token processing time
:technique,speculative sampling
:limitation,shared tokenizer requirement
:`model component`,additional input token
:performance,speed-up
:task,referring segmentation
:task,generic vision tasks
:framework,universal visual in-context prompting
:task,open-set segmentation and detection
:component,prompt encoder
:input,reference image segments
:capability,referring and generic segmentation
:model,universal visual in-context prompting model
:metric,PQ (Panoptic Quality)
:url,https://github.com/ux-decoder/dinov
